{
  "base_url": "https://python.langchain.com/docs/introduction/",
  "total_pages": 50,
  "crawl_timestamp": "2025-08-24 15:31:34",
  "pages": [
    {
      "url": "https://python.langchain.com/docs/introduction/",
      "title": "Introduction | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Introduction | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page LangChain is a framework for developing applications powered by large language models (LLMs). LangChain simplifies every stage of the LLM application lifecycle: Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support. Productionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence. Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform. LangChain implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers. See the integrations page for more. Select chat model:Google Geminiâ–¾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") model.invoke(\"Hello, world!\") noteThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library. Architectureâ€‹ The LangChain framework consists of multiple open-source libraries. Read more in the Architecture page. langchain-core: Base abstractions for chat models and other components. Integration packages (e.g. langchain-openai, langchain-anthropic, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers. langchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture. langchain-community: Third-party integrations that are community maintained. langgraph: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See LangGraph documentation. Guidesâ€‹ Tutorialsâ€‹ If you're looking to build something specific or are more of a hands-on learner, check out our tutorials section. This is the best place to get started. These are the best ones to get started with: Build a Simple LLM Application Build a Chatbot Build an Agent Introduction to LangGraph Explore the full list of LangChain tutorials here, and check out other LangGraph tutorials here. To learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available here. How-to guidesâ€‹ Here youâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions. These how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the Tutorials and the API Reference. However, these guides will help you quickly accomplish common tasks using chat models, vector stores, and other common LangChain components. Check out LangGraph-specific how-tos here. Conceptual guideâ€‹ Introductions to all the key parts of LangChain youâ€™ll need to know! Here you'll find high level explanations of all LangChain concepts. For a deeper dive into LangGraph concepts, check out this page. Integrationsâ€‹ LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. If you're looking to get up and running quickly with chat models, vector stores, or other LangChain components from a specific provider, check out our growing list of integrations. API referenceâ€‹ Head to the reference section for full documentation of all classes and methods in the LangChain Python packages. Ecosystemâ€‹ ğŸ¦œğŸ› ï¸ LangSmithâ€‹ Trace and evaluate your language model applications and intelligent agents to help you move from prototype to production. ğŸ¦œğŸ•¸ï¸ LangGraphâ€‹ Build stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more. Additional resourcesâ€‹ Versionsâ€‹ See what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more. Securityâ€‹ Read up on security best practices to make sure you're developing safely with LangChain. Contributingâ€‹ Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.ArchitectureGuidesTutorialsHow-to guidesConceptual guideIntegrationsAPI referenceEcosystemğŸ¦œğŸ› ï¸ LangSmithğŸ¦œğŸ•¸ï¸ LangGraphAdditional resourcesVersionsSecurityContributing",
      "timestamp": "2025-08-24 15:28:13"
    },
    {
      "url": "https://python.langchain.com/docs/tutorials/llm_chain/",
      "title": "Build a simple LLM application with chat models and prompt templates | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Build a simple LLM application with chat models and prompt templates | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call! After reading this tutorial, you'll have a high level overview of: Using language models Using prompt templates Debugging and tracing your application using LangSmith Let's dive in! Setupâ€‹ Jupyter Notebookâ€‹ This and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install. Installationâ€‹ To install LangChain run: PipCondapip install langchainconda install langchain -c conda-forge For more details, see our Installation guide. LangSmithâ€‹ Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. After you sign up at the link above, make sure to set your environment variables to start logging traces: export LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"export LANGSMITH_PROJECT=\"default\" # or any other project name Or, if in a notebook, you can set them with: import getpassimport ostry: # load environment variables from .env file (requires `python-dotenv`) from dotenv import load_dotenv load_dotenv()except ImportError: passos.environ[\"LANGSMITH_TRACING\"] = \"true\"if \"LANGSMITH_API_KEY\" not in os.environ: os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass( prompt=\"Enter your LangSmith API key (optional): \" )if \"LANGSMITH_PROJECT\" not in os.environ: os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass( prompt='Enter your LangSmith Project Name (default = \"default\"): ' ) if not os.environ.get(\"LANGSMITH_PROJECT\"): os.environ[\"LANGSMITH_PROJECT\"] = \"default\" Using Language Modelsâ€‹ First up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations. Select chat model:Google Geminiâ–¾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") Let's first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method. from langchain_core.messages import HumanMessage, SystemMessagemessages = [ SystemMessage(\"Translate the following from English into Italian\"), HumanMessage(\"hi!\"),]model.invoke(messages)API Reference:HumanMessage | SystemMessage AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-32654a56-627c-40e1-a141-ad9350bbfd3e-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}) tipIf we've enabled LangSmith, we can see that this run is logged to LangSmith, and can see the LangSmith trace. The LangSmith trace reports token usage information, latency, standard model parameters (such as temperature), and other information. Note that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts. LangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent: model.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")]) Streamingâ€‹ Because chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model: for token in model.stream(messages): print(token.content, end=\"|\") |C|iao|!|| You can find more details on streaming chat model outputs in this guide. Prompt Templatesâ€‹ Right now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input. Prompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model. Let's create a prompt template here. It will take in two user variables: language: The language to translate text into text: The text to translate from langchain_core.prompts import ChatPromptTemplatesystem_template = \"Translate the following from English into {language}\"prompt_template = ChatPromptTemplate.from_messages( [(\"system\", system_template), (\"user\", \"{text}\")])API Reference:ChatPromptTemplate Note that ChatPromptTemplate supports multiple message roles in a single template. We format the language parameter into the system message, and the user text into a user message. The input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})prompt ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]) We can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do: prompt.to_messages() [SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})] Finally, we can invoke the chat model on the formatted prompt: response = model.invoke(prompt)print(response.content) Ciao! tipMessage content can contain both text and content blocks with additional structure. See this guide for more information. If we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information. Conclusionâ€‹ That's it! In this tutorial you've learned how to create your first simple LLM application. You've learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith. This just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we've got a lot of other resources! For further reading on the core concepts of LangChain, we've got detailed Conceptual Guides. If you have more specific questions on these concepts, check out the following sections of the how-to guides: Chat models Prompt templates And the LangSmith docs: LangSmith SetupJupyter NotebookInstallationLangSmithUsing Language ModelsStreamingPrompt TemplatesConclusion",
      "timestamp": "2025-08-24 15:28:16"
    },
    {
      "url": "https://python.langchain.com/docs/security/",
      "title": "Security Policy | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Security Policy | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources. Best practicesâ€‹ When building such applications developers should remember to follow good security practices: Limit Permissions: Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), specifying proxy configurations to control external requests, etc. as appropriate for your application. Anticipate Potential Misuse: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it's safest to assume that any LLM able to use those credentials may in fact delete data. Defense in Depth: No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It's best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use. Risks of not doing so include, but are not limited to: Data corruption or loss. Unauthorized access to confidential information. Compromised performance or availability of critical resources. Example scenarios with mitigation strategies: A user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container. A user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse. A user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials. If you're building applications that access external resources like file systems, APIs or databases, consider speaking with your company's security team to determine how to best design and secure your applications. Reporting OSS Vulnerabilitiesâ€‹ LangChain is partnered with huntr by Protect AI to provide a bounty program for our open source projects. Please report security vulnerabilities associated with the LangChain open source projects at huntr. Before reporting a vulnerability, please review: In-Scope Targets and Out-of-Scope Targets below. The langchain-ai/langchain monorepo structure. The Best Practices above to understand what we consider to be a security vulnerability vs. developer responsibility. In-Scope Targetsâ€‹ The following packages and repositories are eligible for bug bounties: langchain-core langchain (see exceptions) langchain-community (see exceptions) langgraph langserve Out of Scope Targetsâ€‹ All out of scope targets defined by huntr as well as: langchain-experimental: This repository is for experimental code and is not eligible for bug bounties (see package warning), bug reports to it will be marked as interesting or waste of time and published with no bounty attached. tools: Tools in either langchain or langchain-community are not eligible for bug bounties. This includes the following directories libs/langchain/langchain/tools libs/community/langchain_community/tools Please review the Best Practices for more details, but generally tools interact with the real world. Developers are expected to understand the security implications of their code and are responsible for the security of their tools. Code documented with security notices. This will be decided on a case by case basis, but likely will not be eligible for a bounty as the code is already documented with guidelines for developers that should be followed for making their application secure. Any LangSmith related repositories or APIs (see Reporting LangSmith Vulnerabilities). Reporting LangSmith Vulnerabilitiesâ€‹ Please report security vulnerabilities associated with LangSmith by email to security@langchain.dev. LangSmith site: https://smith.langchain.com SDK client: https://github.com/langchain-ai/langsmith-sdk Other Security Concernsâ€‹ For any other security concerns, please contact us at security@langchain.dev.Best practicesReporting OSS VulnerabilitiesIn-Scope TargetsOut of Scope TargetsReporting LangSmith VulnerabilitiesOther Security Concerns",
      "timestamp": "2025-08-24 15:28:20"
    },
    {
      "url": "https://python.langchain.com/docs/tutorials/agents/",
      "title": "Build an Agent | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Build an Agent | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page LangChain supports the creation of agents, or systems that use LLMs as reasoning engines to determine which actions to take and the inputs necessary to perform the action. After executing actions, the results can be fed back into the LLM to determine whether more actions are needed, or whether it is okay to finish. This is often achieved via tool-calling. In this tutorial we will build an agent that can interact with a search engine. You will be able to ask this agent questions, watch it call the search tool, and have conversations with it. End-to-end agentâ€‹ The code snippet below represents a fully functional agent that uses an LLM to decide which tools to use. It is equipped with a generic search tool. It has conversational memory - meaning that it can be used as a multi-turn chatbot. In the rest of the guide, we will walk through the individual components and what each part does - but if you want to just grab some code and get started, feel free to use this! # Import relevant functionalityfrom langchain.chat_models import init_chat_modelfrom langchain_tavily import TavilySearchfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.prebuilt import create_react_agent# Create the agentmemory = MemorySaver()model = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")search = TavilySearch(max_results=2)tools = [search]agent_executor = create_react_agent(model, tools, checkpointer=memory)API Reference:MemorySaver | create_react_agent # Use the agentconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}input_message = { \"role\": \"user\", \"content\": \"Hi, I'm Bob and I live in SF.\",}for step in agent_executor.stream( {\"messages\": [input_message]}, config, stream_mode=\"values\"): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================Hi, I'm Bob and I live in SF.==================================\u001b[1m Ai Message \u001b[0m==================================Hello Bob! I notice you've introduced yourself and mentioned you live in SF (San Francisco), but you haven't asked a specific question or made a request that requires the use of any tools. Is there something specific you'd like to know about San Francisco or any other topic? I'd be happy to help you find information using the available search tools. input_message = { \"role\": \"user\", \"content\": \"What's the weather where I live?\",}for step in agent_executor.stream( {\"messages\": [input_message]}, config, stream_mode=\"values\"): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================What's the weather where I live?==================================\u001b[1m Ai Message \u001b[0m==================================[{'text': 'Let me search for current weather information in San Francisco.', 'type': 'text'}, {'id': 'toolu_011kSdheoJp8THURoLmeLtZo', 'input': {'query': 'current weather San Francisco CA'}, 'name': 'tavily_search', 'type': 'tool_use'}]Tool Calls: tavily_search (toolu_011kSdheoJp8THURoLmeLtZo) Call ID: toolu_011kSdheoJp8THURoLmeLtZo Args: query: current weather San Francisco CA=================================\u001b[1m Tool Message \u001b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco CA\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168606, 'localtime': '2025-06-17 06:56'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", \"score\": 0.944705, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed âš¡ San Francisco Weather Forecast for June 2025 - day/night ğŸŒ¡ï¸ temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget Â°F. World; United States; California; Weather in San Francisco; ... 17 +64Â° +54Â° 18 +61Â° +54Â° 19\", \"score\": 0.86441374, \"raw_content\": null}], \"response_time\": 2.34}==================================\u001b[1m Ai Message \u001b[0m==================================Based on the search results, here's the current weather in San Francisco:- Temperature: 53.1Â°F (11.7Â°C)- Condition: Foggy- Wind: 4.0 mph from the Southwest- Humidity: 86%- Visibility: 9 milesThis is quite typical weather for San Francisco, with the characteristic fog that the city is known for. Would you like to know anything else about the weather or San Francisco in general? Setupâ€‹ Jupyter Notebookâ€‹ This guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect interactive environments for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc), and observing these cases is a great way to better understand building with LLMs. This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install. Installationâ€‹ To install LangChain run: %pip install -U langgraph langchain-tavily langgraph-checkpoint-sqlite For more details, see our Installation guide. LangSmithâ€‹ Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. After you sign up at the link above, make sure to set your environment variables to start logging traces: export LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\" Or, if in a notebook, you can set them with: import getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass() Tavilyâ€‹ We will be using Tavily (a search engine) as a tool. In order to use it, you will need to get and set an API key: export TAVILY_API_KEY=\"...\" Or, if in a notebook, you can set it with: import getpassimport osos.environ[\"TAVILY_API_KEY\"] = getpass.getpass() Define toolsâ€‹ We first need to create the tools we want to use. Our main tool of choice will be Tavily - a search engine. We can use the dedicated langchain-tavily integration package to easily use Tavily search engine as tool with LangChain. from langchain_tavily import TavilySearchsearch = TavilySearch(max_results=2)search_results = search.invoke(\"What is the weather in SF\")print(search_results)# If we want, we can create other tools.# Once we have all the tools we want, we can put them in a list that we will reference later.tools = [search] {'query': 'What is the weather in SF', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Weather in San Francisco, CA', 'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168606, 'localtime': '2025-06-17 06:56'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", 'score': 0.9185379, 'raw_content': None}, {'title': 'Weather in San Francisco in June 2025', 'url': 'https://world-weather.info/forecast/usa/san_francisco/june-2025/', 'content': \"Weather in San Francisco in June 2025 (California) - Detailed Weather Forecast for a Month * Weather in San Francisco Weather in San Francisco in June 2025 * 1 +63Â° +55Â° * 2 +66Â° +54Â° * 3 +66Â° +55Â° * 4 +66Â° +54Â° * 5 +66Â° +55Â° * 6 +66Â° +57Â° * 7 +64Â° +55Â° * 8 +63Â° +55Â° * 9 +63Â° +54Â° * 10 +59Â° +54Â° * 11 +59Â° +54Â° * 12 +61Â° +54Â° Weather in Washington, D.C.**+68Â°** Sacramento**+81Â°** Pleasanton**+72Â°** Redwood City**+68Â°** San Leandro**+61Â°** San Mateo**+64Â°** San Rafael**+70Â°** San Ramon**+64Â°** South San Francisco**+61Â°** Daly City**+59Â°** Wilder**+66Â°** Woodacre**+70Â°** world's temperature today Colchani day+50Â°F night+16Â°F Az Zubayr day+124Â°F night+93Â°F Weather forecast on your site Install _San Francisco_ +61Â° Temperature units\", 'score': 0.7978881, 'raw_content': None}], 'response_time': 2.62} tipIn many applications, you may want to define custom tools. LangChain supports custom tool creation via Python functions and other means. Refer to the How to create tools guide for details. Using Language Modelsâ€‹ Next, let's learn how to use a language model to call tools. LangChain supports many different language models that you can use interchangably - select the one you want to use below! Select chat model:Google Geminiâ–¾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") You can call the language model by passing in a list of messages. By default, the response is a content string. query = \"Hi!\"response = model.invoke([{\"role\": \"user\", \"content\": query}])response.text() 'Hello! How can I help you today?' We can now see what it is like to enable this model to do tool calling. In order to enable that we use .bind_tools to give the language model knowledge of these tools model_with_tools = model.bind_tools(tools) We can now call the model. Let's first call it with a normal message, and see how it responds. We can look at both the content field as well as the tool_calls field. query = \"Hi!\"response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])print(f\"Message content: {response.text()}\\n\")print(f\"Tool calls: {response.tool_calls}\") Message content: Hello! I'm here to help you. I have access to a powerful search tool that can help answer questions and find information about various topics. What would you like to know about?Feel free to ask any question or request information, and I'll do my best to assist you using the available tools.Tool calls: [] Now, let's try calling it with some input that would expect a tool to be called. query = \"Search for the weather in SF\"response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])print(f\"Message content: {response.text()}\\n\")print(f\"Tool calls: {response.tool_calls}\") Message content: I'll help you search for information about the weather in San Francisco.Tool calls: [{'name': 'tavily_search', 'args': {'query': 'current weather San Francisco'}, 'id': 'toolu_015gdPn1jbB2Z21DmN2RAnti', 'type': 'tool_call'}] We can see that there's now no text content, but there is a tool call! It wants us to call the Tavily Search tool. This isn't calling that tool yet - it's just telling us to. In order to actually call it, we'll want to create our agent. Create the agentâ€‹ Now that we have defined the tools and the LLM, we can create the agent. We will be using LangGraph to construct the agent. Currently, we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic. Now, we can initialize the agent with the LLM and the tools. Note that we are passing in the model, not model_with_tools. That is because create_react_agent will call .bind_tools for us under the hood. from langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(model, tools)API Reference:create_react_agent Run the agentâ€‹ We can now run the agent with a few queries! Note that for now, these are all stateless queries (it won't remember previous interactions). Note that the agent will return the final state at the end of the interaction (which includes any inputs, we will see later on how to get only the outputs). First up, let's see how it responds when there's no need to call a tool: input_message = {\"role\": \"user\", \"content\": \"Hi!\"}response = agent_executor.invoke({\"messages\": [input_message]})for message in response[\"messages\"]: message.pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================Hi!==================================\u001b[1m Ai Message \u001b[0m==================================Hello! I'm here to help you with your questions using the available search tools. Please feel free to ask any question, and I'll do my best to find relevant and accurate information for you. In order to see exactly what is happening under the hood (and to make sure it's not calling a tool) we can take a look at the LangSmith trace Let's now try it out on an example where it should be invoking the tool input_message = {\"role\": \"user\", \"content\": \"Search for the weather in SF\"}response = agent_executor.invoke({\"messages\": [input_message]})for message in response[\"messages\"]: message.pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================Search for the weather in SF==================================\u001b[1m Ai Message \u001b[0m==================================[{'text': \"I'll help you search for weather information in San Francisco. Let me use the search engine to find current weather conditions.\", 'type': 'text'}, {'id': 'toolu_01WWcXGnArosybujpKzdmARZ', 'input': {'query': 'current weather San Francisco SF'}, 'name': 'tavily_search', 'type': 'tool_use'}]Tool Calls: tavily_search (toolu_01WWcXGnArosybujpKzdmARZ) Call ID: toolu_01WWcXGnArosybujpKzdmARZ Args: query: current weather San Francisco SF=================================\u001b[1m Tool Message \u001b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco SF\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168606, 'localtime': '2025-06-17 06:56'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", \"score\": 0.885373, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed âš¡ San Francisco Weather Forecast for June 2025 - day/night ğŸŒ¡ï¸ temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget Â°F. World; United States; California; Weather in San Francisco; ... 17 +64Â° +54Â° 18 +61Â° +54Â° 19\", \"score\": 0.8830044, \"raw_content\": null}], \"response_time\": 2.6}==================================\u001b[1m Ai Message \u001b[0m==================================Based on the search results, here's the current weather in San Francisco:- Temperature: 53.1Â°F (11.7Â°C)- Conditions: Foggy- Wind: 4.0 mph from the SW- Humidity: 86%- Visibility: 9.0 milesThe weather appears to be typical for San Francisco, with morning fog and mild temperatures. The \"feels like\" temperature is 52.4Â°F (11.3Â°C). We can check out the LangSmith trace to make sure it's calling the search tool effectively. Streaming Messagesâ€‹ We've seen how the agent can be called with .invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur. for step in agent_executor.stream({\"messages\": [input_message]}, stream_mode=\"values\"): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================Search for the weather in SF==================================\u001b[1m Ai Message \u001b[0m==================================[{'text': \"I'll help you search for information about the weather in San Francisco.\", 'type': 'text'}, {'id': 'toolu_01DCPnJES53Fcr7YWnZ47kDG', 'input': {'query': 'current weather San Francisco'}, 'name': 'tavily_search', 'type': 'tool_use'}]Tool Calls: tavily_search (toolu_01DCPnJES53Fcr7YWnZ47kDG) Call ID: toolu_01DCPnJES53Fcr7YWnZ47kDG Args: query: current weather San Francisco=================================\u001b[1m Tool Message \u001b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168506, 'localtime': '2025-06-17 06:55'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", \"score\": 0.9542825, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed âš¡ San Francisco Weather Forecast for June 2025 - day/night ğŸŒ¡ï¸ temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget Â°F. World; United States; California; Weather in San Francisco; ... 17 +64Â° +54Â° 18 +61Â° +54Â° 19\", \"score\": 0.8638634, \"raw_content\": null}], \"response_time\": 2.57}==================================\u001b[1m Ai Message \u001b[0m==================================Based on the search results, here's the current weather in San Francisco:- Temperature: 53.1Â°F (11.7Â°C)- Condition: Foggy- Wind: 4.0 mph from the Southwest- Humidity: 86%- Visibility: 9.0 miles- Feels like: 52.4Â°F (11.3Â°C)This is quite typical weather for San Francisco, which is known for its fog, especially during the morning hours. The city's proximity to the ocean and unique geographical features often result in mild temperatures and foggy conditions. Streaming tokensâ€‹ In addition to streaming back messages, it is also useful to stream back tokens. We can do this by specifying stream_mode=\"messages\". ::: note Below we use message.text(), which requires langchain-core>=0.3.37. ::: for step, metadata in agent_executor.stream( {\"messages\": [input_message]}, stream_mode=\"messages\"): if metadata[\"langgraph_node\"] == \"agent\" and (text := step.text()): print(text, end=\"|\") I|'ll help you search for information| about the weather in San Francisco.|Base|d on the search results, here|'s the current weather in| San Francisco:-| Temperature: 53.1Â°F (|11.7Â°C)-| Condition: Foggy- Wind:| 4.0 mph from| the Southwest- Humidity|: 86%|- Visibility: 9|.0 miles- Pressure: |30.02 in|HgThe weather| is characteristic of San Francisco, with| foggy conditions and mild temperatures|. The \"feels like\" temperature is slightly| lower at 52.4|Â°F (11.|3Â°C)| due to the wind chill effect|.| Adding in memoryâ€‹ As mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in a checkpointer. When passing in a checkpointer, we also have to pass in a thread_id when invoking the agent (so it knows which thread/conversation to resume from). from langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()API Reference:MemorySaver agent_executor = create_react_agent(model, tools, checkpointer=memory)config = {\"configurable\": {\"thread_id\": \"abc123\"}} input_message = {\"role\": \"user\", \"content\": \"Hi, I'm Bob!\"}for step in agent_executor.stream( {\"messages\": [input_message]}, config, stream_mode=\"values\"): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================Hi, I'm Bob!==================================\u001b[1m Ai Message \u001b[0m==================================Hello Bob! I'm an AI assistant who can help you search for information using specialized search tools. Is there anything specific you'd like to know about or search for? I'm happy to help you find accurate and up-to-date information on various topics. input_message = {\"role\": \"user\", \"content\": \"What's my name?\"}for step in agent_executor.stream( {\"messages\": [input_message]}, config, stream_mode=\"values\"): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================What's my name?==================================\u001b[1m Ai Message \u001b[0m==================================Your name is Bob, as you introduced yourself earlier. I can remember information shared within our conversation without needing to search for it. Example LangSmith trace If you want to start a new conversation, all you have to do is change the thread_id used config = {\"configurable\": {\"thread_id\": \"xyz123\"}}input_message = {\"role\": \"user\", \"content\": \"What's my name?\"}for step in agent_executor.stream( {\"messages\": [input_message]}, config, stream_mode=\"values\"): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================What's my name?==================================\u001b[1m Ai Message \u001b[0m==================================I apologize, but I don't have access to any tools that would tell me your name. I can only assist you with searching for publicly available information using the tavily_search function. I don't have access to personal information about users. If you'd like to tell me your name, I'll be happy to address you by it. Conclusionâ€‹ That's a wrap! In this quick start we covered how to create a simple agent. We've then shown how to stream back a response - not only with the intermediate steps, but also tokens! We've also added in memory so you can have a conversation with them. Agents are a complex topic with lots to learn! For more information on Agents, please check out the LangGraph documentation. This has it's own set of concepts, tutorials, and how-to guides.End-to-end agentSetupJupyter NotebookInstallationLangSmithTavilyDefine toolsUsing Language ModelsCreate the agentRun the agentStreaming MessagesStreaming tokensAdding in memoryConclusion",
      "timestamp": "2025-08-24 15:28:23"
    },
    {
      "url": "https://python.langchain.com/docs/tutorials/",
      "title": "Tutorials | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Tutorials | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page New to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications. Get startedâ€‹ Familiarize yourself with LangChain's open-source components by building simple applications. If you're looking to get started with chat models, vector stores, or other LangChain components from a specific provider, check out our supported integrations. Chat models and prompts: Build a simple LLM application with prompt templates and chat models. Semantic search: Build a semantic search engine over a PDF with document loaders, embedding models, and vector stores. Classification: Classify text into categories or labels using chat models with structured outputs. Extraction: Extract structured data from text and other unstructured media using chat models and few-shot examples. Refer to the how-to guides for more detail on using all LangChain components. Orchestrationâ€‹ Get started using LangGraph to assemble LangChain components into full-featured applications. Chatbots: Build a chatbot that incorporates memory. Agents: Build an agent that interacts with external tools. Retrieval Augmented Generation (RAG) Part 1: Build an application that uses your own documents to inform its responses. Retrieval Augmented Generation (RAG) Part 2: Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval. Question-Answering with SQL: Build a question-answering system that executes SQL queries to inform its responses. Summarization: Generate summaries of (potentially long) texts. Question-Answering with Graph Databases: Build a question-answering system that queries a graph database to inform its responses. LangSmithâ€‹ LangSmith allows you to closely trace, monitor and evaluate your LLM application. It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build. LangSmith documentation is hosted on a separate site. You can peruse LangSmith tutorials here. Evaluationâ€‹ LangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started: Evaluate your LLM application Get startedOrchestrationLangSmithEvaluation",
      "timestamp": "2025-08-24 15:28:27"
    },
    {
      "url": "https://python.langchain.com/docs/integrations/chat/",
      "title": "Chat models | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Chat models | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Chat models are language models that use a sequence of messages as inputs and return messages as outputs (as opposed to using plain text). These are generally newer models. infoIf you'd like to write your own chat model, see this how-to. If you'd like to contribute an integration, see Contributing integrations. Select chat model:Google Geminiâ–¾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") model.invoke(\"Hello, world!\") Featured Providersâ€‹ infoWhile all these LangChain classes support the indicated advanced feature, you may have to open the provider-specific documentation to learn which hosted models or backends support the feature. ProviderTool callingStructured outputJSON modeLocalMultimodalPackageChatAnthropicâœ…âœ…âŒâŒâœ…langchain-anthropicChatMistralAIâœ…âœ…âŒâŒâŒlangchain-mistralaiChatFireworksâœ…âœ…âœ…âŒâŒlangchain-fireworksAzureChatOpenAIâœ…âœ…âœ…âŒâœ…langchain-openaiChatOpenAIâœ…âœ…âœ…âŒâœ…langchain-openaiChatTogetherâœ…âœ…âœ…âŒâŒlangchain-togetherChatVertexAIâœ…âœ…âŒâŒâœ…langchain-google-vertexaiChatGoogleGenerativeAIâœ…âœ…âŒâŒâœ…langchain-google-genaiChatGroqâœ…âœ…âœ…âŒâŒlangchain-groqChatCohereâœ…âœ…âŒâŒâŒlangchain-cohereChatBedrockâœ…âœ…âŒâŒâŒlangchain-awsChatHuggingFaceâœ…âœ…âŒâœ…âŒlangchain-huggingfaceChatNVIDIAâœ…âœ…âœ…âœ…âœ…langchain-nvidia-ai-endpointsChatOllamaâœ…âœ…âœ…âœ…âŒlangchain-ollamaChatLlamaCppâœ…âœ…âŒâœ…âŒlangchain-communityChatAI21âœ…âœ…âŒâŒâŒlangchain-ai21ChatUpstageâœ…âœ…âŒâŒâŒlangchain-upstageChatDatabricksâœ…âœ…âŒâŒâŒdatabricks-langchainChatWatsonxâœ…âœ…âœ…âŒâŒlangchain-ibmChatXAIâœ…âœ…âŒâŒâŒlangchain-xaiChatPerplexityâŒâœ…âœ…âŒâœ…langchain-perplexity All chat modelsâ€‹ NameDescriptionAbsoThis will help you get started with ChatAbso chat models. For detaile...AI21 LabsThis notebook covers how to get started with AI21 chat models.Alibaba Cloud PAI EASAlibaba Cloud PAI (Platform for AI) is a lightweight and cost-efficie...AnthropicThis notebook provides a quick overview for getting started with Anth...AnyscaleThis notebook demonstrates the use of langchain.chat_models.ChatAnysc...AzureAIChatCompletionsModelThis will help you get started with AzureAIChatCompletionsModel chat ...Azure OpenAIThis guide will help you get started with AzureOpenAI chat models. Fo...Azure ML EndpointAzure Machine Learning is a platform used to build, train, and deploy...Baichuan ChatBaichuan chat models API by Baichuan Intelligent Technology. For more...Baidu QianfanBaidu AI Cloud Qianfan Platform is a one-stop large model development...AWS BedrockThis doc will help you get started with AWS Bedrock chat models. Amaz...CerebrasThis notebook provides a quick overview for getting started with Cere...CloudflareWorkersAIThis will help you get started with CloudflareWorkersAI chat models. ...CohereThis notebook covers how to get started with Cohere chat models.ContextualAIThis will help you get started with Contextual AI's Grounded Language...Coze ChatChatCoze chat models API by coze.com. For more information, see https...Dappier AIDappier: Powering AI with Dynamic, Real-Time Data ModelsDatabricksDatabricks Lakehouse Platform unifies data, analytics, and AI on one ...DeepInfraDeepInfra is a serverless inference as a service that provides access...DeepSeekThis will help you get started with DeepSeek's hosted chat models. Fo...Eden AIEden AI is revolutionizing the AI landscape by uniting the best AI pr...EverlyAIEverlyAI allows you to run your ML models at scale in the cloud. It a...Featherless AIThis will help you get started with FeatherlessAi chat models. For de...FireworksThis doc helps you get started with Fireworks AI chat models. For det...ChatFriendliFriendli enhances AI application performance and optimizes cost savin...GoodfireThis will help you get started with Goodfire chat models. For detaile...Google GeminiAccess Google's Generative AI models, including the Gemini family, di...Google Cloud Vertex AIThis page provides a quick overview for getting started with VertexAI...GPTRouterGPTRouter is an open source LLM API Gateway that offers a universal A...DigitalOcean GradientThis will help you getting started with DigitalOcean Gradient Chat Mo...GreenNodeGreenNode is a global AI solutions provider and a NVIDIA Preferred Pa...GroqThis will help you get started with Groq chat models. For detailed do...ChatHuggingFaceThis will help you get started with langchainhuggingface chat models....IBM watsonx.aiChatWatsonx is a wrapper for IBM watsonx.ai foundation models.JinaChatThis notebook covers how to get started with JinaChat chat models.KineticaThis notebook demonstrates how to use Kinetica to transform natural l...KonkoKonko API is a fully managed Web API designed to help application dev...LiteLLMLiteLLM is a library that simplifies calling Anthropic, Azure, Huggin...Llama 2 ChatThis notebook shows how to augment Llama-2 LLMs with the Llama2Chat w...Llama APIThis notebook shows how to use LangChain with LlamaAPI - a hosted ver...LlamaEdgeLlamaEdge allows you to chat with LLMs of GGUF format both locally an...Llama.cppllama.cpp python library is a simple Python bindings for @ggerganovmaritalkMariTalk is an assistant developed by the Brazilian company Maritaca ...MiniMaxMinimax is a Chinese startup that provides LLM service for companies ...MistralAIThis will help you get started with Mistral chat models. For detailed...MLXThis notebook shows how to get started using MLX LLM's as chat models.ModelScopeModelScope (Home | GitHub) is built upon the notion of â€œModel-as-a-Se...MoonshotMoonshot is a Chinese startup that provides LLM service for companies...NaverThis notebook provides a quick overview for getting started with Nave...NebiusThis page will help you get started with Nebius AI Studio chat models...NetmindThis will help you get started with Netmind chat models. For detailed...NVIDIA AI EndpointsThis will help you get started with NVIDIA chat models. For detailed ...ChatOCIModelDeploymentThis will help you get started with OCIModelDeployment chat models. F...OCIGenAIThis notebook provides a quick overview for getting started with OCIG...ChatOctoAIOctoAI offers easy access to efficient compute and enables users to i...OllamaOllama allows you to run open-source large language models, such as g...OpenAIThis notebook provides a quick overview for getting started with Open...OutlinesThis will help you get started with Outlines chat models. For detaile...PerplexityThis page will help you get started with Perplexity chat models. For ...PipeshiftThis will help you get started with Pipeshift chat models. For detail...ChatPredictionGuardPrediction Guard is a secure, scalable GenAI platform that safeguards...PremAIPremAI is an all-in-one platform that simplifies the creation of robu...PromptLayer ChatOpenAIThis example showcases how to connect to PromptLayer to start recordi...Qwen QwQThis will help you get started with QwQ chat models. For detailed doc...RekaThis notebook provides a quick overview for getting started with Reka...RunPod Chat ModelGet started with RunPod chat models.SambaNovaCloudThis will help you get started with SambaNovaCloud chat models. For d...SambaStudioThis will help you get started with SambaStudio chat models. For deta...ChatSeekrFlowSeekr provides AI-powered solutions for structured, explainable, and ...Snowflake CortexSnowflake Cortex gives you instant access to industry-leading large l...solarDeprecated since version 0.0.34: Use langchain_upstage.ChatUpstage in...SparkLLM ChatSparkLLM chat models API by iFlyTek. For more information, see iFlyTe...Nebula (Symbl.ai)This notebook covers how to get started with Nebula - Symbl.ai's chat...Tencent HunyuanTencent's hybrid model API (Hunyuan API)TogetherThis page will help you get started with Together AI chat models. For...Tongyi QwenTongyi Qwen is a large language model developed by Alibaba's Damo Aca...UpstageThis notebook covers how to get started with Upstage chat models.vectaraVectara is the trusted AI Assistant and Agent platform, which focuses...vLLM ChatvLLM can be deployed as a server that mimics the OpenAI API protocol....Volc Engine MaasThis notebook provides you with a guide on how to get started with vo...Chat WriterThis notebook provides a quick overview for getting started with Writ...xAIThis page will help you get started with xAI chat models. For detaile...XinferenceXinference is a powerful and versatile library designed to serve LLMs,YandexGPTThis notebook goes over how to use Langchain with YandexGPT chat mode...ChatYIThis will help you get started with Yi chat models. For detailed docu...Yuan2.0This notebook shows how to use YUAN2 API in LangChain with the langch...ZHIPU AIThis notebook shows how to use ZHIPU AI API in LangChain with the lan...Featured ProvidersAll chat models",
      "timestamp": "2025-08-24 15:28:30"
    },
    {
      "url": "https://python.langchain.com/docs/how_to/",
      "title": "How-to guides | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "How-to guides | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Here youâ€™ll find answers to â€œHow do Iâ€¦.?â€ types of questions. These guides are goal-oriented and concrete; they're meant to help you complete a specific task. For conceptual explanations see the Conceptual guide. For end-to-end walkthroughs see Tutorials. For comprehensive descriptions of every class and function see the API Reference. Installationâ€‹ How to: install LangChain packages How to: use LangChain with different Pydantic versions Key featuresâ€‹ This highlights functionality that is core to using LangChain. How to: return structured data from a model How to: use a model to call tools How to: stream runnables How to: debug your LLM apps Componentsâ€‹ These are the core building blocks you can use when building applications. Chat modelsâ€‹ Chat Models are newer forms of language models that take messages in and output a message. See supported integrations for details on getting started with chat models from a specific provider. How to: initialize any model in one line How to: work with local models How to: do function/tool calling How to: get models to return structured output How to: cache model responses How to: get log probabilities How to: create a custom chat model class How to: stream a response back How to: track token usage How to: track response metadata across providers How to: use chat model to call tools How to: stream tool calls How to: handle rate limits How to: few-shot prompt tool behavior How to: bind model-specific formatted tools How to: force a specific tool call How to: pass multimodal data directly to models Messagesâ€‹ Messages are the input and output of chat models. They have some content and a role, which describes the source of the message. How to: trim messages How to: filter messages How to: merge consecutive messages of the same type Prompt templatesâ€‹ Prompt Templates are responsible for formatting user input into a format that can be passed to a language model. How to: use few-shot examples How to: use few-shot examples in chat models How to: partially format prompt templates How to: compose prompts together How to: use multimodal prompts Example selectorsâ€‹ Example Selectors are responsible for selecting the correct few shot examples to pass to the prompt. How to: use example selectors How to: select examples by length How to: select examples by semantic similarity How to: select examples by semantic ngram overlap How to: select examples by maximal marginal relevance How to: select examples from LangSmith few-shot datasets LLMsâ€‹ What LangChain calls LLMs are older forms of language models that take a string in and output a string. How to: cache model responses How to: create a custom LLM class How to: stream a response back How to: track token usage How to: work with local models Output parsersâ€‹ Output Parsers are responsible for taking the output of an LLM and parsing into more structured format. How to: parse text from message objects How to: use output parsers to parse an LLM response into structured format How to: parse JSON output How to: parse XML output How to: parse YAML output How to: retry when output parsing errors occur How to: try to fix errors in output parsing How to: write a custom output parser class Document loadersâ€‹ Document Loaders are responsible for loading documents from a variety of sources. How to: load PDF files How to: load web pages How to: load CSV data How to: load data from a directory How to: load HTML data How to: load JSON data How to: load Markdown data How to: load Microsoft Office data How to: write a custom document loader Text splittersâ€‹ Text Splitters take a document and split into chunks that can be used for retrieval. How to: recursively split text How to: split HTML How to: split by character How to: split code How to: split Markdown by headers How to: recursively split JSON How to: split text into semantic chunks How to: split by tokens Embedding modelsâ€‹ Embedding Models take a piece of text and create a numerical representation of it. See supported integrations for details on getting started with embedding models from a specific provider. How to: embed text data How to: cache embedding results How to: create a custom embeddings class Vector storesâ€‹ Vector stores are databases that can efficiently store and retrieve embeddings. See supported integrations for details on getting started with vector stores from a specific provider. How to: use a vector store to retrieve data Retrieversâ€‹ Retrievers are responsible for taking a query and returning relevant documents. How to: use a vector store to retrieve data How to: generate multiple queries to retrieve data for How to: use contextual compression to compress the data retrieved How to: write a custom retriever class How to: add similarity scores to retriever results How to: combine the results from multiple retrievers How to: reorder retrieved results to mitigate the \"lost in the middle\" effect How to: generate multiple embeddings per document How to: retrieve the whole document for a chunk How to: generate metadata filters How to: create a time-weighted retriever How to: use hybrid vector and keyword retrieval Indexingâ€‹ Indexing is the process of keeping your vectorstore in-sync with the underlying data source. How to: reindex data to keep your vectorstore in sync with the underlying data source Toolsâ€‹ LangChain Tools contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer here for a list of pre-built tools. How to: create tools How to: use built-in tools and toolkits How to: use chat models to call tools How to: pass tool outputs to chat models How to: pass runtime values to tools How to: add a human-in-the-loop for tools How to: handle tool errors How to: force models to call a tool How to: disable parallel tool calling How to: access the RunnableConfig from a tool How to: stream events from a tool How to: return artifacts from a tool How to: convert Runnables to tools How to: add ad-hoc tool calling capability to models How to: pass in runtime secrets Multimodalâ€‹ How to: pass multimodal data directly to models How to: use multimodal prompts Agentsâ€‹ noteFor in depth how-to guides for agents, please check out LangGraph documentation. How to: use legacy LangChain Agents (AgentExecutor) How to: migrate from legacy LangChain agents to LangGraph Callbacksâ€‹ Callbacks allow you to hook into the various stages of your LLM application's execution. How to: pass in callbacks at runtime How to: attach callbacks to a module How to: pass callbacks into a module constructor How to: create custom callback handlers How to: use callbacks in async environments How to: dispatch custom callback events Customâ€‹ All of LangChain components can easily be extended to support your own versions. How to: create a custom chat model class How to: create a custom LLM class How to: create a custom embeddings class How to: write a custom retriever class How to: write a custom document loader How to: write a custom output parser class How to: create custom callback handlers How to: define a custom tool How to: dispatch custom callback events Serializationâ€‹ How to: save and load LangChain objects Use casesâ€‹ These guides cover use-case specific details. Q&A with RAGâ€‹ Retrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data. For a high-level tutorial on RAG, check out this guide. How to: add chat history How to: stream How to: return sources How to: return citations How to: do per-user retrieval Extractionâ€‹ Extraction is when you use LLMs to extract structured information from unstructured text. For a high level tutorial on extraction, check out this guide. How to: use reference examples How to: handle long text How to: do extraction without using function calling Chatbotsâ€‹ Chatbots involve using an LLM to have a conversation. For a high-level tutorial on building chatbots, check out this guide. How to: manage memory How to: do retrieval How to: use tools How to: manage large chat history Query analysisâ€‹ Query Analysis is the task of using an LLM to generate a query to send to a retriever. For a high-level tutorial on query analysis, check out this guide. How to: add examples to the prompt How to: handle cases where no queries are generated How to: handle multiple queries How to: handle multiple retrievers How to: construct filters How to: deal with high cardinality categorical variables Q&A over SQL + CSVâ€‹ You can use LLMs to do question answering over tabular data. For a high-level tutorial, check out this guide. How to: use prompting to improve results How to: do query validation How to: deal with large databases How to: deal with CSV files Q&A over graph databasesâ€‹ You can use an LLM to do question answering over graph databases. For a high-level tutorial, check out this guide. How to: add a semantic layer over a database How to: construct knowledge graphs Summarizationâ€‹ LLMs can summarize and otherwise distill desired information from text, including large volumes of text. For a high-level tutorial, check out this guide. How to: summarize text in a single LLM call How to: summarize text through parallelization How to: summarize text through iterative refinement LangChain Expression Language (LCEL)â€‹ Should I use LCEL?LCEL is an orchestration solution. See our concepts page for recommendations on when to use LCEL. LangChain Expression Language is a way to create arbitrary custom chains. It is built on the Runnable protocol. LCEL cheatsheet: For a quick overview of how to use the main LCEL primitives. Migration guide: For migrating legacy chain abstractions to LCEL. How to: chain runnables How to: stream runnables How to: invoke runnables in parallel How to: add default invocation args to runnables How to: turn any function into a runnable How to: pass through inputs from one chain step to the next How to: configure runnable behavior at runtime How to: add message history (memory) to a chain How to: route between sub-chains How to: create a dynamic (self-constructing) chain How to: inspect runnables How to: add fallbacks to a runnable How to: pass runtime secrets to a runnable LangGraphâ€‹ LangGraph is an extension of LangChain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. LangGraph documentation is currently hosted on a separate site. You can find the LangGraph guides here. LangSmithâ€‹ LangSmith allows you to closely trace, monitor and evaluate your LLM application. It seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build. LangSmith documentation is hosted on a separate site. You can peruse LangSmith how-to guides here, but we'll highlight a few sections that are particularly relevant to LangChain below: Evaluationâ€‹ Evaluating performance is a vital part of building LLM-powered applications. LangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators. To learn more, check out the LangSmith evaluation how-to guides. Tracingâ€‹ Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues. How to: trace with LangChain How to: add metadata and tags to traces You can see general tracing-related how-tos in this section of the LangSmith docs.InstallationKey featuresComponentsChat modelsMessagesPrompt templatesExample selectorsLLMsOutput parsersDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingToolsMultimodalAgentsCallbacksCustomSerializationUse casesQ&A with RAGExtractionChatbotsQuery analysisQ&A over SQL + CSVQ&A over graph databasesSummarizationLangChain Expression Language (LCEL)LangGraphLangSmithEvaluationTracing",
      "timestamp": "2025-08-24 15:28:47"
    },
    {
      "url": "https://python.langchain.com/docs/concepts/",
      "title": "Conceptual guide | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Conceptual guide | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly. We recommend that you go through at least one of the Tutorials before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here. The conceptual guide does not cover step-by-step instructions or specific implementation examples â€” those are found in the How-to guides and Tutorials. For detailed reference material, please see the API reference. High levelâ€‹ Why LangChain?: Overview of the value that LangChain provides. Architecture: How packages are organized in the LangChain ecosystem. Conceptsâ€‹ Chat models: LLMs exposed via a chat API that process sequences of messages as input and output a message. Messages: The unit of communication in chat models, used to represent model input and output. Chat history: A conversation represented as a sequence of messages, alternating between user messages and model responses. Tools: A function with an associated schema defining the function's name, description, and the arguments it accepts. Tool calling: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message. Structured output: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema. Memory: Information about a conversation that is persisted so that it can be used in future conversations. Multimodality: The ability to work with data that comes in different forms, such as text, audio, images, and video. Runnable interface: The base abstraction that many LangChain components and the LangChain Expression Language are built on. Streaming: LangChain streaming APIs for surfacing results as they are generated. LangChain Expression Language (LCEL): A syntax for orchestrating LangChain components. Most useful for simpler applications. Document loaders: Load a source as a list of documents. Retrieval: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query. Text splitters: Split long text into smaller chunks that can be individually indexed to enable granular retrieval. Embedding models: Models that represent data such as text or images in a vector space. Vector stores: Storage of and efficient search over vectors and associated metadata. Retriever: A component that returns relevant documents from a knowledge base in response to a query. Retrieval Augmented Generation (RAG): A technique that enhances language models by combining them with external knowledge bases. Agents: Use a language model to choose a sequence of actions to take. Agents can interact with external resources via tool. Prompt templates: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts. Output parsers: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of tool calling and structured outputs. Few-shot prompting: A technique for improving model performance by providing a few examples of the task to perform in the prompt. Example selectors: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt. Async programming: The basics that one should know to use LangChain in an asynchronous context. Callbacks: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more. Tracing: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications. Evaluation: The process of assessing the performance and effectiveness of AI applications. This involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications. Testing: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs. Glossaryâ€‹ AIMessageChunk: A partial response from an AI message. Used when streaming responses from a chat model. AIMessage: Represents a complete response from an AI model. astream_events: Stream granular information from LCEL chains. BaseTool: The base class for all tools in LangChain. batch: Use to execute a runnable with batch inputs. bind_tools: Allows models to interact with tools. Caching: Storing results to avoid redundant calls to a chat model. Chat models: Chat models that handle multiple data modalities. Configurable runnables: Creating configurable Runnables. Context window: The maximum size of input a chat model can process. Conversation patterns: Common patterns in chat interactions. Document: LangChain's representation of a document. Embedding models: Models that generate vector embeddings for various data types. HumanMessage: Represents a message from a human user. InjectedState: A state injected into a tool function. InjectedStore: A store that can be injected into a tool for data persistence. InjectedToolArg: Mechanism to inject arguments into tool functions. input and output types: Types used for input and output in Runnables. Integration packages: Third-party packages that integrate with LangChain. Integration tests: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration. invoke: A standard method to invoke a Runnable. JSON mode: Returning responses in JSON format. langchain-community: Community-driven components for LangChain. langchain-core: Core langchain package. Includes base interfaces and in-memory implementations. langchain: A package for higher level components (e.g., some pre-built chains). langgraph: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows. langserve: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph. LLMs (legacy): Older language models that take a string as input and return a string as output. Managing chat history: Techniques to maintain and manage the chat history. OpenAI format: OpenAI's message format for chat models. Propagation of RunnableConfig: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async. rate-limiting: Client side rate limiting for chat models. RemoveMessage: An abstraction used to remove a message from chat history, used primarily in LangGraph. role: Represents the role (e.g., user, assistant) of a chat message. RunnableConfig: Use to pass run time information to Runnables (e.g., run_name, run_id, tags, metadata, max_concurrency, recursion_limit, configurable). Standard parameters for chat models: Parameters such as API key, temperature, and max_tokens. Standard tests: A defined set of unit and integration tests that all integrations must pass. stream: Use to stream output from a Runnable or a graph. Tokenization: The process of converting data into tokens and vice versa. Tokens: The basic unit that a language model reads, processes, and generates under the hood. Tool artifacts: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing. Tool binding: Binding tools to models. @tool: Decorator for creating tools in LangChain. Toolkits: A collection of tools that can be used together. ToolMessage: Represents a message that contains the results of a tool execution. Unit tests: Tests that verify the correctness of individual components, run in isolation without access to the Internet. Vector stores: Datastores specialized for storing and efficiently searching vector embeddings. with_structured_output: A helper method for chat models that natively support tool calling to get structured output matching a given schema specified via Pydantic, JSON schema or a function. with_types: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe. High levelConceptsGlossary",
      "timestamp": "2025-08-24 15:28:50"
    },
    {
      "url": "https://python.langchain.com/docs/integrations/vectorstores/",
      "title": "Vector stores | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Vector stores | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page A vector store stores embedded data and performs similarity search. Select embedding model: Select embeddings model:OpenAIâ–¾OpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakepip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"): os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") Select vector store: Select vector store:In-memoryâ–¾In-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings) VectorstoreDelete by IDFilteringSearch by VectorSearch with scoreAsyncPasses Standard TestsMulti TenancyIDs in add DocumentsAstraDBVectorStoreâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…Chromaâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…Clickhouseâœ…âœ…âŒâœ…âŒâŒâŒâœ…CouchbaseSearchVectorStoreâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…DatabricksVectorSearchâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…ElasticsearchStoreâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…FAISSâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…InMemoryVectorStoreâœ…âœ…âŒâœ…âœ…âŒâŒâœ…Milvusâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…MongoDBAtlasVectorSearchâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…openGaussâœ…âœ…âœ…âœ…âŒâœ…âŒâœ…PGVectorâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…PGVectorStoreâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…PineconeVectorStoreâœ…âœ…âœ…âŒâœ…âŒâŒâœ…QdrantVectorStoreâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…Redisâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…Weaviateâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…SQLServerâœ…âœ…âœ…âœ…âŒâŒâŒâœ… All Vectorstoresâ€‹ NameDescriptionActiveloop Deep LakeActiveloop Deep Lake as a Multi-Modal Vector Store that stores embedd...AerospikeAerospike Vector Search (AVS) is anAlibaba Cloud OpenSearchAlibaba Cloud Opensearch is a one-stop platform to develop intelligen...AnalyticDBAnalyticDB for PostgreSQL is a massively parallel processing (MPP) da...AnnoyAnnoy (Approximate Nearest Neighbors Oh Yeah) is a C++ library with P...Apache DorisApache Doris is a modern data warehouse for real-time analytics.ApertureDBApertureDB is a database that stores, indexes, and manages multi-moda...Astra DB Vector StoreThis page provides a quickstart for using Astra DB as a Vector Store.AtlasAtlas is a platform by Nomic made for interacting with both small and...AwaDBAwaDB is an AI Native database for the search and storage of embeddin...Azure Cosmos DB Mongo vCoreThis notebook shows you how to leverage this integrated vector databa...Azure Cosmos DB No SQLThis notebook shows you how to leverage this integrated vector databa...Azure AI SearchAzure AI Search (formerly known as Azure Search and Azure Cognitive S...BagelBagel (Open Inference platform for AI), is like GitHub for AI data.BagelDBBagelDB (Open Vector Database for AI), is like GitHub for AI data.Baidu Cloud ElasticSearch VectorSearchBaidu Cloud VectorSearch is a fully managed, enterprise-level distrib...Baidu VectorDBBaidu VectorDB is a robust, enterprise-level distributed database ser...Apache CassandraThis page provides a quickstart for using Apache CassandraÂ® as a Vect...ChromaThis notebook covers how to get started with the Chroma vector store.ClarifaiClarifai is an AI Platform that provides the full AI lifecycle rangin...ClickHouseClickHouse is the fastest and most resource efficient open-source dat...CloudflareVectorizeThis notebook covers how to get started with the CloudflareVectorize ...CouchbaseCouchbase is an award-winning distributed NoSQL cloud database that d...DashVectorDashVector is a fully-managed vectorDB service that supports high-dim...DatabricksDatabricks Vector Search is a serverless similarity search engine tha...IBM Db2 Vector Store and Vector SearchLangChain's Db2 integration (langchain-db2) provides vector store and...DingoDBDingoDB is a distributed multi-mode vector database, which combines t...DocArray HnswSearchDocArrayHnswSearch is a lightweight Document Index implementation pro...DocArray InMemorySearchDocArrayInMemorySearch is a document index provided by Docarray that ...Amazon Document DBAmazon DocumentDB (with MongoDB Compatibility) makes it easy to set u...DuckDBThis notebook shows how to use DuckDB as a vector store.China Mobile ECloud ElasticSearch VectorSearchChina Mobile ECloud VectorSearch is a fully managed, enterprise-level...ElasticsearchElasticsearch is a distributed, RESTful search and analytics engine, ...EpsillaEpsilla is an open-source vector database that leverages the advanced...FaissFacebook AI Similarity Search (FAISS) is a library for efficient simi...Faiss (Async)Facebook AI Similarity Search (Faiss) is a library for efficient simi...FalkorDBVectorStoreFalkorDB is an open-source graph database with integrated support for...GelAn implementation of LangChain vectorstore abstraction using gel as t...Google AlloyDB for PostgreSQLAlloyDB is a fully managed relational database service that offers hi...Google BigQuery Vector SearchGoogle Cloud BigQuery Vector Search lets you use GoogleSQL to do sema...Google Cloud SQL for MySQLCloud SQL is a fully managed relational database service that offers ...Google Cloud SQL for PostgreSQLCloud SQL is a fully managed relational database service that offers ...FirestoreFirestore is a serverless document-oriented database that scales to m...Google Memorystore for RedisGoogle Memorystore for Redis is a fully-managed service that is power...Google SpannerSpanner is a highly scalable database that combines unlimited scalabi...Google Vertex AI Feature StoreGoogle Cloud Vertex Feature Store streamlines your ML feature managem...Google Vertex AI Vector SearchThis notebook shows how to use functionality related to the Google Cl...HippoTranswarp Hippo is an enterprise-level cloud-native distributed vecto...HologresHologres is a unified real-time data warehousing service developed by...InfinispanInfinispan is an open-source key-value data grid, it can work as sing...Jaguar Vector Database1. It is a distributed vector databaseKDB.AIKDB.AI is a powerful knowledge-based vector database and search engin...KineticaKinetica is a database with integrated support for vector similarity ...LanceDBLanceDB is an open-source database for vector-search built with persi...LanternLantern is an open-source vector similarity search for PostgresLindormThis notebook covers how to get started with the Lindorm vector store.LLMRailsLLMRails is a API platform for building GenAI applications. It provid...ManticoreSearch VectorStoreManticoreSearch is an open-source search engine that offers fast, sca...MariaDBLangChain's MariaDB integration (langchain-mariadb) provides vector c...MarqoThis notebook shows how to use functionality related to the Marqo vec...MeilisearchMeilisearch is an open-source, lightning-fast, and hyper relevant sea...Amazon MemoryDBVector Search introduction and langchain integration guide.MilvusMilvus is a database that stores, indexes, and manages massive embedd...Momento Vector Index (MVI)MVI: the most productive, easiest to use, serverless vector index for...MongoDB AtlasThis notebook covers how to MongoDB Atlas vector search in LangChain,...MyScaleMyScale is a cloud-based database optimized for AI applications and s...Neo4j Vector IndexNeo4j is an open-source graph database with integrated support for ve...NucliaDBYou can use a local NucliaDB instance or use Nuclia Cloud.OceanbaseThis notebook covers how to get started with the Oceanbase vector sto...openGaussThis notebook covers how to get started with the openGauss VectorStor...OpenSearchOpenSearch is a scalable, flexible, and extensible open-source softwa...Oracle AI Vector Search: Vector StoreOracle AI Vector Search is designed for Artificial Intelligence (AI) ...PathwayPathway is an open data processing framework. It allows you to easily...Postgres EmbeddingPostgres Embedding is an open-source vector similarity search for Pos...PGVecto.rsThis notebook shows how to use functionality related to the Postgres ...PGVectorAn implementation of LangChain vectorstore abstraction using postgres...PGVectorStorePGVectorStore is an implementation of a LangChain vectorstore using p...PineconePinecone is a vector database with broad functionality.Pinecone (sparse)Pinecone is a vector database with broad functionality.QdrantQdrant (read: quadrant) is a vector similarity search engine. It prov...RedisThis notebook covers how to get started with the Redis vector store.RelytRelyt is a cloud native data warehousing service that is designed to ...RocksetRockset is a real-time search and analytics database built for the cl...SAP HANA Cloud Vector EngineSAP HANA Cloud Vector Engine is a vector store fully integrated into ...ScaNNScaNN (Scalable Nearest Neighbors) is a method for efficient vector s...SemaDBSemaDB from SemaFind is a no fuss vector similarity database for buil...SingleStoreSingleStore is a robust, high-performance distributed SQL database so...scikit-learnscikit-learn is an open-source collection of machine learning algorit...SQLiteVecThis notebook covers how to get started with the SQLiteVec vector sto...SQLite-VSSSQLite-VSS is an SQLite extension designed for vector search, emphasi...SQLServerAzure SQL provides a dedicatedÂ Vector data type that simplifies the c...StarRocksStarRocks is a High-Performance Analytical Database.Supabase (Postgres)Supabase is an open-source Firebase alternative. Supabase is built on...SurrealDBVectorStoreSurrealDB is a unified, multi-model database purpose-built for AI sys...TablestoreTablestore is a fully managed NoSQL cloud database service.TairTair is a cloud native in-memory database service developed by Alibab...Tencent Cloud VectorDBTencent Cloud VectorDB is a fully managed, self-developed, enterprise...ThirdAI NeuralDBNeuralDB is a CPU-friendly and fine-tunable vector store developed by...TiDB VectorTiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution...TigrisTigris is an open-source Serverless NoSQL Database and Search Platfor...TileDBTileDB is a powerful engine for indexing and querying dense and spars...Timescale Vector (Postgres)Timescale Vector is PostgreSQL++ vector database for AI applications.TypesenseTypesense is an open-source, in-memory search engine, that you can ei...Upstash VectorUpstash Vector is a serverless vector database designed for working w...USearchUSearch is a Smaller & Faster Single-File Vector Search EngineValdVald is a highly scalable distributed fast approximate nearest neighb...VDMSThis notebook covers how to get started with VDMS as a vector store.VearchVearch is the vector search infrastructure for deeping learning and A...VectaraVectara is the trusted AI Assistant and Agent platform which focuses ...VespaVespa is a fully featured search engine and vector database. It suppo...viking DBviking DB is a database that stores, indexes, and manages massive emb...vliteVLite is a simple and blazing fast vector database that allows you to...WeaviateThis notebook covers how to get started with the Weaviate vector stor...XataXata is a serverless data platform, based on PostgreSQL. It provides ...YDBYDB is a versatile open source Distributed SQL Database that combines...YellowbrickYellowbrick is an elastic, massively parallel processing (MPP) SQL da...ZepRecall, understand, and extract data from chat histories. Power perso...Zep CloudRecall, understand, and extract data from chat histories. Power perso...ZillizZilliz Cloud is a fully managed service on cloud for LF AI MilvusÂ®,All Vectorstores",
      "timestamp": "2025-08-24 15:28:54"
    },
    {
      "url": "https://python.langchain.com/docs/integrations/providers/",
      "title": "Providers | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Providers | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page infoIf you'd like to write your own integration, see Extending LangChain. If you'd like to contribute an integration, see Contributing integrations. Integration Packagesâ€‹ These providers have standalone langchain-{provider} packages for improved versioning, dependency management and testing. ProviderPackageDownloadsLatestJSOpenAIlangchain-openaiâœ…Google VertexAIlangchain-google-vertexaiâœ…AWSlangchain-awsâœ…Anthropiclangchain-anthropicâœ…Google Generative AIlangchain-google-genaiâœ…Google Communitylangchain-google-communityâŒOllamalangchain-ollamaâœ…Groqlangchain-groqâœ…Chromalangchain-chromaâœ…Coherelangchain-cohereâœ…Huggingfacelangchain-huggingfaceâœ…MistralAIlangchain-mistralaiâœ…Postgreslangchain-postgresâŒPineconelangchain-pineconeâœ…Deepseeklangchain-deepseekâœ…Perplexitylangchain-perplexityâœ…Nvidia AI Endpointslangchain-nvidia-ai-endpointsâŒIbmlangchain-ibmâœ…Milvuslangchain-milvusâŒMongoDBlangchain-mongodbâœ…Databricksdatabricks-langchainâŒQdrantlangchain-qdrantâœ…Fireworkslangchain-fireworksâœ…Tavilylangchain-tavilyâœ…Elasticsearchlangchain-elasticsearchâœ…Unstructuredlangchain-unstructuredâŒDataStax Astra DBlangchain-astradbâœ…Neo4Jlangchain-neo4jâœ…Togetherlangchain-togetherâœ…Redislangchain-redisâœ…XAIlangchain-xaiâœ…Sambanovalangchain-sambanovaâŒGraph RAGlangchain-graph-retrieverâŒAzure AIlangchain-azure-aiâœ…Weaviatelangchain-weaviateâœ…VoyageAIlangchain-voyageaiâŒCerebraslangchain-cerebrasâœ…Litellmlangchain-litellmâŒDoclinglangchain-doclingâŒUpstagelangchain-upstageâŒNomiclangchain-nomicâœ…Azure Dynamic Sessionslangchain-azure-dynamic-sessionsâœ…Pymupdf4Llmlangchain-pymupdf4llmâŒAI21langchain-ai21âŒExalangchain-exaâœ…Predictionguardlangchain-predictionguardâŒWriterlangchain-writerâŒMemgraphlangchain-memgraphâŒQwqlangchain-qwqâŒDB2langchain-db2âŒPromptylangchain-promptyâŒSqlserverlangchain-sqlserverâŒHyperbrowserlangchain-hyperbrowserâŒApifylangchain-apifyâŒLangFairlangfairâŒVDMSlangchain-vdmsâŒNaverlangchain-naverâŒMariaDBlangchain-mariadbâŒSAP HANA Cloudlangchain-hanaâŒScrapegraphlangchain-scrapegraphâŒSema4langchain-sema4âŒSnowflakelangchain-snowflakeâŒCouchbaselangchain-couchbaseâŒLinkuplangchain-linkupâŒADS4GPTsads4gpts-langchainâŒLocalAIlangchain-localaiâŒSalesforcelangchain-salesforceâŒDeeplakelangchain-deeplakeâŒProloglangchain-prologâŒTableaulangchain-tableauâŒContextual AIlangchain-contextualâŒCloudflarelangchain-cloudflareâœ…YDBlangchain-ydbâŒValyulangchain-valyuâŒBoxlangchain-boxâŒTilorestilores-langchainâŒRecalliolangchain-recallioâŒSurrealDBlangchain-surrealdbâŒKuzulangchain-kuzuâŒNaverlangchain-naver-communityâŒBrightdatalangchain-brightdataâŒDigitalOcean Gradientlangchain-gradientâŒAgentqllangchain-agentqlâŒNebiuslangchain-nebiusâŒNimblelangchain-nimbleâŒDappierlangchain-dappierâŒRunPodlangchain-runpodâŒGoodfirelangchain-goodfireâŒTaigalangchain-taigaâŒXinferencelangchain-xinferenceâŒJenkinslangchain-jenkinsâŒGOAT SDKgoat-sdk-adapter-langchainâŒOxylabslangchain-oxylabsâŒCrateDBlangchain-cratedbâŒCogneelangchain-cogneeâŒAbsolangchain-absoâŒValtheralangchain-valtheraâŒFeatherless AIlangchain-featherless-aiâŒOceanbaselangchain-oceanbaseâŒPull Mdlangchain-pull-mdâŒGalaxia Retrieverlangchain-galaxia-retrieverâŒGellangchain-gelâŒVectaralangchain-vectaraâŒOpengradientlangchain-opengradientâŒPermitlangchain-permitâŒFalkorDBlangchain-falkordbâŒDiscord (Shikenso)langchain-discord-shikensoâŒSingleStorelangchain-singlestoreâŒGreennodelangchain-greennodeâŒModelscopelangchain-modelscopeâŒPipeshiftlangchain-pipeshiftâŒFmp Datalangchain-fmp-dataâŒNetmindlangchain-netmindâŒZoterolangchain-zotero-retrieverâŒAerospikelangchain-aerospikeâŒPowerScale RAG Connectorpowerscale-rag-connectorâŒLindormlangchain-lindorm-integrationâŒPayman Toollangchain-payman-toolâŒTensorlakelangchain-tensorlakeâŒAnchorbrowserlangchain-anchorbrowserâŒMCP Toolboxtoolbox-langchainâŒScrapelesslangchain-scrapelessâŒ All Providersâ€‹ Click here to see all providers. Or search for a provider using the Search field in the top-right corner of the screen.Integration PackagesAll Providers",
      "timestamp": "2025-08-24 15:28:58"
    },
    {
      "url": "https://python.langchain.com/docs/contributing/",
      "title": "Welcome Contributors | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Welcome Contributors | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Hi there! Thank you for your interest in contributing to LangChain. As an open-source project in a fast developing field, we are extremely open to contributions, whether they involve new features, improved infrastructure, better documentation, or bug fixes. Tutorialsâ€‹ More coming soon! We are working on tutorials to help you make your first contribution to the project. Make your first docs PR How-to guidesâ€‹ Documentation: Help improve our docs, including this one! Code: Help us write code, fix bugs, or improve our infrastructure. Integrations: Help us integrate with your favorite vendors and tools. Standard Tests: Ensure your integration passes an expected set of tests. Referenceâ€‹ Repository Structure: Understand the high level structure of the repository. Review Process: Learn about the review process for pull requests. Frequently Asked Questions (FAQ): Get answers to common questions about contributing. Communityâ€‹ ğŸ’­ Forumâ€‹ We have a LangChain Forum where users can ask usage questions, discuss design decisions, and propose new features. If you are able to help answer questions, please do so! This will allow the maintainers to spend more time focused on development and bug fixing. ğŸš© GitHub Issuesâ€‹ Our issues page is kept up to date with bugs, docs improvements, and triaged feature requests that are being worked on. There is a taxonomy of labels to help with sorting and discovery of issues of interest. Please use these to help organize issues. Check out the Help Wanted and Good First Issue tags for recommendations. If you start working on an issue, please assign it to yourself. If you are adding an issue, please try to keep it focused on a single, modular bug/improvement/feature. If two issues are related, or blocking, please link them rather than combining them. We will try to keep these issues as up-to-date as possible, though with the rapid rate of development in this field some may get out of date. If you notice this happening, please let us know. ğŸ“¢ Community Slackâ€‹ We have a community slack where you can ask questions, get help, and discuss the project with other contributors and users. ğŸ™‹ Getting Helpâ€‹ Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please ask in community slack or open a forum post. In a similar vein, we do enforce certain linting, formatting, and documentation standards in the codebase. If you are finding these difficult (or even just annoying) to work with, feel free to ask in community slack!TutorialsHow-to guidesReferenceCommunityğŸ’­ ForumğŸš© GitHub IssuesğŸ“¢ Community SlackğŸ™‹ Getting Help",
      "timestamp": "2025-08-24 15:29:01"
    },
    {
      "url": "https://python.langchain.com/docs/concepts/architecture/",
      "title": "Architecture | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Architecture | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page LangChain is a framework that consists of a number of packages. langchain-coreâ€‹ This package contains base abstractions for different components and ways to compose them together. The interfaces for core components like chat models, vector stores, tools and more are defined here. No third-party integrations are defined here. The dependencies are kept purposefully very lightweight. langchainâ€‹ The main langchain package contains chains and retrieval strategies that make up an application's cognitive architecture. These are NOT third-party integrations. All chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations. Integration packagesâ€‹ Popular integrations have their own packages (e.g. langchain-openai, langchain-anthropic, etc) so that they can be properly versioned and appropriately lightweight. For more information see: A list integrations packages The API Reference where you can find detailed information about each of the integration package. langchain-communityâ€‹ This package contains third-party integrations that are maintained by the LangChain community. Key integration packages are separated out (see above). This contains integrations for various components (chat models, vector stores, tools, etc). All dependencies in this package are optional to keep the package as lightweight as possible. langgraphâ€‹ langgraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. LangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows. Further reading See our LangGraph overview here. See our LangGraph Academy Course here. langserveâ€‹ A package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running. importantLangServe is designed to primarily deploy simple Runnables and work with well-known primitives in langchain-core.If you need a deployment option for LangGraph, you should instead be looking at LangGraph Platform (beta) which will be better suited for deploying LangGraph applications. For more information, see the LangServe documentation. LangSmithâ€‹ A developer platform that lets you debug, test, evaluate, and monitor LLM applications. For more information, see the LangSmith documentationlangchain-corelangchainIntegration packageslangchain-communitylanggraphlangserveLangSmith",
      "timestamp": "2025-08-24 15:29:05"
    },
    {
      "url": "https://python.langchain.com/docs/tutorials/chatbot/",
      "title": "Build a Chatbot | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Build a Chatbot | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page noteThis tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.Please see How to migrate to LangGraph Memory for more details. Overviewâ€‹ We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions with a chat model. Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for: Conversational RAG: Enable a chatbot experience over an external source of data Agents: Build a chatbot that can take actions This tutorial will cover the basics which will be helpful for those two more advanced topics, but feel free to skip directly to there should you choose. Setupâ€‹ Jupyter Notebookâ€‹ This guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them. This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install. Installationâ€‹ For this tutorial we will need langchain-core and langgraph. This guide requires langgraph >= 0.2.28. PipCondapip install langchain-core langgraph>0.2.27conda install langchain-core langgraph>0.2.27 -c conda-forge For more details, see our Installation guide. LangSmithâ€‹ Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. After you sign up at the link above, (you'll need to create an API key from the Settings -> API Keys page on the LangSmith website), make sure to set your environment variables to start logging traces: export LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\" Or, if in a notebook, you can set them with: import getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass() Quickstartâ€‹ First up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably - select the one you want to use below! Select chat model:Google Geminiâ–¾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") Let's first use the model directly. ChatModels are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the .invoke method. from langchain_core.messages import HumanMessagemodel.invoke([HumanMessage(content=\"Hi! I'm Bob\")])API Reference:HumanMessage AIMessage(content='Hi Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-5211544f-da9f-4325-8b8e-b3d92b2fc71a-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}) The model on its own does not have any concept of state. For example, if you ask a followup question: model.invoke([HumanMessage(content=\"What's my name?\")]) AIMessage(content=\"I'm sorry, but I don't have access to personal information about users unless it has been shared with me in the course of our conversation. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 11, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-a2d13a18-7022-4784-b54f-f85c097d1075-0', usage_metadata={'input_tokens': 11, 'output_tokens': 34, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}) Let's take a look at the example LangSmith trace We can see that it doesn't take the previous conversation turn into context, and cannot answer the question. This makes for a terrible chatbot experience! To get around this, we need to pass the entire conversation history into the model. Let's see what happens when we do that: from langchain_core.messages import AIMessagemodel.invoke( [ HumanMessage(content=\"Hi! I'm Bob\"), AIMessage(content=\"Hello Bob! How can I assist you today?\"), HumanMessage(content=\"What's my name?\"), ])API Reference:AIMessage AIMessage(content='Your name is Bob! How can I help you today, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 33, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-34bcccb3-446e-42f2-b1de-52c09936c02c-0', usage_metadata={'input_tokens': 33, 'output_tokens': 14, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}) And now we can see that we get a good response! This is the basic idea underpinning a chatbot's ability to interact conversationally. So how do we best implement this? Message persistenceâ€‹ LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns. Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications. LangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres). from langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import START, MessagesState, StateGraph# Define a new graphworkflow = StateGraph(state_schema=MessagesState)# Define the function that calls the modeldef call_model(state: MessagesState): response = model.invoke(state[\"messages\"]) return {\"messages\": response}# Define the (single) node in the graphworkflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)# Add memorymemory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:MemorySaver | StateGraph We now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a thread_id. This should look like: config = {\"configurable\": {\"thread_id\": \"abc123\"}} This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users. We can then invoke the application: query = \"Hi! I'm Bob.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print() # output contains all messages in state ==================================\u001b[1m Ai Message \u001b[0m==================================Hi Bob! How can I assist you today? query = \"What's my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print() ==================================\u001b[1m Ai Message \u001b[0m==================================Your name is Bob! How can I help you today, Bob? Great! Our chatbot now remembers things about us. If we change the config to reference a different thread_id, we can see that it starts the conversation fresh. config = {\"configurable\": {\"thread_id\": \"abc234\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print() ==================================\u001b[1m Ai Message \u001b[0m==================================I'm sorry, but I don't have access to personal information about you unless you've shared it in this conversation. How can I assist you today? However, we can always go back to the original conversation (since we are persisting it in a database) config = {\"configurable\": {\"thread_id\": \"abc123\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print() ==================================\u001b[1m Ai Message \u001b[0m==================================Your name is Bob. What would you like to discuss today? This is how we can support a chatbot having conversations with many users! tipFor async support, update the call_model node to be an async function and use .ainvoke when invoking the application:# Async function for node:async def call_model(state: MessagesState): response = await model.ainvoke(state[\"messages\"]) return {\"messages\": response}# Define graph as before:workflow = StateGraph(state_schema=MessagesState)workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)app = workflow.compile(checkpointer=MemorySaver())# Async invocation:output = await app.ainvoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print() Right now, all we've done is add a simple persistence layer around the model. We can start to make the chatbot more complicated and personalized by adding in a prompt template. Prompt templatesâ€‹ Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages. To add in a system message, we will create a ChatPromptTemplate. We will utilize MessagesPlaceholder to pass all the messages in. from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderprompt_template = ChatPromptTemplate.from_messages( [ ( \"system\", \"You talk like a pirate. Answer all questions to the best of your ability.\", ), MessagesPlaceholder(variable_name=\"messages\"), ])API Reference:ChatPromptTemplate | MessagesPlaceholder We can now update our application to incorporate this template: workflow = StateGraph(state_schema=MessagesState)def call_model(state: MessagesState): prompt = prompt_template.invoke(state) response = model.invoke(prompt) return {\"messages\": response}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory) We invoke the application in the same way: config = {\"configurable\": {\"thread_id\": \"abc345\"}}query = \"Hi! I'm Jim.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print() ==================================\u001b[1m Ai Message \u001b[0m==================================Ahoy there, Jim! What brings ye to these waters today? Be ye seekin' treasure, knowledge, or perhaps a good tale from the high seas? Arrr! query = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print() ==================================\u001b[1m Ai Message \u001b[0m==================================Ye be called Jim, matey! A fine name fer a swashbuckler such as yerself! What else can I do fer ye? Arrr! Awesome! Let's now make our prompt a little bit more complicated. Let's assume that the prompt template now looks something like this: prompt_template = ChatPromptTemplate.from_messages( [ ( \"system\", \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\", ), MessagesPlaceholder(variable_name=\"messages\"), ]) Note that we have added a new language input to the prompt. Our application now has two parameters-- the input messages and language. We should update our application's state to reflect this: from typing import Sequencefrom langchain_core.messages import BaseMessagefrom langgraph.graph.message import add_messagesfrom typing_extensions import Annotated, TypedDictclass State(TypedDict): messages: Annotated[Sequence[BaseMessage], add_messages] language: strworkflow = StateGraph(state_schema=State)def call_model(state: State): prompt = prompt_template.invoke(state) response = model.invoke(prompt) return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:BaseMessage | add_messages config = {\"configurable\": {\"thread_id\": \"abc456\"}}query = \"Hi! I'm Bob.\"language = \"Spanish\"input_messages = [HumanMessage(query)]output = app.invoke( {\"messages\": input_messages, \"language\": language}, config,)output[\"messages\"][-1].pretty_print() ==================================\u001b[1m Ai Message \u001b[0m==================================Â¡Hola, Bob! Â¿CÃ³mo puedo ayudarte hoy? Note that the entire state is persisted, so we can omit parameters like language if no changes are desired: query = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke( {\"messages\": input_messages}, config,)output[\"messages\"][-1].pretty_print() ==================================\u001b[1m Ai Message \u001b[0m==================================Tu nombre es Bob. Â¿Hay algo mÃ¡s en lo que pueda ayudarte? To help you understand what's happening internally, check out this LangSmith trace. Managing Conversation Historyâ€‹ One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in. Importantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History. We can do this by adding a simple step in front of the prompt that modifies the messages key appropriately, and then wrap that new chain in the Message History class. LangChain comes with a few built-in helpers for managing a list of messages. In this case we'll use the trim_messages helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages: from langchain_core.messages import SystemMessage, trim_messagestrimmer = trim_messages( max_tokens=65, strategy=\"last\", token_counter=model, include_system=True, allow_partial=False, start_on=\"human\",)messages = [ SystemMessage(content=\"you're a good assistant\"), HumanMessage(content=\"hi! I'm bob\"), AIMessage(content=\"hi!\"), HumanMessage(content=\"I like vanilla ice cream\"), AIMessage(content=\"nice\"), HumanMessage(content=\"whats 2 + 2\"), AIMessage(content=\"4\"), HumanMessage(content=\"thanks\"), AIMessage(content=\"no problem!\"), HumanMessage(content=\"having fun?\"), AIMessage(content=\"yes!\"),]trimmer.invoke(messages)API Reference:SystemMessage | trim_messages [SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}), HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}), AIMessage(content='4', additional_kwargs={}, response_metadata={}), HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}), AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}), HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}), AIMessage(content='yes!', additional_kwargs={}, response_metadata={})] To use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt. workflow = StateGraph(state_schema=State)def call_model(state: State): print(f\"Messages before trimming: {len(state['messages'])}\") trimmed_messages = trimmer.invoke(state[\"messages\"]) print(f\"Messages after trimming: {len(trimmed_messages)}\") print(\"Remaining messages:\") for msg in trimmed_messages: print(f\" {type(msg).__name__}: {msg.content}\") prompt = prompt_template.invoke( {\"messages\": trimmed_messages, \"language\": state[\"language\"]} ) response = model.invoke(prompt) return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory) Now if we try asking the model our name, it won't know it since we trimmed that part of the chat history. (By defining our trim stragegy as 'last', we are only keeping the most recent messages that fit within the max_tokens.) config = {\"configurable\": {\"thread_id\": \"abc567\"}}query = \"What is my name?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke( {\"messages\": input_messages, \"language\": language}, config,)output[\"messages\"][-1].pretty_print() Messages before trimming: 12Messages after trimming: 8Remaining messages: SystemMessage: you're a good assistant HumanMessage: whats 2 + 2 AIMessage: 4 HumanMessage: thanks AIMessage: no problem! HumanMessage: having fun? AIMessage: yes! HumanMessage: What is my name?==================================\u001b[1m Ai Message \u001b[0m==================================I don't know your name. If you'd like to share it, feel free! But if we ask about information that is within the last few messages, it remembers: config = {\"configurable\": {\"thread_id\": \"abc678\"}}query = \"What math problem was asked?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke( {\"messages\": input_messages, \"language\": language}, config,)output[\"messages\"][-1].pretty_print() Messages before trimming: 12Messages after trimming: 8Remaining messages: SystemMessage: you're a good assistant HumanMessage: whats 2 + 2 AIMessage: 4 HumanMessage: thanks AIMessage: no problem! HumanMessage: having fun? AIMessage: yes! HumanMessage: What math problem was asked?==================================\u001b[1m Ai Message \u001b[0m==================================The math problem that was asked was \"what's 2 + 2.\" If you take a look at LangSmith, you can see exactly what is happening under the hood in the LangSmith trace. Streamingâ€‹ Now we've got a functioning chatbot. However, one really important UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress. It's actually super easy to do this! By default, .stream in our LangGraph application streams application steps-- in this case, the single step of the model response. Setting stream_mode=\"messages\" allows us to stream output tokens instead: config = {\"configurable\": {\"thread_id\": \"abc789\"}}query = \"Hi I'm Todd, please tell me a joke.\"language = \"English\"input_messages = [HumanMessage(query)]for chunk, metadata in app.stream( {\"messages\": input_messages, \"language\": language}, config, stream_mode=\"messages\",): if isinstance(chunk, AIMessage): # Filter to just model responses print(chunk.content, end=\"|\") |Hi| Todd|!| Here|â€™s| a| joke| for| you|:|Why| don't| scientists| trust| atoms|?|Because| they| make| up| everything|!|| Next Stepsâ€‹ Now that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are: Conversational RAG: Enable a chatbot experience over an external source of data Agents: Build a chatbot that can take actions If you want to dive deeper on specifics, some things worth checking out are: Streaming: streaming is crucial for chat applications How to add message history: for a deeper dive into all things related to message history How to manage large message history: more techniques for managing a large chat history LangGraph main docs: for more detail on building with LangGraph OverviewSetupJupyter NotebookInstallationLangSmithQuickstartMessage persistencePrompt templatesManaging Conversation HistoryStreamingNext Steps",
      "timestamp": "2025-08-24 15:29:09"
    },
    {
      "url": "https://python.langchain.com/docs/versions/v0_3/",
      "title": "LangChain v0.3 | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "LangChain v0.3 | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Last updated: 09.16.24 What's changedâ€‹ All packages have been upgraded from Pydantic 1 to Pydantic 2 internally. Use of Pydantic 2 in user code is fully supported with all packages without the need for bridges like langchain_core.pydantic_v1 or pydantic.v1. Pydantic 1 will no longer be supported as it reached its end-of-life in June 2024. Python 3.8 will no longer be supported as its end-of-life is October 2024. These are the only breaking changes. What's newâ€‹ The following features have been added during the development of 0.2.x: Moved more integrations from langchain-community to their own langchain-x packages. This is a non-breaking change, as the legacy implementations are left in langchain-community and marked as deprecated. This allows us to better manage the dependencies of, test, and version these integrations. You can see all the latest integration packages in the API reference. Simplified tool definition and usage. Read more here. Added utilities for interacting with chat models: universal model constructor, rate limiter, message utilities, Added the ability to dispatch custom events. Revamped integration docs and API reference. Read more here. Marked as deprecated a number of legacy chains and added migration guides for all of them. These are slated for removal in langchain 1.0.0. See the deprecated chains and associated migration guides here. How to update your codeâ€‹ If you're using langchain / langchain-community / langchain-core 0.0 or 0.1, we recommend that you first upgrade to 0.2. If you're using langgraph, upgrade to langgraph>=0.2.20,<0.3. This will work with either 0.2 or 0.3 versions of all the base packages. Here is a complete list of all packages that have been released and what we recommend upgrading your version constraints to. Any package that now requires langchain-core 0.3 had a minor version bump. Any package that is now compatible with both langchain-core 0.2 and 0.3 had a patch version bump. You can use the langchain-cli to update deprecated imports automatically. The CLI will handle updating deprecated imports that were introduced in LangChain 0.0.x and LangChain 0.1, as well as updating the langchain_core.pydantic_v1 and langchain.pydantic_v1 imports. Base packagesâ€‹ PackageLatestRecommended constraintlangchain0.3.0>=0.3,<0.4langchain-community0.3.0>=0.3,<0.4langchain-text-splitters0.3.0>=0.3,<0.4langchain-core0.3.0>=0.3,<0.4langchain-experimental0.3.0>=0.3,<0.4 Downstream packagesâ€‹ PackageLatestRecommended constraintlanggraph0.2.20>=0.2.20,<0.3langserve0.3.0>=0.3,<0.4 Integration packagesâ€‹ PackageLatestRecommended constraintlangchain-ai210.2.0>=0.2,<0.3langchain-aws0.2.0>=0.2,<0.3langchain-anthropic0.2.0>=0.2,<0.3langchain-astradb0.4.1>=0.4.1,<0.5langchain-azure-dynamic-sessions0.2.0>=0.2,<0.3langchain-box0.2.0>=0.2,<0.3langchain-chroma0.1.4>=0.1.4,<0.2langchain-cohere0.3.0>=0.3,<0.4langchain-elasticsearch0.3.0>=0.3,<0.4langchain-exa0.2.0>=0.2,<0.3langchain-fireworks0.2.0>=0.2,<0.3langchain-groq0.2.0>=0.2,<0.3langchain-google-community2.0.0>=2,<3langchain-google-genai2.0.0>=2,<3langchain-google-vertexai2.0.0>=2,<3langchain-huggingface0.1.0>=0.1,<0.2langchain-ibm0.3.0>=0.3,<0.4langchain-milvus0.1.6>=0.1.6,<0.2langchain-mistralai0.2.0>=0.2,<0.3langchain-mongodb0.2.0>=0.2,<0.3langchain-nomic0.1.3>=0.1.3,<0.2langchain-nvidia0.3.0>=0.3,<0.4langchain-ollama0.2.0>=0.2,<0.3langchain-openai0.2.0>=0.2,<0.3langchain-pinecone0.2.0>=0.2,<0.3langchain-postgres0.0.13>=0.0.13,<0.1langchain-prompty0.1.0>=0.1,<0.2langchain-qdrant0.1.4>=0.1.4,<0.2langchain-redis0.1.0>=0.1,<0.2langchain-sema40.2.0>=0.2,<0.3langchain-together0.2.0>=0.2,<0.3langchain-unstructured0.1.4>=0.1.4,<0.2langchain-upstage0.3.0>=0.3,<0.4langchain-voyageai0.2.0>=0.2,<0.3langchain-weaviate0.0.3>=0.0.3,<0.1 Once you've updated to recent versions of the packages, you may need to address the following issues stemming from the internal switch from Pydantic v1 to Pydantic v2: If your code depends on Pydantic aside from LangChain, you will need to upgrade your pydantic version constraints to be pydantic>=2,<3. See Pydantic's migration guide for help migrating your non-LangChain code to Pydantic v2 if you use pydantic v1. There are a number of side effects to LangChain components caused by the internal switch from Pydantic v1 to v2. We have listed some of the common cases below together with the recommended solutions. Common issues when transitioning to Pydantic 2â€‹ 1. Do not use the langchain_core.pydantic_v1 namespaceâ€‹ Replace any usage of langchain_core.pydantic_v1 or langchain.pydantic_v1 with direct imports from pydantic. For example, from langchain_core.pydantic_v1 import BaseModel to: from pydantic import BaseModel This may require you to make additional updates to your Pydantic code given that there are a number of breaking changes in Pydantic 2. See the Pydantic Migration for how to upgrade your code from Pydantic 1 to 2. 2. Passing Pydantic objects to LangChain APIsâ€‹ Users using the following APIs: BaseChatModel.bind_tools BaseChatModel.with_structured_output Tool.from_function StructuredTool.from_function should ensure that they are passing Pydantic 2 objects to these APIs rather than Pydantic 1 objects (created via the pydantic.v1 namespace of pydantic 2). cautionWhile v1 objects may be accepted by some of these APIs, users are advised to use Pydantic 2 objects to avoid future issues. 3. Sub-classing LangChain modelsâ€‹ Any sub-classing from existing LangChain models (e.g., BaseTool, BaseChatModel, LLM) should upgrade to use Pydantic 2 features. For example, any user code that's relying on Pydantic 1 features (e.g., validator) should be updated to the Pydantic 2 equivalent (e.g., field_validator), and any references to pydantic.v1, langchain_core.pydantic_v1, langchain.pydantic_v1 should be replaced with imports from pydantic. from pydantic.v1 import validator, Field # if pydantic 2 is installed# from pydantic import validator, Field # if pydantic 1 is installed# from langchain_core.pydantic_v1 import validator, Field# from langchain.pydantic_v1 import validator, Fieldclass CustomTool(BaseTool): # BaseTool is v1 code x: int = Field(default=1) def _run(*args, **kwargs): return \"hello\" @validator('x') # v1 code @classmethod def validate_x(cls, x: int) -> int: return 1 Should change to: from pydantic import Field, field_validator # pydantic v2from langchain_core.pydantic_v1 import BaseToolclass CustomTool(BaseTool): # BaseTool is v1 code x: int = Field(default=1) def _run(*args, **kwargs): return \"hello\" @field_validator('x') # v2 code @classmethod def validate_x(cls, x: int) -> int: return 1CustomTool( name='custom_tool', description=\"hello\", x=1,) 4. model_rebuild()â€‹ When sub-classing from LangChain models, users may need to add relevant imports to the file and rebuild the model. You can read more about model_rebuild here. from langchain_core.output_parsers import BaseOutputParserclass FooParser(BaseOutputParser): ...API Reference:BaseOutputParser New code: from typing import Optional as Optionalfrom langchain_core.output_parsers import BaseOutputParserclass FooParser(BaseOutputParser): ...FooParser.model_rebuild()API Reference:BaseOutputParser Migrate using langchain-cliâ€‹ The langchain-cli can help update deprecated LangChain imports in your code automatically. Please note that the langchain-cli only handles deprecated LangChain imports and cannot help to upgrade your code from pydantic 1 to pydantic 2. For help with the Pydantic 1 to 2 migration itself please refer to the Pydantic Migration Guidelines. As of 0.0.31, the langchain-cli relies on gritql for applying code mods. Installationâ€‹ pip install -U langchain-clilangchain-cli --version # <-- Make sure the version is at least 0.0.31 Usageâ€‹ Given that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like git). The langchain-cli will handle the langchain_core.pydantic_v1 deprecation introduced in LangChain 0.3 as well as older deprecations (e.g.,from langchain.chat_models import ChatOpenAI which should be from langchain_openai import ChatOpenAI), You will need to run the migration script twice as it only applies one import replacement per run. For example, say that your code is still using the old import from langchain.chat_models import ChatOpenAI: After the first run, you'll get: from langchain_community.chat_models import ChatOpenAI After the second run, you'll get: from langchain_openai import ChatOpenAI # Run a first time# Will replace from langchain.chat_models import ChatOpenAIlangchain-cli migrate --help [path to code] # Helplangchain-cli migrate [path to code] # Apply# Run a second time to apply more import replacementslangchain-cli migrate --diff [path to code] # Previewlangchain-cli migrate [path to code] # Apply Other optionsâ€‹ # See help menulangchain-cli migrate --help# Preview Changes without applyinglangchain-cli migrate --diff [path to code]# Approve changes interactivelylangchain-cli migrate --interactive [path to code]What's changedWhat's newHow to update your codeBase packagesDownstream packagesIntegration packagesCommon issues when transitioning to Pydantic 21. Do not use the langchain_core.pydantic_v1 namespace2. Passing Pydantic objects to LangChain APIs3. Sub-classing LangChain models4. model_rebuild()Migrate using langchain-cliInstallationUsageOther options",
      "timestamp": "2025-08-24 15:29:12"
    },
    {
      "url": "https://python.langchain.com/api_reference/",
      "title": "LangChain Python API Reference â€” ğŸ¦œğŸ”— LangChain  documentation",
      "content": "LangChain Python API Reference â€” ğŸ¦œğŸ”— LangChain documentation Skip to main content Back to top Ctrl+K Docs Light Dark System Settings GitHub X / Twitter LangChain Python API Reference# Welcome to the LangChain Python API reference. This is a reference for all langchain-x packages. For user guides see https://python.langchain.com. For the legacy API reference hosted on ReadTheDocs see https://api.python.langchain.com/. Base packages# Core langchain-core: 0.3.74 core/index.html Langchain langchain: 0.3.27 langchain/index.html Text Splitters langchain-text-splitters: 0.3.9 text_splitters/index.html Community langchain-community: 0.3.27 community/index.html Experimental langchain-experimental: 0.3.5rc1 experimental/index.html Standard Tests langchain-tests: 0.3.20 standard_tests/index.html Integrations# OpenAI langchain-openai 0.3.31 openai/index.html Anthropic langchain-anthropic 0.3.19 anthropic/index.html Google VertexAI langchain-google-vertexai 2.0.28 google_vertexai/index.html AWS langchain-aws 0.2.31 aws/index.html Huggingface langchain-huggingface 0.3.1 huggingface/index.html MistralAI langchain-mistralai 0.2.11 mistralai/index.html See the full list of integrations in the Section Navigation. On this page",
      "timestamp": "2025-08-24 15:29:17"
    },
    {
      "url": "https://python.langchain.com/docs/how_to/installation/",
      "title": "How to install LangChain packages | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "How to install LangChain packages | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of functionality to install. Official releaseâ€‹ To install the main langchain package, run: PipCondapip install langchainconda install langchain -c conda-forge While this package acts as a sane starting point to using LangChain, much of the value of LangChain comes when integrating it with various model providers, datastores, etc. By default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below. Ecosystem packagesâ€‹ With the exception of the langsmith SDK, all packages in the LangChain ecosystem depend on langchain-core, which contains base classes and abstractions that other packages use. The dependency graph below shows how the different packages are related. A directed arrow indicates that the source package depends on the target package: When installing a package, you do not need to explicitly install that package's explicit dependencies (such as langchain-core). However, you may choose to if you are using a feature only available in a certain version of that dependency. If you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use. LangChain coreâ€‹ The langchain-core package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by langchain, but can also be used separately. Install with: pip install langchain-core Integration packagesâ€‹ Certain integrations like OpenAI and Anthropic have their own packages. Any integrations that require their own package will be documented as such in the Integration docs. You can see a list of all integration packages in the API reference under the \"Partner libs\" dropdown. To install one of these run: pip install langchain-openai Any integrations that haven't been split out into their own packages will live in the langchain-community package. Install with: pip install langchain-community LangChain experimentalâ€‹ The langchain-experimental package holds experimental LangChain code, intended for research and experimental uses. Install with: pip install langchain-experimental LangGraphâ€‹ langgraph is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it. Install with: pip install langgraph LangServeâ€‹ LangServe helps developers deploy LangChain runnables and chains as a REST API. LangServe is automatically installed by LangChain CLI. If not using LangChain CLI, install with: pip install \"langserve[all]\" for both client and server dependencies. Or pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code. LangChain CLIâ€‹ The LangChain CLI is useful for working with LangChain templates and other LangServe projects. Install with: pip install langchain-cli LangSmith SDKâ€‹ The LangSmith SDK is automatically installed by LangChain. However, it does not depend on langchain-core, and can be installed and used independently if desired. If you are not using LangChain, you can install it with: pip install langsmith From sourceâ€‹ If you want to install a package from source, you can do so by cloning the main LangChain repo, enter the directory of the package you want to install PATH/TO/REPO/langchain/libs/{package}, and run: pip install -e . LangGraph, LangSmith SDK, and certain integration packages live outside the main LangChain repo. You can see all repos here.Official releaseEcosystem packagesLangChain coreIntegration packagesLangChain experimentalLangGraphLangServeLangChain CLILangSmith SDKFrom source",
      "timestamp": "2025-08-24 15:29:20"
    },
    {
      "url": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html",
      "title": "HumanMessage â€” ğŸ¦œğŸ”— LangChain  documentation",
      "content": "HumanMessage â€” ğŸ¦œğŸ”— LangChain documentation Skip to main content Back to top Ctrl+K Docs Light Dark System Settings GitHub X / Twitter HumanMessage# class langchain_core.messages.human.HumanMessage[source]# Bases: BaseMessage Message from a human. HumanMessages are messages that are passed in from a human to the model. Example from langchain_core.messages import HumanMessage, SystemMessage messages = [ SystemMessage( content=\"You are a helpful assistant! Your name is Bob.\" ), HumanMessage( content=\"What is your name?\" ) ] # Instantiate a chat model and invoke it with the messages model = ... print(model.invoke(messages)) Copy to clipboard Pass in content as positional arg. Parameters: content â€“ The string contents of the message. kwargs â€“ Additional fields to pass to the message. param additional_kwargs: dict [Optional]# Reserved for additional payload data associated with the message. For example, for a message from an AI, this could include tool calls as encoded by the model provider. param content: str | list[str | dict] [Required]# The string contents of the message. param example: bool = False# Use to denote that a message is part of an example conversation. At the moment, this is ignored by most models. Usage is discouraged. Defaults to False. param id: str | None = None# An optional unique identifier for the message. This should ideally be provided by the provider/model which created the message. Constraints: coerce_numbers_to_str = True param name: str | None = None# An optional name for the message. This can be used to provide a human-readable name for the message. Usage of this field is optional, and whether itâ€™s used or not is up to the model implementation. param response_metadata: dict [Optional]# Response metadata. For example: response headers, logprobs, token counts, model name. param type: Literal['human'] = 'human'# The type of the message (used for serialization). Defaults to â€œhumanâ€. pretty_print() â†’ None# Print a pretty representation of the message. Return type: None pretty_repr(html: bool = False) â†’ str# Get a pretty representation of the message. Parameters: html (bool) â€“ Whether to format the message as HTML. If True, the message will be formatted with HTML tags. Default is False. Returns: A pretty representation of the message. Return type: str text() â†’ str# Get the text content of the message. Returns: The text content of the message. Return type: str Examples using HumanMessage # Related Activeloop Deep Memory Alibaba Cloud PAI EAS Arthur Azure ML AzureMLChatOnlineEndpoint Browserbase Build a Chatbot Build a Question/Answering system over SQL data Build a Simple LLM Application with LCEL Build an Agent Build an Agent with AgentExecutor (Legacy) Chat Bot Feedback Template Chat with Baichuan-192K Chat with Coze Bot ChatAnyscale ChatEverlyAI ChatFriendli ChatHuggingFace ChatKonko ChatLiteLLM ChatLiteLLMRouter ChatNVIDIA ChatOCIGenAI ChatOctoAI ChatOllama ChatPremAI ChatTongyi ChatWatsonx ChatYI ChatYandexGPT Cohere Conceptual guide Context Conversational RAG Dappier AI DeepInfra Discord Eden AI ErnieBotChat Flyte GPTRouter GigaChat Google Google Cloud Vertex AI Google Imagen How to add a semantic layer over graph database How to add chat history How to add examples to the prompt for query analysis How to add message history How to add retrieval to chatbots How to add tools to chatbots How to compose prompts together How to convert tools to OpenAI Functions How to create a custom chat model class How to do tool/function calling How to filter messages How to handle tool errors How to merge consecutive messages of the same type How to pass multimodal data directly to models How to pass tool outputs to chat models How to return structured data from a model How to trim messages How to use callbacks in async environments How to use few-shot prompting with tool calling How to use reference examples when doing extraction Javelin AI Gateway Javelin AI Gateway Tutorial JinaChat LLMonitor Label Studio LlamaEdge Log10 MLX MLflow AI Gateway MLflow Deployments for LLMs Maritalk MiniMaxChat MoonshotChat PremAI PromptLayer PromptLayerChatOpenAI QianfanChatEndpoint Snowflake Cortex SparkLLM Chat Tencent Hunyuan Trubrics VolcEngineMaasChat WeChat Yuan2.0 ZHIPU AI Zep Cloud Zep Cloud Memory Zep Open Source Zep Open Source Memory ZepCloudChatMessageHistory vLLM Chat ğŸ¦œï¸ğŸ“ LangServe On this page",
      "timestamp": "2025-08-24 15:29:23"
    },
    {
      "url": "https://python.langchain.com/docs/how_to/output_parser_string/",
      "title": "How to parse text from message objects | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "How to parse text from message objects | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free. PrerequisitesThis guide assumes familiarity with the following concepts: Chat models Messages Output parsers LangChain Expression Language (LCEL) LangChain message objects support content in a variety of formats, including text, multimodal data, and a list of content block dicts. The format of Chat model response content may depend on the provider. For example, the chat model for Anthropic will return string content for typical string input: from langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-5-haiku-latest\")response = llm.invoke(\"Hello\")response.content 'Hi there! How are you doing today? Is there anything I can help you with?' But when tool calls are generated, the response content is structured into content blocks that convey the model's reasoning process: from langchain_core.tools import tool@tooldef get_weather(location: str) -> str: \"\"\"Get the weather from a location.\"\"\" return \"Sunny.\"llm_with_tools = llm.bind_tools([get_weather])response = llm_with_tools.invoke(\"What's the weather in San Francisco, CA?\")response.contentAPI Reference:tool [{'text': \"I'll help you get the current weather for San Francisco, California. Let me check that for you right away.\", 'type': 'text'}, {'id': 'toolu_015PwwcKxWYctKfY3pruHFyy', 'input': {'location': 'San Francisco, CA'}, 'name': 'get_weather', 'type': 'tool_use'}] To automatically parse text from message objects irrespective of the format of the underlying content, we can use StrOutputParser. We can compose it with a chat model as follows: from langchain_core.output_parsers import StrOutputParserchain = llm_with_tools | StrOutputParser()API Reference:StrOutputParser StrOutputParser simplifies the extraction of text from message objects: response = chain.invoke(\"What's the weather in San Francisco, CA?\")print(response) I'll help you check the weather in San Francisco, CA right away. This is particularly useful in streaming contexts: for chunk in chain.stream(\"What's the weather in San Francisco, CA?\"): print(chunk, end=\"|\") |I'll| help| you get| the current| weather for| San Francisco, California|. Let| me retrieve| that| information for you.|||||||||| See the API Reference for more information.",
      "timestamp": "2025-08-24 15:29:27"
    },
    {
      "url": "https://python.langchain.com/docs/concepts/tokens/",
      "title": "Tokens | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Tokens | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Modern large language models (LLMs) are typically based on a transformer architecture that processes a sequence of units known as tokens. Tokens are the fundamental elements that models use to break down input and generate output. In this section, we'll discuss what tokens are and how they are used by language models. What is a token?â€‹ A token is the basic unit that a language model reads, processes, and generates. These units can vary based on how the model provider defines them, but in general, they could represent: A whole word (e.g., \"apple\"), A part of a word (e.g., \"app\"), Or other linguistic components such as punctuation or spaces. The way the model tokenizes the input depends on its tokenizer algorithm, which converts the input into tokens. Similarly, the modelâ€™s output comes as a stream of tokens, which is then decoded back into human-readable text. How tokens work in language modelsâ€‹ The reason language models use tokens is tied to how they understand and predict language. Rather than processing characters or entire sentences directly, language models focus on tokens, which represent meaningful linguistic units. Here's how the process works: Input Tokenization: When you provide a model with a prompt (e.g., \"LangChain is cool!\"), the tokenizer algorithm splits the text into tokens. For example, the sentence could be tokenized into parts like [\"Lang\", \"Chain\", \" is\", \" cool\", \"!\"]. Note that token boundaries donâ€™t always align with word boundaries. Processing: The transformer architecture behind these models processes tokens sequentially to predict the next token in a sentence. It does this by analyzing the relationships between tokens, capturing context and meaning from the input. Output Generation: The model generates new tokens one by one. These output tokens are then decoded back into human-readable text. Using tokens instead of raw characters allows the model to focus on linguistically meaningful units, which helps it capture grammar, structure, and context more effectively. Tokens donâ€™t have to be textâ€‹ Although tokens are most commonly used to represent text, they donâ€™t have to be limited to textual data. Tokens can also serve as abstract representations of multi-modal data, such as: Images, Audio, Video, And other types of data. At the time of writing, virtually no models support multi-modal output, and only a few models can handle multi-modal inputs (e.g., text combined with images or audio). However, as advancements in AI continue, we expect multi-modality to become much more common. This would allow models to process and generate a broader range of media, significantly expanding the scope of what tokens can represent and how models can interact with diverse types of data. noteIn principle, anything that can be represented as a sequence of tokens could be modeled in a similar way. For example, DNA sequencesâ€”which are composed of a series of nucleotides (A, T, C, G)â€”can be tokenized and modeled to capture patterns, make predictions, or generate sequences. This flexibility allows transformer-based models to handle diverse types of sequential data, further broadening their potential applications across various domains, including bioinformatics, signal processing, and other fields that involve structured or unstructured sequences. Please see the multimodality section for more information on multi-modal inputs and outputs. Why not use characters?â€‹ Using tokens instead of individual characters makes models both more efficient and better at understanding context and grammar. Tokens represent meaningful units, like whole words or parts of words, allowing models to capture language structure more effectively than by processing raw characters. Token-level processing also reduces the number of units the model has to handle, leading to faster computation. In contrast, character-level processing would require handling a much larger sequence of input, making it harder for the model to learn relationships and context. Tokens enable models to focus on linguistic meaning, making them more accurate and efficient in generating responses. How tokens correspond to textâ€‹ Please see this post from OpenAI for more details on how tokens are counted and how they correspond to text. According to the OpenAI post, the approximate token counts for English text are as follows: 1 token ~= 4 chars in English 1 token ~= Â¾ words 100 tokens ~= 75 words What is a token?How tokens work in language modelsTokens donâ€™t have to be textWhy not use characters?How tokens correspond to text",
      "timestamp": "2025-08-24 15:29:30"
    },
    {
      "url": "https://python.langchain.com/docs/concepts/prompt_templates/",
      "title": "Prompt Templates | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Prompt Templates | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output. Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in. Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages. The reason this PromptValue exists is to make it easy to switch between strings and messages. There are a few different types of prompt templates: String PromptTemplatesâ€‹ These prompt templates are used to format a single string, and generally are used for simpler inputs. For example, a common way to construct and use a PromptTemplate is as follows: from langchain_core.prompts import PromptTemplateprompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt_template.invoke({\"topic\": \"cats\"})API Reference:PromptTemplate ChatPromptTemplatesâ€‹ These prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves. For example, a common way to construct and use a ChatPromptTemplate is as follows: from langchain_core.prompts import ChatPromptTemplateprompt_template = ChatPromptTemplate([ (\"system\", \"You are a helpful assistant\"), (\"user\", \"Tell me a joke about {topic}\")])prompt_template.invoke({\"topic\": \"cats\"})API Reference:ChatPromptTemplate In the above example, this ChatPromptTemplate will construct two messages when called. The first is a system message, that has no variables to format. The second is a HumanMessage, and will be formatted by the topic variable the user passes in. MessagesPlaceholderâ€‹ This prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw how we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would slot into a particular spot? This is how you use MessagesPlaceholder. from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.messages import HumanMessage, AIMessageprompt_template = ChatPromptTemplate([ (\"system\", \"You are a helpful assistant\"), MessagesPlaceholder(\"msgs\")])# Simple example with one messageprompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})# More complex example with conversation historymessages_to_pass = [ HumanMessage(content=\"What's the capital of France?\"), AIMessage(content=\"The capital of France is Paris.\"), HumanMessage(content=\"And what about Germany?\")]formatted_prompt = prompt_template.invoke({\"msgs\": messages_to_pass})print(formatted_prompt)API Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage | AIMessage This will produce a list of four messages total: the system message plus the three messages we passed in (two HumanMessages and one AIMessage). If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in). This is useful for letting a list of messages be slotted into a particular spot. An alternative way to accomplish the same thing without using the MessagesPlaceholder class explicitly is: prompt_template = ChatPromptTemplate([ (\"system\", \"You are a helpful assistant\"), (\"placeholder\", \"{msgs}\") # <-- This is the changed part]) For specifics on how to use prompt templates, see the relevant how-to guides here.String PromptTemplatesChatPromptTemplatesMessagesPlaceholder",
      "timestamp": "2025-08-24 15:29:34"
    },
    {
      "url": "https://python.langchain.com/docs/concepts/runnables/",
      "title": "Runnable interface | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Runnable interface | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as language models, output parsers, retrievers, compiled LangGraph graphs and more. This guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner. Related Resources The \"Runnable\" Interface API Reference provides a detailed overview of the Runnable interface and its methods. A list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using the LangChain Expression Language (LCEL). Overview of runnable interfaceâ€‹ The Runnable way defines a standard interface that allows a Runnable component to be: Invoked: A single input is transformed into an output. Batched: Multiple inputs are efficiently transformed into outputs. Streamed: Outputs are streamed as they are produced. Inspected: Schematic information about Runnable's input, output, and configuration can be accessed. Composed: Multiple Runnables can be composed to work together using the LangChain Expression Language (LCEL) to create complex pipelines. Please review the LCEL Cheatsheet for some common patterns that involve the Runnable interface and LCEL expressions. Optimized parallel execution (batch)â€‹ LangChain Runnables offer a built-in batch (and batch_as_completed) API that allow you to process multiple inputs in parallel. Using these methods can significantly improve performance when needing to process multiple independent inputs, as the processing can be done in parallel instead of sequentially. The two batching options are: batch: Process multiple inputs in parallel, returning results in the same order as the inputs. batch_as_completed: Process multiple inputs in parallel, returning results as they complete. Results may arrive out of order, but each includes the input index for matching. The default implementation of batch and batch_as_completed use a thread pool executor to run the invoke method in parallel. This allows for efficient parallel execution without the need for users to manage threads, and speeds up code that is I/O-bound (e.g., making API requests, reading files, etc.). It will not be as effective for CPU-bound operations, as the GIL (Global Interpreter Lock) in Python will prevent true parallel execution. Some Runnables may provide their own implementations of batch and batch_as_completed that are optimized for their specific use case (e.g., rely on a batch API provided by a model provider). noteThe async versions of abatch and abatch_as_completed relies on asyncio's gather and as_completed functions to run the ainvoke method in parallel. tipWhen processing a large number of inputs using batch or batch_as_completed, users may want to control the maximum number of parallel calls. This can be done by setting the max_concurrency attribute in the RunnableConfig dictionary. See the RunnableConfig for more information.Chat Models also have a built-in rate limiter that can be used to control the rate at which requests are made. Asynchronous supportâ€‹ Runnables expose an asynchronous API, allowing them to be called using the await syntax in Python. Asynchronous methods can be identified by the \"a\" prefix (e.g., ainvoke, abatch, astream, abatch_as_completed). Please refer to the Async Programming with LangChain guide for more details. Streaming APIsâ€‹ Streaming is critical in making applications based on LLMs feel responsive to end-users. Runnables expose the following three streaming APIs: sync stream and async astream: yields the output a Runnable as it is generated. The async astream_events: a more advanced streaming API that allows streaming intermediate steps and final output The legacy async astream_log: a legacy streaming API that streams intermediate steps and final output Please refer to the Streaming Conceptual Guide for more details on how to stream in LangChain. Input and output typesâ€‹ Every Runnable is characterized by an input and output type. These input and output types can be any Python object, and are defined by the Runnable itself. Runnable methods that result in the execution of the Runnable (e.g., invoke, batch, stream, astream_events) work with these input and output types. invoke: Accepts an input and returns an output. batch: Accepts a list of inputs and returns a list of outputs. stream: Accepts an input and returns a generator that yields outputs. The input type and output type vary by component: ComponentInput TypeOutput TypePromptdictionaryPromptValueChatModela string, list of chat messages or a PromptValueChatMessageLLMa string, list of chat messages or a PromptValueStringOutputParserthe output of an LLM or ChatModelDepends on the parserRetrievera stringList of DocumentsToola string or dictionary, depending on the toolDepends on the tool Please refer to the individual component documentation for more information on the input and output types and how to use them. Inspecting schemasâ€‹ noteThis is an advanced feature that is unnecessary for most users. You should probably skip this section unless you have a specific need to inspect the schema of a Runnable. In more advanced use cases, you may want to programmatically inspect the Runnable and determine what input and output types the Runnable expects and produces. The Runnable interface provides methods to get the JSON Schema of the input and output types of a Runnable, as well as Pydantic schemas for the input and output types. These APIs are mostly used internally for unit-testing and by LangServe which uses the APIs for input validation and generation of OpenAPI documentation. In addition, to the input and output types, some Runnables have been set up with additional run time configuration options. There are corresponding APIs to get the Pydantic Schema and JSON Schema of the configuration options for the Runnable. Please see the Configurable Runnables section for more information. MethodDescriptionget_input_schemaGives the Pydantic Schema of the input schema for the Runnable.get_output_schemaGives the Pydantic Schema of the output schema for the Runnable.config_schemaGives the Pydantic Schema of the config schema for the Runnable.get_input_jsonschemaGives the JSONSchema of the input schema for the Runnable.get_output_jsonschemaGives the JSONSchema of the output schema for the Runnable.get_config_jsonschemaGives the JSONSchema of the config schema for the Runnable. With_typesâ€‹ LangChain will automatically try to infer the input and output types of a Runnable based on available information. Currently, this inference does not work well for more complex Runnables that are built using LCEL composition, and the inferred input and / or output types may be incorrect. In these cases, we recommend that users override the inferred input and output types using the with_types method (API Reference). RunnableConfigâ€‹ Any of the methods that are used to execute the runnable (e.g., invoke, batch, stream, astream_events) accept a second argument called RunnableConfig (API Reference). This argument is a dictionary that contains configuration for the Runnable that will be used at run time during the execution of the runnable. A RunnableConfig can have any of the following properties defined: AttributeDescriptionrun_nameName used for the given Runnable (not inherited).run_idUnique identifier for this call. sub-calls will get their own unique run ids.tagsTags for this call and any sub-calls.metadataMetadata for this call and any sub-calls.callbacksCallbacks for this call and any sub-calls.max_concurrencyMaximum number of parallel calls to make (e.g., used by batch).recursion_limitMaximum number of times a call can recurse (e.g., used by Runnables that return Runnables)configurableRuntime values for configurable attributes of the Runnable. Passing config to the invoke method is done like so: some_runnable.invoke( some_input, config={ 'run_name': 'my_run', 'tags': ['tag1', 'tag2'], 'metadata': {'key': 'value'} }) Propagation of RunnableConfigâ€‹ Many Runnables are composed of other Runnables, and it is important that the RunnableConfig is propagated to all sub-calls made by the Runnable. This allows providing run time configuration values to the parent Runnable that are inherited by all sub-calls. If this were not the case, it would be impossible to set and propagate callbacks or other configuration values like tags and metadata which are expected to be inherited by all sub-calls. There are two main patterns by which new Runnables are created: Declaratively using LangChain Expression Language (LCEL): chain = prompt | chat_model | output_parser Using a custom Runnable (e.g., RunnableLambda) or using the @tool decorator: def foo(input): # Note that .invoke() is used directly here return bar_runnable.invoke(input)foo_runnable = RunnableLambda(foo) LangChain will try to propagate RunnableConfig automatically for both of the patterns. For handling the second pattern, LangChain relies on Python's contextvars. In Python 3.11 and above, this works out of the box, and you do not need to do anything special to propagate the RunnableConfig to the sub-calls. In Python 3.9 and 3.10, if you are using async code, you need to manually pass the RunnableConfig through to the Runnable when invoking it. This is due to a limitation in asyncio's tasks in Python 3.9 and 3.10 which did not accept a context argument. Propagating the RunnableConfig manually is done like so: async def foo(input, config): # <-- Note the config argument return await bar_runnable.ainvoke(input, config=config)foo_runnable = RunnableLambda(foo) cautionWhen using Python 3.10 or lower and writing async code, RunnableConfig cannot be propagated automatically, and you will need to do it manually! This is a common pitfall when attempting to stream data using astream_events and astream_log as these methods rely on proper propagation of callbacks defined inside of RunnableConfig. Setting custom run name, tags, and metadataâ€‹ The run_name, tags, and metadata attributes of the RunnableConfig dictionary can be used to set custom values for the run name, tags, and metadata for a given Runnable. The run_name is a string that can be used to set a custom name for the run. This name will be used in logs and other places to identify the run. It is not inherited by sub-calls. The tags and metadata attributes are lists and dictionaries, respectively, that can be used to set custom tags and metadata for the run. These values are inherited by sub-calls. Using these attributes can be useful for tracking and debugging runs, as they will be surfaced in LangSmith as trace attributes that you can filter and search on. The attributes will also be propagated to callbacks, and will appear in streaming APIs like astream_events as part of each event in the stream. Related How-to trace with LangChain Setting run idâ€‹ noteThis is an advanced feature that is unnecessary for most users. You may need to set a custom run_id for a given run, in case you want to reference it later or correlate it with other systems. The run_id MUST be a valid UUID string and unique for each run. It is used to identify the parent run, sub-class will get their own unique run ids automatically. To set a custom run_id, you can pass it as a key-value pair in the config dictionary when invoking the Runnable: import uuidrun_id = uuid.uuid4()some_runnable.invoke( some_input, config={ 'run_id': run_id })# Do something with the run_id Setting recursion limitâ€‹ noteThis is an advanced feature that is unnecessary for most users. Some Runnables may return other Runnables, which can lead to infinite recursion if not handled properly. To prevent this, you can set a recursion_limit in the RunnableConfig dictionary. This will limit the number of times a Runnable can recurse. Setting max concurrencyâ€‹ If using the batch or batch_as_completed methods, you can set the max_concurrency attribute in the RunnableConfig dictionary to control the maximum number of parallel calls to make. This can be useful when you want to limit the number of parallel calls to prevent overloading a server or API. tipIf you're trying to rate limit the number of requests made by a Chat Model, you can use the built-in rate limiter instead of setting max_concurrency, which will be more effective.See the How to handle rate limits guide for more information. Setting configurableâ€‹ The configurable field is used to pass runtime values for configurable attributes of the Runnable. It is used frequently in LangGraph with LangGraph Persistence and memory. It is used for a similar purpose in RunnableWithMessageHistory to specify either a session_id / conversation_id to keep track of conversation history. In addition, you can use it to specify any custom configuration options to pass to any Configurable Runnable that they create. Setting callbacksâ€‹ Use this option to configure callbacks for the runnable at runtime. The callbacks will be passed to all sub-calls made by the runnable. some_runnable.invoke( some_input, { \"callbacks\": [ SomeCallbackHandler(), AnotherCallbackHandler(), ] }) Please read the Callbacks Conceptual Guide for more information on how to use callbacks in LangChain. importantIf you're using Python 3.9 or 3.10 in an async environment, you must propagate the RunnableConfig manually to sub-calls in some cases. Please see the Propagating RunnableConfig section for more information. Creating a runnable from a functionâ€‹ You may need to create a custom Runnable that runs arbitrary logic. This is especially useful if using LangChain Expression Language (LCEL) to compose multiple Runnables and you need to add custom processing logic in one of the steps. There are two ways to create a custom Runnable from a function: RunnableLambda: Use this for simple transformations where streaming is not required. RunnableGenerator: use this for more complex transformations when streaming is needed. See the How to run custom functions guide for more information on how to use RunnableLambda and RunnableGenerator. importantUsers should not try to subclass Runnables to create a new custom Runnable. It is much more complex and error-prone than simply using RunnableLambda or RunnableGenerator. Configurable runnablesâ€‹ noteThis is an advanced feature that is unnecessary for most users.It helps with configuration of large \"chains\" created using the LangChain Expression Language (LCEL) and is leveraged by LangServe for deployed Runnables. Sometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models. To simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime: configurable_fields: This method allows you to configure specific attributes in a Runnable. For example, the temperature attribute of a chat model. configurable_alternatives: This method enables you to specify alternative Runnables that can be run during runtime. For example, you could specify a list of different chat models that can be used. See the How to configure runtime chain internals guide for more information on how to configure runtime chain internals.Overview of runnable interfaceOptimized parallel execution (batch)Asynchronous supportStreaming APIsInput and output typesInspecting schemasRunnableConfigPropagation of RunnableConfigSetting custom run name, tags, and metadataSetting run idSetting recursion limitSetting max concurrencySetting configurableSetting callbacksCreating a runnable from a functionConfigurable runnables",
      "timestamp": "2025-08-24 15:29:37"
    },
    {
      "url": "https://python.langchain.com/docs/concepts/messages/",
      "title": "Messages | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Messages | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Prerequisites Chat Models Overviewâ€‹ Messages are the unit of communication in chat models. They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation. Each message has a role (e.g., \"user\", \"assistant\") and content (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider. LangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider. What is inside a message?â€‹ A message typically consists of the following pieces of information: Role: The role of the message (e.g., \"user\", \"assistant\"). Content: The content of the message (e.g., text, multimodal data). Additional metadata: id, name, token usage and other model-specific metadata. Roleâ€‹ Roles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages. RoleDescriptionsystemUsed to tell the chat model how to behave and provide additional context. Not supported by all chat model providers.userRepresents input from a user interacting with the model, usually in the form of text or other interactive input.assistantRepresents a response from the model, which can include text or a request to invoke tools.toolA message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support tool calling.function (legacy)This is a legacy role, corresponding to OpenAI's legacy function-calling API. tool role should be used instead. Contentâ€‹ The content of a message text or a list of dictionaries representing multimodal data (e.g., images, audio, video). The exact format of the content can vary between different chat model providers. Currently, most chat models support text as the primary content type, with some models also supporting multimodal data. However, support for multimodal data is still limited across most chat model providers. For more information see: SystemMessage -- for content which should be passed to direct the conversation HumanMessage -- for content in the input from the user. AIMessage -- for content in the response from the model. Multimodality -- for more information on multimodal content. Other Message Dataâ€‹ Depending on the chat model provider, messages can include other data such as: ID: An optional unique identifier for the message. Name: An optional name property which allows differentiate between different entities/speakers with the same role. Not all models support this! Metadata: Additional information about the message, such as timestamps, token usage, etc. Tool Calls: A request made by the model to call one or more tools> See tool calling for more information. Conversation Structureâ€‹ The sequence of messages into a chat model should follow a specific structure to ensure that the chat model can generate a valid response. For example, a typical conversation structure might look like this: User Message: \"Hello, how are you?\" Assistant Message: \"I'm doing well, thank you for asking.\" User Message: \"Can you tell me a joke?\" Assistant Message: \"Sure! Why did the scarecrow win an award? Because he was outstanding in his field!\" Please read the chat history guide for more information on managing chat history and ensuring that the conversation structure is correct. LangChain Messagesâ€‹ LangChain provides a unified message format that can be used across all chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider. LangChain messages are Python objects that subclass from a BaseMessage. The five main message types are: SystemMessage: corresponds to system role HumanMessage: corresponds to user role AIMessage: corresponds to assistant role AIMessageChunk: corresponds to assistant role, used for streaming responses ToolMessage: corresponds to tool role Other important messages include: RemoveMessage -- does not correspond to any role. This is an abstraction, mostly used in LangGraph to manage chat history. Legacy FunctionMessage: corresponds to the function role in OpenAI's legacy function-calling API. You can find more information about messages in the API Reference. SystemMessageâ€‹ A SystemMessage is used to prime the behavior of the AI model and provide additional context, such as instructing the model to adopt a specific persona or setting the tone of the conversation (e.g., \"This is a conversation about cooking\"). Different chat providers may support system message in one of the following ways: Through a \"system\" message role: In this case, a system message is included as part of the message sequence with the role explicitly set as \"system.\" Through a separate API parameter for system instructions: Instead of being included as a message, system instructions are passed via a dedicated API parameter. No support for system messages: Some models do not support system messages at all. Most major chat model providers support system instructions via either a chat message or a separate API parameter. LangChain will automatically adapt based on the providerâ€™s capabilities. If the provider supports a separate API parameter for system instructions, LangChain will extract the content of a system message and pass it through that parameter. If no system message is supported by the provider, in most cases LangChain will attempt to incorporate the system message's content into a HumanMessage or raise an exception if that is not possible. However, this behavior is not yet consistently enforced across all implementations, and if using a less popular implementation of a chat model (e.g., an implementation from the langchain-community package) it is recommended to check the specific documentation for that model. HumanMessageâ€‹ The HumanMessage corresponds to the \"user\" role. A human message represents input from a user interacting with the model. Text Contentâ€‹ Most chat models expect the user input to be in the form of text. from langchain_core.messages import HumanMessagemodel.invoke([HumanMessage(content=\"Hello, how are you?\")])API Reference:HumanMessage tipWhen invoking a chat model with a string as input, LangChain will automatically convert the string into a HumanMessage object. This is mostly useful for quick testing.model.invoke(\"Hello, how are you?\") Multi-modal Contentâ€‹ Some chat models accept multimodal inputs, such as images, audio, video, or files like PDFs. Please see the multimodality guide for more information. AIMessageâ€‹ AIMessage is used to represent a message with the role \"assistant\". This is the response from the model, which can include text or a request to invoke tools. It could also include other media types like images, audio, or video -- though this is still uncommon at the moment. from langchain_core.messages import HumanMessageai_message = model.invoke([HumanMessage(\"Tell me a joke\")])ai_message # <-- AIMessageAPI Reference:HumanMessage An AIMessage has the following attributes. The attributes which are standardized are the ones that LangChain attempts to standardize across different chat model providers. raw fields are specific to the model provider and may vary. AttributeStandardized/RawDescriptioncontentRawUsually a string, but can be a list of content blocks. See content for details.tool_callsStandardizedTool calls associated with the message. See tool calling for details.invalid_tool_callsStandardizedTool calls with parsing errors associated with the message. See tool calling for details.usage_metadataStandardizedUsage metadata for a message, such as token counts. See Usage Metadata API Reference.idStandardizedAn optional unique identifier for the message, ideally provided by the provider/model that created the message. See Message IDs for details.response_metadataRawResponse metadata, e.g., response headers, logprobs, token counts. contentâ€‹ The content property of an AIMessage represents the response generated by the chat model. The content is either: text -- the norm for virtually all chat models. A list of dictionaries -- Each dictionary represents a content block and is associated with a type. Used by Anthropic for surfacing agent thought process when doing tool calling. Used by OpenAI for audio outputs. Please see multi-modal content for more information. importantThe content property is not standardized across different chat model providers, mostly because there are still few examples to generalize from. AIMessageChunkâ€‹ It is common to stream responses for the chat model as they are being generated, so the user can see the response in real-time instead of waiting for the entire response to be generated before displaying it. It is returned from the stream, astream and astream_events methods of the chat model. For example, for chunk in model.stream([HumanMessage(\"what color is the sky?\")]): print(chunk) AIMessageChunk follows nearly the same structure as AIMessage, but uses a different ToolCallChunk to be able to stream tool calling in a standardized manner. Aggregatingâ€‹ AIMessageChunks support the + operator to merge them into a single AIMessage. This is useful when you want to display the final response to the user. ai_message = chunk1 + chunk2 + chunk3 + ... ToolMessageâ€‹ This represents a message with role \"tool\", which contains the result of calling a tool. In addition to role and content, this message has: a tool_call_id field which conveys the id of the call to the tool that was called to produce this result. an artifact field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model. Please see tool calling for more information. RemoveMessageâ€‹ This is a special message type that does not correspond to any roles. It is used for managing chat history in LangGraph. Please see the following for more information on how to use the RemoveMessage: Memory conceptual guide How to delete messages (Legacy) FunctionMessageâ€‹ This is a legacy message type, corresponding to OpenAI's legacy function-calling API. ToolMessage should be used instead to correspond to the updated tool-calling API. OpenAI Formatâ€‹ Inputsâ€‹ Chat models also accept OpenAI's format as inputs to chat models: chat_model.invoke([ { \"role\": \"user\", \"content\": \"Hello, how are you?\", }, { \"role\": \"assistant\", \"content\": \"I'm doing well, thank you for asking.\", }, { \"role\": \"user\", \"content\": \"Can you tell me a joke?\", }]) Outputsâ€‹ At the moment, the output of the model will be in terms of LangChain messages, so you will need to convert the output to the OpenAI format if you need OpenAI format for the output as well. The convert_to_openai_messages utility function can be used to convert from LangChain messages to OpenAI format. Message IDsâ€‹ LangChain messages include an optional id field that serves as a unique identifier. Understanding when and how these IDs are assigned can be helpful for debugging, tracing, and working with message history. When Messages Get IDsâ€‹ Messages receive IDs in the following scenarios: Automatically assigned by LangChain: When generated through chat model invocation (.invoke(), .stream(), .astream()) with an active run manager/tracing context IDs follow the format: run-$RUN_ID (e.g., run-ba48f958-6402-41a5-b461-5e250a4ebd36-0) run-$RUN_ID-$IDX (e.g., run-ba48f958-6402-41a5-b461-5e250a4ebd36-1) when there are multiple generations from a single chat model invocation. Provider-assigned IDs (highest priority): When the model provider assigns its own ID to the message These take precedence over LangChain-generated run IDs Format varies by provider When Messages Don't Get IDsâ€‹ Messages will not receive IDs in these situations: Manual message creation: Messages created directly (e.g., AIMessage(content=\"hello\")) without going through chat models No run manager context: When there's no active callback/tracing infrastructure ID Priority Systemâ€‹ LangChain follows a clear precedence system for message IDs: Provider-assigned IDs (highest priority): IDs from the model provider LangChain run IDs (medium priority): IDs starting with run- Manual IDs (lowest priority): IDs explicitly set by users OverviewWhat is inside a message?RoleContentOther Message DataConversation StructureLangChain MessagesSystemMessageHumanMessageAIMessageAIMessageChunkToolMessageRemoveMessage(Legacy) FunctionMessageOpenAI FormatInputsOutputsMessage IDsWhen Messages Get IDsWhen Messages Don't Get IDsID Priority System",
      "timestamp": "2025-08-24 15:29:41"
    },
    {
      "url": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html",
      "title": "SystemMessage â€” ğŸ¦œğŸ”— LangChain  documentation",
      "content": "SystemMessage â€” ğŸ¦œğŸ”— LangChain documentation Skip to main content Back to top Ctrl+K Docs Light Dark System Settings GitHub X / Twitter SystemMessage# class langchain_core.messages.system.SystemMessage[source]# Bases: BaseMessage Message for priming AI behavior. The system message is usually passed in as the first of a sequence of input messages. Example from langchain_core.messages import HumanMessage, SystemMessage messages = [ SystemMessage( content=\"You are a helpful assistant! Your name is Bob.\" ), HumanMessage( content=\"What is your name?\" ) ] # Define a chat model and invoke it with the messages print(model.invoke(messages)) Copy to clipboard Pass in content as positional arg. Parameters: content â€“ The string contents of the message. kwargs â€“ Additional fields to pass to the message. param additional_kwargs: dict [Optional]# Reserved for additional payload data associated with the message. For example, for a message from an AI, this could include tool calls as encoded by the model provider. param content: str | list[str | dict] [Required]# The string contents of the message. param id: str | None = None# An optional unique identifier for the message. This should ideally be provided by the provider/model which created the message. Constraints: coerce_numbers_to_str = True param name: str | None = None# An optional name for the message. This can be used to provide a human-readable name for the message. Usage of this field is optional, and whether itâ€™s used or not is up to the model implementation. param response_metadata: dict [Optional]# Response metadata. For example: response headers, logprobs, token counts, model name. param type: Literal['system'] = 'system'# The type of the message (used for serialization). Defaults to â€œsystemâ€. pretty_print() â†’ None# Print a pretty representation of the message. Return type: None pretty_repr(html: bool = False) â†’ str# Get a pretty representation of the message. Parameters: html (bool) â€“ Whether to format the message as HTML. If True, the message will be formatted with HTML tags. Default is False. Returns: A pretty representation of the message. Return type: str text() â†’ str# Get the text content of the message. Returns: The text content of the message. Return type: str Examples using SystemMessage # Related Activeloop Deep Memory Build a Chatbot Build a Question/Answering system over SQL data Build a Simple LLM Application with LCEL ChatAnyscale ChatEverlyAI ChatFriendli ChatHuggingFace ChatKonko ChatOCIGenAI ChatOctoAI ChatPremAI ChatTongyi ChatWatsonx ChatYI ChatYandexGPT Context Exa Search GigaChat Google Cloud Vertex AI How to add examples to the prompt for query analysis How to compose prompts together How to create a custom chat model class How to filter messages How to merge consecutive messages of the same type How to migrate from legacy LangChain agents to LangGraph How to trim messages How to use reference examples when doing extraction Javelin AI Gateway Javelin AI Gateway Tutorial JinaChat LLMonitor Label Studio Llama2Chat LlamaEdge MLflow AI Gateway MLflow Deployments for LLMs MoonshotChat PremAI Robocorp Toolkit Snowflake Cortex Trubrics Yuan2.0 ZHIPU AI vLLM Chat ğŸ¦œï¸ğŸ“ LangServe On this page",
      "timestamp": "2025-08-24 15:29:44"
    },
    {
      "url": "https://python.langchain.com/docs/concepts/tool_calling/",
      "title": "Tool calling | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Tool calling | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Prerequisites Tools Chat Models Overviewâ€‹ Many AI applications interact directly with humans. In these cases, it is appropriate for models to respond in natural language. But what about cases where we want a model to also interact directly with systems, such as databases or an API? These systems often have a particular input schema; for example, APIs frequently have a required payload structure. This need motivates the concept of tool calling. You can use tool calling to request model responses that match a particular schema. infoYou will sometimes hear the term function calling. We use this term interchangeably with tool calling. Key conceptsâ€‹ Tool Creation: Use the @tool decorator to create a tool. A tool is an association between a function and its schema. Tool Binding: The tool needs to be connected to a model that supports tool calling. This gives the model awareness of the tool and the associated input schema required by the tool. Tool Calling: When appropriate, the model can decide to call a tool and ensure its response conforms to the tool's input schema. Tool Execution: The tool can be executed using the arguments provided by the model. Recommended usageâ€‹ This pseudocode illustrates the recommended workflow for using tool calling. Created tools are passed to .bind_tools() method as a list. This model can be called, as usual. If a tool call is made, model's response will contain the tool call arguments. The tool call arguments can be passed directly to the tool. # Tool creationtools = [my_tool]# Tool bindingmodel_with_tools = model.bind_tools(tools)# Tool callingresponse = model_with_tools.invoke(user_input) Tool creationâ€‹ The recommended way to create a tool is using the @tool decorator. from langchain_core.tools import tool@tooldef multiply(a: int, b: int) -> int: \"\"\"Multiply a and b.\"\"\" return a * bAPI Reference:tool Further reading See our conceptual guide on tools for more details. See our model integrations that support tool calling. See our how-to guide on tool calling. Tool bindingâ€‹ Many model providers support tool calling. tipSee our model integration page for a list of providers that support tool calling. The central concept to understand is that LangChain provides a standardized interface for connecting tools to models. The .bind_tools() method can be used to specify which tools are available for a model to call. model_with_tools = model.bind_tools(tools_list) As a specific example, let's take a function multiply and bind it as a tool to a model that supports tool calling. def multiply(a: int, b: int) -> int: \"\"\"Multiply a and b. Args: a: first int b: second int \"\"\" return a * bllm_with_tools = tool_calling_model.bind_tools([multiply]) Tool callingâ€‹ A key principle of tool calling is that the model decides when to use a tool based on the input's relevance. The model doesn't always need to call a tool. For example, given an unrelated input, the model would not call the tool: result = llm_with_tools.invoke(\"Hello world!\") The result would be an AIMessage containing the model's response in natural language (e.g., \"Hello!\"). However, if we pass an input relevant to the tool, the model should choose to call it: result = llm_with_tools.invoke(\"What is 2 multiplied by 3?\") As before, the output result will be an AIMessage. But, if the tool was called, result will have a tool_calls attribute. This attribute includes everything needed to execute the tool, including the tool name and input arguments: result.tool_calls[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'xxx', 'type': 'tool_call'}] For more details on usage, see our how-to guides! Tool executionâ€‹ Tools implement the Runnable interface, which means that they can be invoked (e.g., tool.invoke(args)) directly. LangGraph offers pre-built components (e.g., ToolNode) that will often invoke the tool in behalf of the user. Further reading See our how-to guide on tool calling. See the LangGraph documentation on using ToolNode. Forcing tool useâ€‹ By default, the model has the freedom to choose which tool to use based on the user's input. However, in certain scenarios, you might want to influence the model's decision-making process. LangChain allows you to enforce tool choice (using tool_choice), ensuring the model uses either a particular tool or any tool from a given list. This is useful for structuring the model's behavior and guiding it towards a desired outcome. Further reading See our how-to guide on forcing tool use. Best practicesâ€‹ When designing tools to be used by a model, it is important to keep in mind that: Models that have explicit tool-calling APIs will be better at tool calling than non-fine-tuned models. Models will perform better if the tools have well-chosen names and descriptions. Simple, narrowly scoped tools are easier for models to use than complex tools. Asking the model to select from a large list of tools poses challenges for the model. OverviewKey conceptsRecommended usageTool creationTool bindingTool callingTool executionForcing tool useBest practices",
      "timestamp": "2025-08-24 15:29:48"
    },
    {
      "url": "https://python.langchain.com/docs/concepts/chat_models/",
      "title": "Chat models | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Chat models | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Overviewâ€‹ Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario. Modern LLMs are typically accessed through a chat model interface that takes a list of messages as input and returns a message as output. The newest generation of chat models offer additional capabilities: Tool calling: Many popular chat models offer a native tool calling API. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks. Structured output: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema. Multimodality: The ability to work with data other than text; for example, images, audio, and video. Featuresâ€‹ LangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs. Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see chat model integrations for an up-to-date list of supported models. Use either LangChain's messages format or OpenAI format. Standard tool calling API: standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model. Standard API for structuring outputs via the with_structured_output method. Provides support for async programming, efficient batching, a rich streaming API. Integration with LangSmith for monitoring and debugging production-grade applications based on LLMs. Additional features like standardized token usage, rate limiting, caching and more. Integrationsâ€‹ LangChain has many chat model integrations that allow you to use a wide variety of models from different providers. These integrations are one of two types: Official models: These are models that are officially supported by LangChain and/or model provider. You can find these models in the langchain-<provider> packages. Community models: There are models that are mostly contributed and supported by the community. You can find these models in the langchain-community package. LangChain chat models are named with a convention that prefixes \"Chat\" to their class names (e.g., ChatOllama, ChatAnthropic, ChatOpenAI, etc.). Please review the chat model integrations for a list of supported models. noteModels that do not include the prefix \"Chat\" in their name or include \"LLM\" as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. Interfaceâ€‹ LangChain chat models implement the BaseChatModel interface. Because BaseChatModel also implements the Runnable Interface, chat models support a standard streaming interface, async programming, optimized batching, and more. Please see the Runnable Interface for more details. Many of the key methods of chat models operate on messages as input and return messages as output. Chat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the standard parameters section for more details. noteIn documentation, we will often use the terms \"LLM\" and \"Chat Model\" interchangeably. This is because most modern LLMs are exposed to users via a chat model interface.However, LangChain also has implementations of older LLMs that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. These models are typically named without the \"Chat\" prefix (e.g., Ollama, Anthropic, OpenAI, etc.). These models implement the BaseLLM interface and may be named with the \"LLM\" suffix (e.g., OllamaLLM, AnthropicLLM, OpenAILLM, etc.). Generally, users should not use these models. Key methodsâ€‹ The key methods of a chat model are: invoke: The primary method for interacting with a chat model. It takes a list of messages as input and returns a list of messages as output. stream: A method that allows you to stream the output of a chat model as it is generated. batch: A method that allows you to batch multiple requests to a chat model together for more efficient processing. bind_tools: A method that allows you to bind a tool to a chat model for use in the model's execution context. with_structured_output: A wrapper around the invoke method for models that natively support structured output. Other important methods can be found in the BaseChatModel API Reference. Inputs and outputsâ€‹ Modern LLMs are typically accessed through a chat model interface that takes messages as input and returns messages as output. Messages are typically associated with a role (e.g., \"system\", \"human\", \"assistant\") and one or more content blocks that contain text or potentially multimodal data (e.g., images, audio, video). LangChain supports two message formats to interact with chat models: LangChain Message Format: LangChain's own message format, which is used by default and is used internally by LangChain. OpenAI's Message Format: OpenAI's message format. Standard parametersâ€‹ Many chat models have standardized parameters that can be used to configure the model: ParameterDescriptionmodelThe name or identifier of the specific AI model you want to use (e.g., \"gpt-3.5-turbo\" or \"gpt-4\").temperatureControls the randomness of the model's output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused.timeoutThe maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesnâ€™t hang indefinitely.max_tokensLimits the total number of tokens (words and punctuation) in the response. This controls how long the output can be.stopSpecifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response.max_retriesThe maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.api_keyThe API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model.base_urlThe URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests.rate_limiterAn optional BaseRateLimiter to space out requests to avoid exceeding rate limits. See rate-limiting below for more details. Some important things to note: Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can't be supported on these. Standard parameters are currently only enforced on integrations that have their own integration packages (e.g. langchain-openai, langchain-anthropic, etc.), they're not enforced on models in langchain-community. Chat models also accept other parameters that are specific to that integration. To find all the parameters supported by a Chat model head to the their respective API reference for that model. Tool callingâ€‹ Chat models can call tools to perform tasks such as fetching data from a database, making API requests, or running custom code. Please see the tool calling guide for more information. Structured outputsâ€‹ Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely useful for information extraction tasks. Please read more about the technique in the structured outputs guide. Multimodalityâ€‹ Large Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as multimodality. Currently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details. Context windowâ€‹ A chat model's context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models. If the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can \"remember\" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the memory. The size of the input is measured in tokens which are the unit of processing that the model uses. Advanced topicsâ€‹ Rate-limitingâ€‹ Many chat model providers impose a limit on the number of requests that can be made in a given time period. If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests. You have a few options to deal with rate limits: Try to avoid hitting rate limits by spacing out requests: Chat models accept a rate_limiter parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the how to handle rate limits for more information on how to use this feature. Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a max_retries parameter that can be used to control the number of retries. See the standard parameters section for more information. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited. Cachingâ€‹ Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution. The reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the exact inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages? An alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others. A semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an embedding model to convert text to a vector representation), and it's not guaranteed to capture the meaning of the input accurately. However, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve response times. Please see the how to cache chat model responses guide for more details. Related resourcesâ€‹ How-to guides on using chat models: how-to guides. List of supported chat models: chat model integrations. Conceptual guidesâ€‹ Messages Tool calling Multimodality Structured outputs Tokens OverviewFeaturesIntegrationsInterfaceKey methodsInputs and outputsStandard parametersTool callingStructured outputsMultimodalityContext windowAdvanced topicsRate-limitingCachingRelated resourcesConceptual guides",
      "timestamp": "2025-08-24 15:29:52"
    },
    {
      "url": "https://python.langchain.com/docs/how_to/chat_streaming/",
      "title": "How to stream chat model responses | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "How to stream chat model responses | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page All chat models implement the Runnable interface, which comes with a default implementations of standard runnable methods (i.e. ainvoke, batch, abatch, stream, astream, astream_events). The default streaming implementation provides anIterator (or AsyncIterator for asynchronous streaming) that yields a single value: the final output from the underlying chat model provider. tipThe default implementation does not provide support for token-by-token streaming, but it ensures that the model can be swapped in for any other model as it supports the same standard interface. The ability to stream the output token-by-token depends on whether the provider has implemented proper streaming support. See which integrations support token-by-token streaming here. Sync streamingâ€‹ Below we use a | to help visualize the delimiter between tokens. from langchain_anthropic.chat_models import ChatAnthropicchat = ChatAnthropic(model=\"claude-3-haiku-20240307\")for chunk in chat.stream(\"Write me a 1 verse song about goldfish on the moon\"): print(chunk.content, end=\"|\", flush=True) Here| is| a| |1| |verse| song| about| gol|dfish| on| the| moon|:|Floating| up| in| the| star|ry| night|,|Fins| a|-|gl|im|mer| in| the| pale| moon|light|.|Gol|dfish| swimming|,| peaceful| an|d free|,|Se|ren|ely| |drif|ting| across| the| lunar| sea|.| Async Streamingâ€‹ from langchain_anthropic.chat_models import ChatAnthropicchat = ChatAnthropic(model=\"claude-3-haiku-20240307\")async for chunk in chat.astream(\"Write me a 1 verse song about goldfish on the moon\"): print(chunk.content, end=\"|\", flush=True) Here| is| a| |1| |verse| song| about| gol|dfish| on| the| moon|:|Floating| up| above| the| Earth|,|Gol|dfish| swim| in| alien| m|irth|.|In| their| bowl| of| lunar| dust|,|Gl|it|tering| scales| reflect| the| trust|Of| swimming| free| in| this| new| worl|d,|Where| their| aqu|atic| dream|'s| unf|ur|le|d.| Astream eventsâ€‹ Chat models also support the standard astream events method. This method is useful if you're streaming output from a larger LLM application that contains multiple steps (e.g., an LLM chain composed of a prompt, llm and parser). from langchain_anthropic.chat_models import ChatAnthropicchat = ChatAnthropic(model=\"claude-3-haiku-20240307\")idx = 0async for event in chat.astream_events( \"Write me a 1 verse song about goldfish on the moon\"): idx += 1 if idx >= 5: # Truncate the output print(\"...Truncated\") break print(event) {'event': 'on_chat_model_start', 'data': {'input': 'Write me a 1 verse song about goldfish on the moon'}, 'name': 'ChatAnthropic', 'tags': [], 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'parent_ids': []}{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e', usage_metadata={'input_tokens': 21, 'output_tokens': 2, 'total_tokens': 23, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'parent_ids': []}{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content=\"Here's\", additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e')}, 'parent_ids': []}{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content=' a short one-verse song', additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e')}, 'parent_ids': []}...TruncatedSync streamingAsync StreamingAstream events",
      "timestamp": "2025-08-24 15:29:55"
    },
    {
      "url": "https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html",
      "title": "ChatPromptTemplate â€” ğŸ¦œğŸ”— LangChain  documentation",
      "content": "ChatPromptTemplate â€” ğŸ¦œğŸ”— LangChain documentation Skip to main content Back to top Ctrl+K Docs Light Dark System Settings GitHub X / Twitter ChatPromptTemplate# class langchain_core.prompts.chat.ChatPromptTemplate[source]# Bases: BaseChatPromptTemplate Prompt template for chat models. Use to create flexible templated prompts for chat models. Examples Changed in version 0.2.24: You can pass any Message-like formats supported by ChatPromptTemplate.from_messages() directly to ChatPromptTemplate() init. from langchain_core.prompts import ChatPromptTemplate template = ChatPromptTemplate([ (\"system\", \"You are a helpful AI bot. Your name is {name}.\"), (\"human\", \"Hello, how are you doing?\"), (\"ai\", \"I'm doing well, thanks!\"), (\"human\", \"{user_input}\"), ]) prompt_value = template.invoke( { \"name\": \"Bob\", \"user_input\": \"What is your name?\" } ) # Output: # ChatPromptValue( # messages=[ # SystemMessage(content='You are a helpful AI bot. Your name is Bob.'), # HumanMessage(content='Hello, how are you doing?'), # AIMessage(content=\"I'm doing well, thanks!\"), # HumanMessage(content='What is your name?') # ] #) Copy to clipboard Messages Placeholder: # In addition to Human/AI/Tool/Function messages, # you can initialize the template with a MessagesPlaceholder # either using the class directly or with the shorthand tuple syntax: template = ChatPromptTemplate([ (\"system\", \"You are a helpful AI bot.\"), # Means the template will receive an optional list of messages under # the \"conversation\" key (\"placeholder\", \"{conversation}\") # Equivalently: # MessagesPlaceholder(variable_name=\"conversation\", optional=True) ]) prompt_value = template.invoke( { \"conversation\": [ (\"human\", \"Hi!\"), (\"ai\", \"How can I assist you today?\"), (\"human\", \"Can you make me an ice cream sundae?\"), (\"ai\", \"No.\") ] } ) # Output: # ChatPromptValue( # messages=[ # SystemMessage(content='You are a helpful AI bot.'), # HumanMessage(content='Hi!'), # AIMessage(content='How can I assist you today?'), # HumanMessage(content='Can you make me an ice cream sundae?'), # AIMessage(content='No.'), # ] #) Copy to clipboard Single-variable template: If your prompt has only a single input variable (i.e., 1 instance of â€œ{variable_nams}â€), and you invoke the template with a non-dict object, the prompt template will inject the provided argument into that variable location. from langchain_core.prompts import ChatPromptTemplate template = ChatPromptTemplate([ (\"system\", \"You are a helpful AI bot. Your name is Carl.\"), (\"human\", \"{user_input}\"), ]) prompt_value = template.invoke(\"Hello, there!\") # Equivalent to # prompt_value = template.invoke({\"user_input\": \"Hello, there!\"}) # Output: # ChatPromptValue( # messages=[ # SystemMessage(content='You are a helpful AI bot. Your name is Carl.'), # HumanMessage(content='Hello, there!'), # ] # ) Copy to clipboard Create a chat prompt template from a variety of message formats. Parameters: messages â€“ sequence of message representations. A message can be represented using the following formats: (1) BaseMessagePromptTemplate, (2) BaseMessage, (3) 2-tuple of (message type, template); e.g., (â€œhumanâ€, â€œ{user_input}â€), (4) 2-tuple of (message class, template), (5) a string which is shorthand for (â€œhumanâ€, template); e.g., â€œ{user_input}â€. template_format â€“ format of the template. Defaults to â€œf-stringâ€. input_variables â€“ A list of the names of the variables whose values are required as inputs to the prompt. optional_variables â€“ A list of the names of the variables for placeholder or MessagePlaceholder that are optional. These variables are auto inferred from the prompt and user need not provide them. partial_variables â€“ A dictionary of the partial variables the prompt template carries. Partial variables populate the template so that you donâ€™t need to pass them in every time you call the prompt. validate_template â€“ Whether to validate the template. input_types â€“ A dictionary of the types of the variables the prompt template expects. If not provided, all variables are assumed to be strings. Returns: A chat prompt template. Examples Instantiation from a list of message templates: template = ChatPromptTemplate([ (\"human\", \"Hello, how are you?\"), (\"ai\", \"I'm doing well, thanks!\"), (\"human\", \"That's good to hear.\"), ]) Copy to clipboard Instantiation from mixed message formats: template = ChatPromptTemplate([ SystemMessage(content=\"hello\"), (\"human\", \"Hello, how are you?\"), ]) Copy to clipboard Note ChatPromptTemplate implements the standard Runnable Interface. ğŸƒ The Runnable Interface has additional methods that are available on runnables, such as with_config, with_types, with_retry, assign, bind, get_graph, and more. param input_types: Dict[str, Any] [Optional]# A dictionary of the types of the variables the prompt template expects. If not provided, all variables are assumed to be strings. param input_variables: list[str] [Required]# A list of the names of the variables whose values are required as inputs to the prompt. param messages: Annotated[list[MessageLike], SkipValidation()] [Required]# List of messages consisting of either message prompt templates or messages. param metadata: Dict[str, Any] | None = None# Metadata to be used for tracing. param optional_variables: list[str] = []# optional_variables: A list of the names of the variables for placeholder or MessagePlaceholder that are optional. These variables are auto inferred from the prompt and user need not provide them. param output_parser: BaseOutputParser | None = None# How to parse the output of calling an LLM on this formatted prompt. param partial_variables: Mapping[str, Any] [Optional]# A dictionary of the partial variables the prompt template carries. Partial variables populate the template so that you donâ€™t need to pass them in every time you call the prompt. param tags: list[str] | None = None# Tags to be used for tracing. param validate_template: bool = False# Whether or not to try validating the template. classmethod from_messages( messages: Sequence[MessageLikeRepresentation], template_format: PromptTemplateFormat = 'f-string', ) â†’ ChatPromptTemplate[source]# Create a chat prompt template from a variety of message formats. Examples Instantiation from a list of message templates: template = ChatPromptTemplate.from_messages([ (\"human\", \"Hello, how are you?\"), (\"ai\", \"I'm doing well, thanks!\"), (\"human\", \"That's good to hear.\"), ]) Copy to clipboard Instantiation from mixed message formats: template = ChatPromptTemplate.from_messages([ SystemMessage(content=\"hello\"), (\"human\", \"Hello, how are you?\"), ]) Copy to clipboard Parameters: messages (Sequence[MessageLikeRepresentation]) â€“ sequence of message representations. A message can be represented using the following formats: (1) BaseMessagePromptTemplate, (2) BaseMessage, (3) 2-tuple of (message type, template); e.g., (â€œhumanâ€, â€œ{user_input}â€), (4) 2-tuple of (message class, template), (5) a string which is shorthand for (â€œhumanâ€, template); e.g., â€œ{user_input}â€. template_format (PromptTemplateFormat) â€“ format of the template. Defaults to â€œf-stringâ€. Returns: a chat prompt template. Return type: ChatPromptTemplate classmethod from_role_strings( string_messages: list[tuple[str, str]], ) â†’ ChatPromptTemplate[source]# Deprecated since version 0.0.1: Use from_messages() instead. Create a chat prompt template from a list of (role, template) tuples. Parameters: string_messages (list[tuple[str, str]]) â€“ list of (role, template) tuples. Returns: a chat prompt template. Return type: ChatPromptTemplate classmethod from_strings( string_messages: list[tuple[type[BaseMessagePromptTemplate], str]], ) â†’ ChatPromptTemplate[source]# Deprecated since version 0.0.1: Use from_messages() instead. Create a chat prompt template from a list of (role class, template) tuples. Parameters: string_messages (list[tuple[type[BaseMessagePromptTemplate], str]]) â€“ list of (role class, template) tuples. Returns: a chat prompt template. Return type: ChatPromptTemplate classmethod from_template( template: str, **kwargs: Any, ) â†’ ChatPromptTemplate[source]# Create a chat prompt template from a template string. Creates a chat template consisting of a single message assumed to be from the human. Parameters: template (str) â€“ template string **kwargs (Any) â€“ keyword arguments to pass to the constructor. Returns: A new instance of this class. Return type: ChatPromptTemplate async abatch( inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None, ) â†’ list[Output]# Default implementation runs ainvoke in parallel using asyncio.gather. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying Runnable uses an API which supports a batch mode. Parameters: inputs (list[Input]) â€“ A list of inputs to the Runnable. config (RunnableConfig | list[RunnableConfig] | None) â€“ A config to use when invoking the Runnable. The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys. Please refer to the RunnableConfig for more details. Defaults to None. return_exceptions (bool) â€“ Whether to return exceptions instead of raising them. Defaults to False. kwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable. Returns: A list of outputs from the Runnable. Return type: list[Output] async abatch_as_completed( inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None, ) â†’ AsyncIterator[tuple[int, Output | Exception]]# Run ainvoke in parallel on a list of inputs. Yields results as they complete. Parameters: inputs (Sequence[Input]) â€“ A list of inputs to the Runnable. config (RunnableConfig | Sequence[RunnableConfig] | None) â€“ A config to use when invoking the Runnable. The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys. Please refer to the RunnableConfig for more details. Defaults to None. return_exceptions (bool) â€“ Whether to return exceptions instead of raising them. Defaults to False. kwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable. Yields: A tuple of the index of the input and the output from the Runnable. Return type: AsyncIterator[tuple[int, Output | Exception]] async aformat(**kwargs: Any) â†’ str# Async format the chat template into a string. Parameters: **kwargs (Any) â€“ keyword arguments to use for filling in template variables in all the template messages in this chat template. Returns: formatted string. Return type: str async aformat_messages( **kwargs: Any, ) â†’ list[BaseMessage][source]# Async format the chat template into a list of finalized messages. Parameters: **kwargs (Any) â€“ keyword arguments to use for filling in template variables in all the template messages in this chat template. Returns: list of formatted messages. Raises: ValueError â€“ If unexpected input. Return type: list[BaseMessage] async aformat_prompt( **kwargs: Any, ) â†’ ChatPromptValue# Async format prompt. Should return a ChatPromptValue. Parameters: **kwargs (Any) â€“ Keyword arguments to use for formatting. Returns: PromptValue. Return type: ChatPromptValue async ainvoke( input: dict, config: RunnableConfig | None = None, **kwargs: Any, ) â†’ PromptValue# Async invoke the prompt. Parameters: input (dict) â€“ Dict, input to the prompt. config (RunnableConfig | None) â€“ RunnableConfig, configuration for the prompt. kwargs (Any) Returns: The output of the prompt. Return type: PromptValue append( message: BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str | dict[str, Any], ) â†’ None[source]# Append a message to the end of the chat template. Parameters: message (BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str | dict[str, Any]) â€“ representation of a message to append. Return type: None async astream( input: Input, config: RunnableConfig | None = None, **kwargs: Any | None, ) â†’ AsyncIterator[Output]# Default implementation of astream, which calls ainvoke. Subclasses should override this method if they support streaming output. Parameters: input (Input) â€“ The input to the Runnable. config (RunnableConfig | None) â€“ The config to use for the Runnable. Defaults to None. kwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable. Yields: The output of the Runnable. Return type: AsyncIterator[Output] async astream_events( input: Any, config: RunnableConfig | None = None, *, version: Literal['v1', 'v2'] = 'v2', include_names: Sequence[str] | None = None, include_types: Sequence[str] | None = None, include_tags: Sequence[str] | None = None, exclude_names: Sequence[str] | None = None, exclude_types: Sequence[str] | None = None, exclude_tags: Sequence[str] | None = None, **kwargs: Any, ) â†’ AsyncIterator[StreamEvent]# Generate a stream of events. Use to create an iterator over StreamEvents that provide real-time information about the progress of the Runnable, including StreamEvents from intermediate results. A StreamEvent is a dictionary with the following schema: event: str - Event names are of the format: on_[runnable_type]_(start|stream|end). name: str - The name of the Runnable that generated the event. run_id: str - randomly generated ID associated with the given execution of the Runnable that emitted the event. A child Runnable that gets invoked as part of the execution of a parent Runnable is assigned its own unique ID. parent_ids: list[str] - The IDs of the parent runnables that generated the event. The root Runnable will have an empty list. The order of the parent IDs is from the root to the immediate parent. Only available for v2 version of the API. The v1 version of the API will return an empty list. tags: Optional[list[str]] - The tags of the Runnable that generated the event. metadata: Optional[dict[str, Any]] - The metadata of the Runnable that generated the event. data: dict[str, Any] Below is a table that illustrates some events that might be emitted by various chains. Metadata fields have been omitted from the table for brevity. Chain definitions have been included after the table. Note This reference table is for the v2 version of the schema. event name chunk input output on_chat_model_start [model name] {\"messages\": [[SystemMessage, HumanMessage]]} on_chat_model_stream [model name] AIMessageChunk(content=\"hello\") on_chat_model_end [model name] {\"messages\": [[SystemMessage, HumanMessage]]} AIMessageChunk(content=\"hello world\") on_llm_start [model name] {'input': 'hello'} on_llm_stream [model name] ``â€™Helloâ€™ `` on_llm_end [model name] 'Hello human!' on_chain_start format_docs on_chain_stream format_docs 'hello world!, goodbye world!' on_chain_end format_docs [Document(...)] 'hello world!, goodbye world!' on_tool_start some_tool {\"x\": 1, \"y\": \"2\"} on_tool_end some_tool {\"x\": 1, \"y\": \"2\"} on_retriever_start [retriever name] {\"query\": \"hello\"} on_retriever_end [retriever name] {\"query\": \"hello\"} [Document(...), ..] on_prompt_start [template_name] {\"question\": \"hello\"} on_prompt_end [template_name] {\"question\": \"hello\"} ChatPromptValue(messages: [SystemMessage, ...]) In addition to the standard events, users can also dispatch custom events (see example below). Custom events will be only be surfaced with in the v2 version of the API! A custom event has following format: Attribute Type Description name str A user defined name for the event. data Any The data associated with the event. This can be anything, though we suggest making it JSON serializable. Here are declarations associated with the standard events shown above: format_docs: def format_docs(docs: list[Document]) -> str: '''Format the docs.''' return \", \".join([doc.page_content for doc in docs]) format_docs = RunnableLambda(format_docs) Copy to clipboard some_tool: @tool def some_tool(x: int, y: str) -> dict: '''Some_tool.''' return {\"x\": x, \"y\": y} Copy to clipboard prompt: template = ChatPromptTemplate.from_messages( [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")] ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]}) Copy to clipboard Example: from langchain_core.runnables import RunnableLambda async def reverse(s: str) -> str: return s[::-1] chain = RunnableLambda(func=reverse) events = [ event async for event in chain.astream_events(\"hello\", version=\"v2\") ] # will produce the following events (run_id, and parent_ids # has been omitted for brevity): [ { \"data\": {\"input\": \"hello\"}, \"event\": \"on_chain_start\", \"metadata\": {}, \"name\": \"reverse\", \"tags\": [], }, { \"data\": {\"chunk\": \"olleh\"}, \"event\": \"on_chain_stream\", \"metadata\": {}, \"name\": \"reverse\", \"tags\": [], }, { \"data\": {\"output\": \"olleh\"}, \"event\": \"on_chain_end\", \"metadata\": {}, \"name\": \"reverse\", \"tags\": [], }, ] Copy to clipboard Example: Dispatch Custom Event from langchain_core.callbacks.manager import ( adispatch_custom_event, ) from langchain_core.runnables import RunnableLambda, RunnableConfig import asyncio async def slow_thing(some_input: str, config: RunnableConfig) -> str: \"\"\"Do something that takes a long time.\"\"\" await asyncio.sleep(1) # Placeholder for some slow operation await adispatch_custom_event( \"progress_event\", {\"message\": \"Finished step 1 of 3\"}, config=config # Must be included for python < 3.10 ) await asyncio.sleep(1) # Placeholder for some slow operation await adispatch_custom_event( \"progress_event\", {\"message\": \"Finished step 2 of 3\"}, config=config # Must be included for python < 3.10 ) await asyncio.sleep(1) # Placeholder for some slow operation return \"Done\" slow_thing = RunnableLambda(slow_thing) async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"): print(event) Copy to clipboard Parameters: input (Any) â€“ The input to the Runnable. config (Optional[RunnableConfig]) â€“ The config to use for the Runnable. version (Literal['v1', 'v2']) â€“ The version of the schema to use either 'v2' or 'v1'. Users should use 'v2'. 'v1' is for backwards compatibility and will be deprecated in 0.4.0. No default will be assigned until the API is stabilized. custom events will only be surfaced in 'v2'. include_names (Optional[Sequence[str]]) â€“ Only include events from Runnables with matching names. include_types (Optional[Sequence[str]]) â€“ Only include events from Runnables with matching types. include_tags (Optional[Sequence[str]]) â€“ Only include events from Runnables with matching tags. exclude_names (Optional[Sequence[str]]) â€“ Exclude events from Runnables with matching names. exclude_types (Optional[Sequence[str]]) â€“ Exclude events from Runnables with matching types. exclude_tags (Optional[Sequence[str]]) â€“ Exclude events from Runnables with matching tags. kwargs (Any) â€“ Additional keyword arguments to pass to the Runnable. These will be passed to astream_log as this implementation of astream_events is built on top of astream_log. Yields: An async stream of StreamEvents. Raises: NotImplementedError â€“ If the version is not 'v1' or 'v2'. Return type: AsyncIterator[StreamEvent] batch( inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None, ) â†’ list[Output]# Default implementation runs invoke in parallel using a thread pool executor. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying Runnable uses an API which supports a batch mode. Parameters: inputs (list[Input]) config (RunnableConfig | list[RunnableConfig] | None) return_exceptions (bool) kwargs (Any | None) Return type: list[Output] batch_as_completed( inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None, ) â†’ Iterator[tuple[int, Output | Exception]]# Run invoke in parallel on a list of inputs. Yields results as they complete. Parameters: inputs (Sequence[Input]) config (RunnableConfig | Sequence[RunnableConfig] | None) return_exceptions (bool) kwargs (Any | None) Return type: Iterator[tuple[int, Output | Exception]] bind( **kwargs: Any, ) â†’ Runnable[Input, Output]# Bind arguments to a Runnable, returning a new Runnable. Useful when a Runnable in a chain requires an argument that is not in the output of the previous Runnable or included in the user input. Parameters: kwargs (Any) â€“ The arguments to bind to the Runnable. Returns: A new Runnable with the arguments bound. Return type: Runnable[Input, Output] Example: from langchain_ollama import ChatOllama from langchain_core.output_parsers import StrOutputParser llm = ChatOllama(model='llama2') # Without bind. chain = ( llm | StrOutputParser() ) chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\") # Output is 'One two three four five.' # With bind. chain = ( llm.bind(stop=[\"three\"]) | StrOutputParser() ) chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\") # Output is 'One two' Copy to clipboard configurable_alternatives( which: ConfigurableField, *, default_key: str = 'default', prefix_keys: bool = False, **kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]], ) â†’ RunnableSerializable# Configure alternatives for Runnables that can be set at runtime. Parameters: which (ConfigurableField) â€“ The ConfigurableField instance that will be used to select the alternative. default_key (str) â€“ The default key to use if no alternative is selected. Defaults to 'default'. prefix_keys (bool) â€“ Whether to prefix the keys with the ConfigurableField id. Defaults to False. **kwargs (Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) â€“ A dictionary of keys to Runnable instances or callables that return Runnable instances. Returns: A new Runnable with the alternatives configured. Return type: RunnableSerializable from langchain_anthropic import ChatAnthropic from langchain_core.runnables.utils import ConfigurableField from langchain_openai import ChatOpenAI model = ChatAnthropic( model_name=\"claude-3-7-sonnet-20250219\" ).configurable_alternatives( ConfigurableField(id=\"llm\"), default_key=\"anthropic\", openai=ChatOpenAI() ) # uses the default model ChatAnthropic print(model.invoke(\"which organization created you?\").content) # uses ChatOpenAI print( model.with_config( configurable={\"llm\": \"openai\"} ).invoke(\"which organization created you?\").content ) Copy to clipboard configurable_fields( **kwargs: ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption, ) â†’ RunnableSerializable# Configure particular Runnable fields at runtime. Parameters: **kwargs (ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) â€“ A dictionary of ConfigurableField instances to configure. Returns: A new Runnable with the fields configured. Return type: RunnableSerializable from langchain_core.runnables import ConfigurableField from langchain_openai import ChatOpenAI model = ChatOpenAI(max_tokens=20).configurable_fields( max_tokens=ConfigurableField( id=\"output_token_number\", name=\"Max tokens in the output\", description=\"The maximum number of tokens in the output\", ) ) # max_tokens = 20 print( \"max_tokens_20: \", model.invoke(\"tell me something about chess\").content ) # max_tokens = 200 print(\"max_tokens_200: \", model.with_config( configurable={\"output_token_number\": 200} ).invoke(\"tell me something about chess\").content ) Copy to clipboard extend( messages: Sequence[MessageLikeRepresentation], ) â†’ None[source]# Extend the chat template with a sequence of messages. Parameters: messages (Sequence[MessageLikeRepresentation]) â€“ sequence of message representations to append. Return type: None format(**kwargs: Any) â†’ str# Format the chat template into a string. Parameters: **kwargs (Any) â€“ keyword arguments to use for filling in template variables in all the template messages in this chat template. Returns: formatted string. Return type: str format_messages( **kwargs: Any, ) â†’ list[BaseMessage][source]# Format the chat template into a list of finalized messages. Parameters: **kwargs (Any) â€“ keyword arguments to use for filling in template variables in all the template messages in this chat template. Returns: list of formatted messages. Return type: list[BaseMessage] format_prompt( **kwargs: Any, ) â†’ ChatPromptValue# Format prompt. Should return a ChatPromptValue. Parameters: **kwargs (Any) â€“ Keyword arguments to use for formatting. Returns: ChatPromptValue. Return type: ChatPromptValue invoke( input: dict, config: RunnableConfig | None = None, **kwargs: Any, ) â†’ PromptValue# Invoke the prompt. Parameters: input (dict) â€“ Dict, input to the prompt. config (RunnableConfig | None) â€“ RunnableConfig, configuration for the prompt. kwargs (Any) Returns: The output of the prompt. Return type: PromptValue partial( **kwargs: Any, ) â†’ ChatPromptTemplate[source]# Get a new ChatPromptTemplate with some input variables already filled in. Parameters: **kwargs (Any) â€“ keyword arguments to use for filling in template variables. Ought to be a subset of the input variables. Returns: A new ChatPromptTemplate. Return type: ChatPromptTemplate Example from langchain_core.prompts import ChatPromptTemplate template = ChatPromptTemplate.from_messages( [ (\"system\", \"You are an AI assistant named {name}.\"), (\"human\", \"Hi I'm {user}\"), (\"ai\", \"Hi there, {user}, I'm {name}.\"), (\"human\", \"{input}\"), ] ) template2 = template.partial(user=\"Lucy\", name=\"R2D2\") template2.format_messages(input=\"hello\") Copy to clipboard pretty_print() â†’ None# Print a human-readable representation. Return type: None pretty_repr(html: bool = False) â†’ str[source]# Human-readable representation. Parameters: html (bool) â€“ Whether to format as HTML. Defaults to False. Returns: Human-readable representation. Return type: str save( file_path: Path | str, ) â†’ None[source]# Save prompt to file. Parameters: file_path (Path | str) â€“ path to file. Return type: None stream( input: Input, config: RunnableConfig | None = None, **kwargs: Any | None, ) â†’ Iterator[Output]# Default implementation of stream, which calls invoke. Subclasses should override this method if they support streaming output. Parameters: input (Input) â€“ The input to the Runnable. config (RunnableConfig | None) â€“ The config to use for the Runnable. Defaults to None. kwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable. Yields: The output of the Runnable. Return type: Iterator[Output] with_alisteners( *, on_start: AsyncListener | None = None, on_end: AsyncListener | None = None, on_error: AsyncListener | None = None, ) â†’ Runnable[Input, Output]# Bind async lifecycle listeners to a Runnable, returning a new Runnable. The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run. Parameters: on_start (Optional[AsyncListener]) â€“ Called asynchronously before the Runnable starts running, with the Run object. Defaults to None. on_end (Optional[AsyncListener]) â€“ Called asynchronously after the Runnable finishes running, with the Run object. Defaults to None. on_error (Optional[AsyncListener]) â€“ Called asynchronously if the Runnable throws an error, with the Run object. Defaults to None. Returns: A new Runnable with the listeners bound. Return type: Runnable[Input, Output] Example: from langchain_core.runnables import RunnableLambda, Runnable from datetime import datetime, timezone import time import asyncio def format_t(timestamp: float) -> str: return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat() async def test_runnable(time_to_sleep : int): print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\") await asyncio.sleep(time_to_sleep) print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\") async def fn_start(run_obj : Runnable): print(f\"on start callback starts at {format_t(time.time())}\") await asyncio.sleep(3) print(f\"on start callback ends at {format_t(time.time())}\") async def fn_end(run_obj : Runnable): print(f\"on end callback starts at {format_t(time.time())}\") await asyncio.sleep(2) print(f\"on end callback ends at {format_t(time.time())}\") runnable = RunnableLambda(test_runnable).with_alisteners( on_start=fn_start, on_end=fn_end ) async def concurrent_runs(): await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3)) asyncio.run(concurrent_runs()) Result: on start callback starts at 2025-03-01T07:05:22.875378+00:00 on start callback starts at 2025-03-01T07:05:22.875495+00:00 on start callback ends at 2025-03-01T07:05:25.878862+00:00 on start callback ends at 2025-03-01T07:05:25.878947+00:00 Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00 Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00 Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00 on end callback starts at 2025-03-01T07:05:27.882360+00:00 Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00 on end callback starts at 2025-03-01T07:05:28.882428+00:00 on end callback ends at 2025-03-01T07:05:29.883893+00:00 on end callback ends at 2025-03-01T07:05:30.884831+00:00 Copy to clipboard with_config( config: RunnableConfig | None = None, **kwargs: Any, ) â†’ Runnable[Input, Output]# Bind config to a Runnable, returning a new Runnable. Parameters: config (RunnableConfig | None) â€“ The config to bind to the Runnable. kwargs (Any) â€“ Additional keyword arguments to pass to the Runnable. Returns: A new Runnable with the config bound. Return type: Runnable[Input, Output] with_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: tuple[type[BaseException], ...] = (<class 'Exception'>,), exception_key: Optional[str] = None) â†’ RunnableWithFallbacksT[Input, Output]# Add fallbacks to a Runnable, returning a new Runnable. The new Runnable will try the original Runnable, and then each fallback in order, upon failures. Parameters: fallbacks (Sequence[Runnable[Input, Output]]) â€“ A sequence of runnables to try if the original Runnable fails. exceptions_to_handle (tuple[type[BaseException], ...]) â€“ A tuple of exception types to handle. Defaults to (Exception,). exception_key (Optional[str]) â€“ If string is specified then handled exceptions will be passed to fallbacks as part of the input under the specified key. If None, exceptions will not be passed to fallbacks. If used, the base Runnable and its fallbacks must accept a dictionary as input. Defaults to None. Returns: A new Runnable that will try the original Runnable, and then each fallback in order, upon failures. Return type: RunnableWithFallbacksT[Input, Output] Example from typing import Iterator from langchain_core.runnables import RunnableGenerator def _generate_immediate_error(input: Iterator) -> Iterator[str]: raise ValueError() yield \"\" def _generate(input: Iterator) -> Iterator[str]: yield from \"foo bar\" runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks( [RunnableGenerator(_generate)] ) print(''.join(runnable.stream({}))) #foo bar Copy to clipboard Parameters: fallbacks (Sequence[Runnable[Input, Output]]) â€“ A sequence of runnables to try if the original Runnable fails. exceptions_to_handle (tuple[type[BaseException], ...]) â€“ A tuple of exception types to handle. exception_key (Optional[str]) â€“ If string is specified then handled exceptions will be passed to fallbacks as part of the input under the specified key. If None, exceptions will not be passed to fallbacks. If used, the base Runnable and its fallbacks must accept a dictionary as input. Returns: A new Runnable that will try the original Runnable, and then each fallback in order, upon failures. Return type: RunnableWithFallbacksT[Input, Output] with_listeners( *, on_start: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_end: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_error: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, ) â†’ Runnable[Input, Output]# Bind lifecycle listeners to a Runnable, returning a new Runnable. The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run. Parameters: on_start (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called before the Runnable starts running, with the Run object. Defaults to None. on_end (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called after the Runnable finishes running, with the Run object. Defaults to None. on_error (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called if the Runnable throws an error, with the Run object. Defaults to None. Returns: A new Runnable with the listeners bound. Return type: Runnable[Input, Output] Example: from langchain_core.runnables import RunnableLambda from langchain_core.tracers.schemas import Run import time def test_runnable(time_to_sleep : int): time.sleep(time_to_sleep) def fn_start(run_obj: Run): print(\"start_time:\", run_obj.start_time) def fn_end(run_obj: Run): print(\"end_time:\", run_obj.end_time) chain = RunnableLambda(test_runnable).with_listeners( on_start=fn_start, on_end=fn_end ) chain.invoke(2) Copy to clipboard with_retry(*, retry_if_exception_type: tuple[type[BaseException], ...] = (<class 'Exception'>,), wait_exponential_jitter: bool = True, exponential_jitter_params: Optional[ExponentialJitterParams] = None, stop_after_attempt: int = 3) â†’ Runnable[Input, Output]# Create a new Runnable that retries the original Runnable on exceptions. Parameters: retry_if_exception_type (tuple[type[BaseException], ...]) â€“ A tuple of exception types to retry on. Defaults to (Exception,). wait_exponential_jitter (bool) â€“ Whether to add jitter to the wait time between retries. Defaults to True. stop_after_attempt (int) â€“ The maximum number of attempts to make before giving up. Defaults to 3. exponential_jitter_params (Optional[ExponentialJitterParams]) â€“ Parameters for tenacity.wait_exponential_jitter. Namely: initial, max, exp_base, and jitter (all float values). Returns: A new Runnable that retries the original Runnable on exceptions. Return type: Runnable[Input, Output] Example: from langchain_core.runnables import RunnableLambda count = 0 def _lambda(x: int) -> None: global count count = count + 1 if x == 1: raise ValueError(\"x is 1\") else: pass runnable = RunnableLambda(_lambda) try: runnable.with_retry( stop_after_attempt=2, retry_if_exception_type=(ValueError,), ).invoke(1) except ValueError: pass assert (count == 2) Copy to clipboard with_types( *, input_type: type[Input] | None = None, output_type: type[Output] | None = None, ) â†’ Runnable[Input, Output]# Bind input and output types to a Runnable, returning a new Runnable. Parameters: input_type (type[Input] | None) â€“ The input type to bind to the Runnable. Defaults to None. output_type (type[Output] | None) â€“ The output type to bind to the Runnable. Defaults to None. Returns: A new Runnable with the types bound. Return type: Runnable[Input, Output] Examples using ChatPromptTemplate # Basic example (short documents) # Example # Legacy AWS DynamoDB Activeloop Deep Memory Apache Cassandra ApertureDB ArxivRetriever AskNews AzureAISearchRetriever AzureChatOpenAI Build a Chatbot Build a Local RAG Application Build a PDF ingestion and Question/Answering system Build a Query Analysis System Build a Retrieval Augmented Generation (RAG) App Build a Simple LLM Application with LCEL Build an Extraction Chain ChatAI21 ChatAnthropic ChatBedrock ChatDatabricks ChatFireworks ChatGoogleGenerativeAI ChatGroq ChatMistralAI ChatNVIDIA ChatOCIGenAI ChatOllama ChatOpenAI ChatPerplexity ChatTogether ChatUpstage ChatVertexAI ChatWatsonx ChatYI Classify Text into Labels Cohere Conceptual guide Context Conversational RAG Couchbase Databricks Unity Catalog (UC) Eden AI ElasticsearchRetriever Facebook Messenger Fiddler Figma FinancialDatasets Toolkit Fleet AI Context Google AlloyDB for PostgreSQL Google El Carro Oracle Google SQL for MySQL Google SQL for PostgreSQL Google SQL for SQL Server Google Vertex AI Search How deal with high cardinality categoricals when doing query analysis How to add a semantic layer over graph database How to add ad-hoc tool calling capability to LLMs and Chat Models How to add chat history How to add default invocation args to a Runnable How to add examples to the prompt for query analysis How to add fallbacks to a runnable How to add memory to chatbots How to add message history How to add retrieval to chatbots How to add tools to chatbots How to add values to a chainâ€™s state How to attach callbacks to a runnable How to chain runnables How to convert Runnables as Tools How to create a custom LLM class How to create a dynamic (self-constructing) chain How to create custom callback handlers How to create tools How to deal with large databases when doing SQL question-answering How to debug your LLM apps How to do per-user retrieval How to do query validation as part of SQL question-answering How to do question answering over CSVs How to do tool/function calling How to get a RAG application to add citations How to get your RAG application to return sources How to handle cases where no queries are generated How to handle long text when doing extraction How to handle multiple queries when doing query analysis How to handle multiple retrievers when doing query analysis How to handle tool errors How to inspect runnables How to invoke runnables in parallel How to map values to a graph database How to migrate from legacy LangChain agents to LangGraph How to pass callbacks in at runtime How to pass through arguments from one step to the next How to propagate callbacks constructor How to retrieve using multiple vectors per document How to return structured data from a model How to run custom functions How to save and load LangChain objects How to stream events from a tool How to stream results from your RAG application How to stream runnables How to summarize text in a single LLM call How to summarize text through iterative refinement How to summarize text through parallelization How to track token usage in ChatModels How to use few shot examples in chat models How to use few-shot prompting with tool calling How to use multimodal prompts How to use prompting alone (no tool calling) to do extraction How to use reference examples when doing extraction Hybrid Search Image captions Jaguar Vector Database JinaChat Kinetica Language To SQL Chat Model LangChain Expression Language Cheatsheet LangSmith LLM Runs Llama.cpp Llama2Chat Load docs MLflow Maritalk MongoDB NVIDIA NIMs OllamaFunctions OllamaLLM OpenAI metadata tagger RAGatouille Redis Riza Code Interpreter SQL (SQLAlchemy) SQLite Streamlit Summarize Text Tavily Search TavilySearchAPIRetriever TiDB UpTrain Vector stores and retrievers Weaviate WikipediaRetriever Yellowbrick You.com Yuan2.0 ZepCloudChatMessageHistory iMessage vLLM Chat ğŸ¦œï¸ğŸ“ LangServe On this page",
      "timestamp": "2025-08-24 15:29:58"
    },
    {
      "url": "https://python.langchain.com/docs/contributing/repo_structure",
      "title": "Repository Structure | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Repository Structure | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page If you plan on contributing to LangChain code or documentation, it can be useful to understand the high level structure of the repository. LangChain is organized as a monorepo that contains multiple packages. You can check out our installation guide for more on how they fit together. Here's the structure visualized as a tree: .â”œâ”€â”€ cookbook # Tutorials and examplesâ”œâ”€â”€ docs # Contains content for the documentation here: https://python.langchain.com/â”œâ”€â”€ libsâ”‚ â”œâ”€â”€ langchainâ”‚ â”‚ â”œâ”€â”€ langchainâ”‚ â”‚ â”œâ”€â”€ tests/unit_tests # Unit tests (present in each package not shown for brevity)â”‚ â”‚ â”œâ”€â”€ tests/integration_tests # Integration tests (present in each package not shown for brevity)â”‚ â”œâ”€â”€ community # Third-party integrationsâ”‚ â”‚ â”œâ”€â”€ langchain-communityâ”‚ â”œâ”€â”€ core # Base interfaces for key abstractionsâ”‚ â”‚ â”œâ”€â”€ langchain-coreâ”‚ â”œâ”€â”€ experimental # Experimental components and chainsâ”‚ â”‚ â”œâ”€â”€ langchain-experimental| â”œâ”€â”€ cli # Command line interfaceâ”‚ â”‚ â”œâ”€â”€ langchain-cliâ”‚ â”œâ”€â”€ text-splittersâ”‚ â”‚ â”œâ”€â”€ langchain-text-splittersâ”‚ â”œâ”€â”€ standard-testsâ”‚ â”‚ â”œâ”€â”€ langchain-standard-testsâ”‚ â”œâ”€â”€ partnersâ”‚ â”œâ”€â”€ langchain-partner-1â”‚ â”œâ”€â”€ langchain-partner-2â”‚ â”œâ”€â”€ ...â”‚â”œâ”€â”€ templates # A collection of easily deployable reference architectures for a wide variety of tasks. The root directory also contains the following files: pyproject.toml: Dependencies for building docs and linting docs, cookbook. Makefile: A file that contains shortcuts for building, linting and docs and cookbook. There are other files in the root directory level, but their presence should be self-explanatory. Feel free to browse around! Documentationâ€‹ The /docs directory contains the content for the documentation that is shown at python.langchain.com and the associated API Reference. See the documentation guidelines to learn how to contribute to the documentation. Codeâ€‹ The /libs directory contains the code for the LangChain packages. To learn more about how to contribute code see the following guidelines: Code: Learn how to develop in the LangChain codebase. Integrations: Learn how to contribute to third-party integrations to langchain-community or to start a new partner package. Testing: Guidelines to learn how to write tests for the packages. DocumentationCode",
      "timestamp": "2025-08-24 15:30:02"
    },
    {
      "url": "https://python.langchain.com/docs/integrations/tools/tavily_search/",
      "title": "Tavily Search | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Tavily Search | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed. Overviewâ€‹ Integration detailsâ€‹ ClassPackageSerializableJS supportPackage latestTavilySearchlangchain-tavilyâœ…âœ… Tool featuresâ€‹ Returns artifactNative asyncReturn dataPricingâŒâœ…title, URL, content snippet, raw_content, answer, images1,000 free searches / month Setupâ€‹ The integration lives in the langchain-tavily package. %pip install -qU langchain-tavily Credentialsâ€‹ We also need to set our Tavily API key. You can get an API key by visiting this site and creating an account. import getpassimport osif not os.environ.get(\"TAVILY_API_KEY\"): os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API key:\\n\") Instantiationâ€‹ Here we show how to instantiate an instance of the Tavily search tool. The tool accepts various parameters to customize the search. After instantiation we invoke the tool with a simple query. This tool allows you to complete search queries using Tavily's Search API endpoint. Instantiation The tool accepts various parameters during instantiation: max_results (optional, int): Maximum number of search results to return. Default is 5. topic (optional, str): Category of the search. Can be 'general', 'news', or 'finance'. Default is 'general'. include_answer (optional, bool): Include an answer to original query in results. Default is False. include_raw_content (optional, bool): Include cleaned and parsed HTML of each search result. Default is False. include_images (optional, bool): Include a list of query related images in the response. Default is False. include_image_descriptions (optional, bool): Include descriptive text for each image. Default is False. search_depth (optional, str): Depth of the search, either 'basic' or 'advanced'. Default is 'basic'. time_range (optional, str): The time range back from the current date to filter results - 'day', 'week', 'month', or 'year'. Default is None. include_domains (optional, List[str]): List of domains to specifically include. Default is None. exclude_domains (optional, List[str]): List of domains to specifically exclude. Default is None. For a comprehensive overview of the available parameters, refer to the Tavily Search API documentation from langchain_tavily import TavilySearchtool = TavilySearch( max_results=5, topic=\"general\", # include_answer=False, # include_raw_content=False, # include_images=False, # include_image_descriptions=False, # search_depth=\"basic\", # time_range=\"day\", # include_domains=None, # exclude_domains=None) Invocationâ€‹ Invoke directly with argsâ€‹ The Tavily search tool accepts the following arguments during invocation: query (required): A natural language search query The following arguments can also be set during invocation : include_images, search_depth , time_range, include_domains, exclude_domains, include_images For reliability and performance reasons, certain parameters that affect response size cannot be modified during invocation: include_answer and include_raw_content. These limitations prevent unexpected context window issues and ensure consistent results. noteThe optional arguments are available for agents to dynamically set, if you set an argument during instantiation and then invoke the tool with a different value, the tool will use the value you passed during invocation. tool.invoke({\"query\": \"What happened at the last wimbledon\"}) Invoke with ToolCallâ€‹ We can also invoke the tool with a model-generated ToolCall, in which case a ToolMessage will be returned: # This is usually generated by a model, but we'll create a tool call directly for demo purposes.model_generated_tool_call = { \"args\": {\"query\": \"euro 2024 host nation\"}, \"id\": \"1\", \"name\": \"tavily\", \"type\": \"tool_call\",}tool_msg = tool.invoke(model_generated_tool_call)# The content is a JSON string of resultsprint(tool_msg.content[:400]) {\"query\": \"euro 2024 host nation\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"UEFA Euro 2024 - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/UEFA_Euro_2024\", \"content\": \"Tournament details Host country Germany Dates 14 June â€“ 14 July Teams 24 Venue(s) 10 (in 10 host cities) Final positions Champions Spain (4th title) Runners-up England Tournament statisti Use within an agentâ€‹ We can use our tools directly with an agent executor by binding the tool to the agent. This gives the agent the ability to dynamically set the available arguments to the Tavily search tool. In the below example when we ask the agent to find \"What nation hosted the Euro 2024? Include only wikipedia sources.\" the agent will dynamically set the argments and invoke Tavily search tool : Invoking tavily_search with {'query': 'Euro 2024 host nation', 'include_domains': ['wikipedia.org'] Select chat model:Google Geminiâ–¾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") if not os.environ.get(\"OPENAI_API_KEY\"): os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY:\\n\") We will need to install langgraph: %pip install -qU langgraph from langchain_tavily import TavilySearchfrom langgraph.prebuilt import create_react_agent# Initialize Tavily Search Tooltavily_search_tool = TavilySearch( max_results=5, topic=\"general\",)agent = create_react_agent(llm, [tavily_search_tool])user_input = \"What nation hosted the Euro 2024? Include only wikipedia sources.\"for step in agent.stream( {\"messages\": user_input}, stream_mode=\"values\",): step[\"messages\"][-1].pretty_print()API Reference:create_react_agent ================================\u001b[1m Human Message \u001b[0m=================================What nation hosted the Euro 2024? Include only wikipedia sources.==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: tavily_search (call_yxmR4K2uadsQ8LKoyi8JyoLD) Call ID: call_yxmR4K2uadsQ8LKoyi8JyoLD Args: query: Euro 2024 host nation include_domains: ['wikipedia.org']=================================\u001b[1m Tool Message \u001b[0m=================================Name: tavily_search{\"query\": \"Euro 2024 host nation\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"UEFA Euro 2024 - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/UEFA_Euro_2024\", \"content\": \"Tournament details Host country Germany Dates 14 June â€“ 14 July Teams 24 Venue(s) 10 (in 10 host cities) Final positions Champions Spain (4th title) Runners-up England Tournament statistics Matches played 51 Goals scored 117 (2.29 per match) Attendance 2,681,288 (52,574 per match) Top scorer(s) Harry Kane Georges Mikautadze Jamal Musiala Cody Gakpo Ivan Schranz Dani Olmo (3 goals each) Best player(s) Rodri Best young player Lamine Yamal â† 2020 2028 â†’ The 2024 UEFA European Football Championship, commonly referred to as UEFA Euro 2024 (stylised as UEFA EURO 2024) or simply Euro 2024, was the 17th UEFA European Championship, the quadrennial international football championship organised by UEFA for the European men's national teams of their member associations. Germany hosted the tournament, which took place from 14 June to 14 July 2024. The tournament involved 24 teams, with Georgia making their European Championship debut. [4] Host nation Germany were eliminated by Spain in the quarter-finals; Spain went on to win the tournament for a record fourth time after defeating England 2â€“1 in the final.\", \"score\": 0.9104262, \"raw_content\": null}, {\"title\": \"UEFA Euro 2024 - Simple English Wikipedia, the free encyclopedia\", \"url\": \"https://simple.wikipedia.org/wiki/UEFA_Euro_2024\", \"content\": \"The 2024 UEFA European Football Championship, also known as UEFA Euro 2024 or simply Euro 2024, was the 17th edition of the UEFA European Championship. Germany was hosting the tournament. ... The UEFA Executive Committee voted for the host in a secret ballot, with only a simple majority (more than half of the valid votes) required to determine\", \"score\": 0.81418616, \"raw_content\": null}, {\"title\": \"Championnat d'Europe de football 2024 â€” WikipÃ©dia\", \"url\": \"https://fr.wikipedia.org/wiki/Championnat_d'Europe_de_football_2024\", \"content\": \"Le Championnat d'Europe de l'UEFA de football 2024 est la 17 e Ã©dition du Championnat d'Europe de football, communÃ©ment abrÃ©gÃ© en Euro 2024, compÃ©tition organisÃ©e par l'UEFA et rassemblant les meilleures Ã©quipes nationales masculines europÃ©ennes. L'Allemagne est dÃ©signÃ©e pays organisateur de la compÃ©tition le 27 septembre 2018. C'est la troisiÃ¨me fois que des matches du Championnat\", \"score\": 0.8055255, \"raw_content\": null}, {\"title\": \"UEFA Euro 2024 bids - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/UEFA_Euro_2024_bids\", \"content\": \"The bidding process of UEFA Euro 2024 ended on 27 September 2018 in Nyon, Switzerland, when Germany was announced to be the host. [1] Two bids came before the deadline, 3 March 2017, which were Germany and Turkey as single bids. ... Press agencies revealed on 24 October 2013, that the European football governing body UEFA would have decided on\", \"score\": 0.7882741, \"raw_content\": null}, {\"title\": \"2024 UEFA European Under-19 Championship - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/2024_UEFA_European_Under-19_Championship\", \"content\": \"The 2024 UEFA European Under-19 Championship (also known as UEFA Under-19 Euro 2024) was the 21st edition of the UEFA European Under-19 Championship (71st edition if the Under-18 and Junior eras are included), the annual international youth football championship organised by UEFA for the men's under-19 national teams of Europe. Northern Ireland hosted the tournament from 15 to 28 July 2024.\", \"score\": 0.7783298, \"raw_content\": null}], \"response_time\": 1.67}==================================\u001b[1m Ai Message \u001b[0m==================================The nation that hosted Euro 2024 was Germany. You can find more information on the [Wikipedia page for UEFA Euro 2024](https://en.wikipedia.org/wiki/UEFA_Euro_2024). API referenceâ€‹ For detailed documentation of all Tavily Search API features and configurations head to the API reference. Relatedâ€‹ Tool conceptual guide Tool how-to guides OverviewIntegration detailsTool featuresSetupCredentialsInstantiationInvocationInvoke directly with argsInvoke with ToolCallUse within an agentAPI referenceRelated",
      "timestamp": "2025-08-24 15:30:06"
    },
    {
      "url": "https://python.langchain.com/docs/concepts/agents/",
      "title": "Agents | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Agents | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page By themselves, language models can't take actions - they just output text. Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions. LangGraph is an extension of LangChain specifically aimed at creating highly controllable and customizable agents. We recommend that you use LangGraph for building agents. Please see the following resources for more information: LangGraph docs on common agent architectures Pre-built agents in LangGraph Legacy agent concept: AgentExecutorâ€‹ LangChain previously introduced the AgentExecutor as a runtime for agents. While it served as an excellent starting point, its limitations became apparent when dealing with more sophisticated and customized agents. As a result, we're gradually phasing out AgentExecutor in favor of more flexible solutions in LangGraph. Transitioning from AgentExecutor to LangGraphâ€‹ If you're currently using AgentExecutor, don't worry! We've prepared resources to help you: For those who still need to use AgentExecutor, we offer a comprehensive guide on how to use AgentExecutor. However, we strongly recommend transitioning to LangGraph for improved flexibility and control. To facilitate this transition, we've created a detailed migration guide to help you move from AgentExecutor to LangGraph seamlessly. Legacy agent concept: AgentExecutorTransitioning from AgentExecutor to LangGraph",
      "timestamp": "2025-08-24 15:30:09"
    },
    {
      "url": "https://python.langchain.com/docs/how_to/custom_tools/",
      "title": "How to create tools | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "How to create tools | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page When constructing an agent, you will need to provide it with a list of Tools that it can use. Besides the actual function that is called, the Tool consists of several components: AttributeTypeDescriptionnamestrMust be unique within a set of tools provided to an LLM or agent.descriptionstrDescribes what the tool does. Used as context by the LLM or agent.args_schemapydantic.BaseModelOptional but recommended, and required if using callback handlers. It can be used to provide more information (e.g., few-shot examples) or validation for expected parameters.return_directbooleanOnly relevant for agents. When True, after invoking the given tool, the agent will stop and return the result direcly to the user. LangChain supports the creation of tools from: Functions; LangChain Runnables; By sub-classing from BaseTool -- This is the most flexible method, it provides the largest degree of control, at the expense of more effort and code. Creating tools from functions may be sufficient for most use cases, and can be done via a simple @tool decorator. If more configuration is needed-- e.g., specification of both sync and async implementations-- one can also use the StructuredTool.from_function class method. In this guide we provide an overview of these methods. tipModels will perform better if the tools have well chosen names, descriptions and JSON schemas. Creating tools from functionsâ€‹ @tool decoratorâ€‹ This @tool decorator is the simplest way to define a custom tool. The decorator uses the function name as the tool name by default, but this can be overridden by passing a string as the first argument. Additionally, the decorator will use the function's docstring as the tool's description - so a docstring MUST be provided. from langchain_core.tools import tool@tooldef multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * b# Let's inspect some of the attributes associated with the tool.print(multiply.name)print(multiply.description)print(multiply.args)API Reference:tool multiplyMultiply two numbers.{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}} Or create an async implementation, like this: from langchain_core.tools import tool@toolasync def amultiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * bAPI Reference:tool Note that @tool supports parsing of annotations, nested schemas, and other features: from typing import Annotated, List@tooldef multiply_by_max( a: Annotated[int, \"scale factor\"], b: Annotated[List[int], \"list of ints over which to take maximum\"],) -> int: \"\"\"Multiply a by the maximum of b.\"\"\" return a * max(b)print(multiply_by_max.args_schema.model_json_schema()) {'description': 'Multiply a by the maximum of b.', 'properties': {'a': {'description': 'scale factor', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'list of ints over which to take maximum', 'items': {'type': 'integer'}, 'title': 'B', 'type': 'array'}}, 'required': ['a', 'b'], 'title': 'multiply_by_maxSchema', 'type': 'object'} You can also customize the tool name and JSON args by passing them into the tool decorator. from pydantic import BaseModel, Fieldclass CalculatorInput(BaseModel): a: int = Field(description=\"first number\") b: int = Field(description=\"second number\")@tool(\"multiplication-tool\", args_schema=CalculatorInput, return_direct=True)def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * b# Let's inspect some of the attributes associated with the tool.print(multiply.name)print(multiply.description)print(multiply.args)print(multiply.return_direct) multiplication-toolMultiply two numbers.{'a': {'description': 'first number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}}True Docstring parsingâ€‹ @tool can optionally parse Google Style docstrings and associate the docstring components (such as arg descriptions) to the relevant parts of the tool schema. To toggle this behavior, specify parse_docstring: @tool(parse_docstring=True)def foo(bar: str, baz: int) -> str: \"\"\"The foo. Args: bar: The bar. baz: The baz. \"\"\" return barprint(foo.args_schema.model_json_schema()) {'description': 'The foo.', 'properties': {'bar': {'description': 'The bar.', 'title': 'Bar', 'type': 'string'}, 'baz': {'description': 'The baz.', 'title': 'Baz', 'type': 'integer'}}, 'required': ['bar', 'baz'], 'title': 'fooSchema', 'type': 'object'} cautionBy default, @tool(parse_docstring=True) will raise ValueError if the docstring does not parse correctly. See API Reference for detail and examples. StructuredToolâ€‹ The StructuredTool.from_function class method provides a bit more configurability than the @tool decorator, without requiring much additional code. from langchain_core.tools import StructuredTooldef multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * basync def amultiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * bcalculator = StructuredTool.from_function(func=multiply, coroutine=amultiply)print(calculator.invoke({\"a\": 2, \"b\": 3}))print(await calculator.ainvoke({\"a\": 2, \"b\": 5}))API Reference:StructuredTool 610 To configure it: class CalculatorInput(BaseModel): a: int = Field(description=\"first number\") b: int = Field(description=\"second number\")def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * bcalculator = StructuredTool.from_function( func=multiply, name=\"Calculator\", description=\"multiply numbers\", args_schema=CalculatorInput, return_direct=True, # coroutine= ... <- you can specify an async method if desired as well)print(calculator.invoke({\"a\": 2, \"b\": 3}))print(calculator.name)print(calculator.description)print(calculator.args) 6Calculatormultiply numbers{'a': {'description': 'first number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}} Creating tools from Runnablesâ€‹ LangChain Runnables that accept string or dict input can be converted to tools using the as_tool method, which allows for the specification of names, descriptions, and additional schema information for arguments. Example usage: from langchain_core.language_models import GenericFakeChatModelfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_messages( [(\"human\", \"Hello. Please respond in the style of {answer_style}.\")])# Placeholder LLMllm = GenericFakeChatModel(messages=iter([\"hello matey\"]))chain = prompt | llm | StrOutputParser()as_tool = chain.as_tool( name=\"Style responder\", description=\"Description of when to use tool.\")as_tool.argsAPI Reference:GenericFakeChatModel | StrOutputParser | ChatPromptTemplate /var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_95770/2548361071.py:14: LangChainBetaWarning: This API is in beta and may change in the future. as_tool = chain.as_tool( {'answer_style': {'title': 'Answer Style', 'type': 'string'}} See this guide for more detail. Subclass BaseToolâ€‹ You can define a custom tool by sub-classing from BaseTool. This provides maximal control over the tool definition, but requires writing more code. from typing import Optionalfrom langchain_core.callbacks import ( AsyncCallbackManagerForToolRun, CallbackManagerForToolRun,)from langchain_core.tools import BaseToolfrom langchain_core.tools.base import ArgsSchemafrom pydantic import BaseModel, Fieldclass CalculatorInput(BaseModel): a: int = Field(description=\"first number\") b: int = Field(description=\"second number\")# Note: It's important that every field has type hints. BaseTool is a# Pydantic class and not having type hints can lead to unexpected behavior.class CustomCalculatorTool(BaseTool): name: str = \"Calculator\" description: str = \"useful for when you need to answer questions about math\" args_schema: Optional[ArgsSchema] = CalculatorInput return_direct: bool = True def _run( self, a: int, b: int, run_manager: Optional[CallbackManagerForToolRun] = None ) -> int: \"\"\"Use the tool.\"\"\" return a * b async def _arun( self, a: int, b: int, run_manager: Optional[AsyncCallbackManagerForToolRun] = None, ) -> int: \"\"\"Use the tool asynchronously.\"\"\" # If the calculation is cheap, you can just delegate to the sync implementation # as shown below. # If the sync calculation is expensive, you should delete the entire _arun method. # LangChain will automatically provide a better implementation that will # kick off the task in a thread to make sure it doesn't block other async code. return self._run(a, b, run_manager=run_manager.get_sync())API Reference:AsyncCallbackManagerForToolRun | CallbackManagerForToolRun | BaseTool multiply = CustomCalculatorTool()print(multiply.name)print(multiply.description)print(multiply.args)print(multiply.return_direct)print(multiply.invoke({\"a\": 2, \"b\": 3}))print(await multiply.ainvoke({\"a\": 2, \"b\": 3})) Calculatoruseful for when you need to answer questions about math{'a': {'description': 'first number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}}True66 How to create async toolsâ€‹ LangChain Tools implement the Runnable interface ğŸƒ. All Runnables expose the invoke and ainvoke methods (as well as other methods like batch, abatch, astream etc). So even if you only provide an sync implementation of a tool, you could still use the ainvoke interface, but there are some important things to know: LangChain's by default provides an async implementation that assumes that the function is expensive to compute, so it'll delegate execution to another thread. If you're working in an async codebase, you should create async tools rather than sync tools, to avoid incuring a small overhead due to that thread. If you need both sync and async implementations, use StructuredTool.from_function or sub-class from BaseTool. If implementing both sync and async, and the sync code is fast to run, override the default LangChain async implementation and simply call the sync code. You CANNOT and SHOULD NOT use the sync invoke with an async tool. from langchain_core.tools import StructuredTooldef multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * bcalculator = StructuredTool.from_function(func=multiply)print(calculator.invoke({\"a\": 2, \"b\": 3}))print( await calculator.ainvoke({\"a\": 2, \"b\": 5})) # Uses default LangChain async implementation incurs small overheadAPI Reference:StructuredTool 610 from langchain_core.tools import StructuredTooldef multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * basync def amultiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * bcalculator = StructuredTool.from_function(func=multiply, coroutine=amultiply)print(calculator.invoke({\"a\": 2, \"b\": 3}))print( await calculator.ainvoke({\"a\": 2, \"b\": 5})) # Uses use provided amultiply without additional overheadAPI Reference:StructuredTool 610 You should not and cannot use .invoke when providing only an async definition. @toolasync def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * btry: multiply.invoke({\"a\": 2, \"b\": 3})except NotImplementedError: print(\"Raised not implemented error. You should not be doing this.\") Raised not implemented error. You should not be doing this. Handling Tool Errorsâ€‹ If you're using tools with agents, you will likely need an error handling strategy, so the agent can recover from the error and continue execution. A simple strategy is to throw a ToolException from inside the tool and specify an error handler using handle_tool_errors. When the error handler is specified, the exception will be caught and the error handler will decide which output to return from the tool. You can set handle_tool_errors to True, a string value, or a function. If it's a function, the function should take a ToolException as a parameter and return a value. Please note that only raising a ToolException won't be effective. You need to first set the handle_tool_errors of the tool because its default value is False. from langchain_core.tools import ToolExceptiondef get_weather(city: str) -> int: \"\"\"Get weather for the given city.\"\"\" raise ToolException(f\"Error: There is no city by the name of {city}.\")API Reference:ToolException Here's an example with the default handle_tool_errors=True behavior. get_weather_tool = StructuredTool.from_function( func=get_weather, handle_tool_errors=True,)get_weather_tool.invoke({\"city\": \"foobar\"}) 'Error: There is no city by the name of foobar.' We can set handle_tool_errors to a string that will always be returned. get_weather_tool = StructuredTool.from_function( func=get_weather, handle_tool_errors=\"There is no such city, but it's probably above 0K there!\",)get_weather_tool.invoke({\"city\": \"foobar\"}) \"There is no such city, but it's probably above 0K there!\" Handling the error using a function: def _handle_error(error: ToolException) -> str: return f\"The following errors occurred during tool execution: `{error.args[0]}`\"get_weather_tool = StructuredTool.from_function( func=get_weather, handle_tool_errors=_handle_error,)get_weather_tool.invoke({\"city\": \"foobar\"}) 'The following errors occurred during tool execution: `Error: There is no city by the name of foobar.`' Returning artifacts of Tool executionâ€‹ Sometimes there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns custom objects like Documents, we may want to pass some view or metadata about this output to the model without passing the raw output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools. The Tool and ToolMessage interfaces make it possible to distinguish between the parts of the tool output meant for the model (this is the ToolMessage.content) and those parts which are meant for use outside the model (ToolMessage.artifact). Requires langchain-core >= 0.2.19This functionality was added in langchain-core == 0.2.19. Please make sure your package is up to date. If we want our tool to distinguish between message content and other artifacts, we need to specify response_format=\"content_and_artifact\" when defining our tool and make sure that we return a tuple of (content, artifact): import randomfrom typing import List, Tuplefrom langchain_core.tools import tool@tool(response_format=\"content_and_artifact\")def generate_random_ints(min: int, max: int, size: int) -> Tuple[str, List[int]]: \"\"\"Generate size random ints in the range [min, max].\"\"\" array = [random.randint(min, max) for _ in range(size)] content = f\"Successfully generated array of {size} random ints in [{min}, {max}].\" return content, arrayAPI Reference:tool If we invoke our tool directly with the tool arguments, we'll get back just the content part of the output: generate_random_ints.invoke({\"min\": 0, \"max\": 9, \"size\": 10}) 'Successfully generated array of 10 random ints in [0, 9].' If we invoke our tool with a ToolCall (like the ones generated by tool-calling models), we'll get back a ToolMessage that contains both the content and artifact generated by the Tool: generate_random_ints.invoke( { \"name\": \"generate_random_ints\", \"args\": {\"min\": 0, \"max\": 9, \"size\": 10}, \"id\": \"123\", # required \"type\": \"tool_call\", # required }) ToolMessage(content='Successfully generated array of 10 random ints in [0, 9].', name='generate_random_ints', tool_call_id='123', artifact=[4, 8, 2, 4, 1, 0, 9, 5, 8, 1]) We can do the same when subclassing BaseTool: from langchain_core.tools import BaseToolclass GenerateRandomFloats(BaseTool): name: str = \"generate_random_floats\" description: str = \"Generate size random floats in the range [min, max].\" response_format: str = \"content_and_artifact\" ndigits: int = 2 def _run(self, min: float, max: float, size: int) -> Tuple[str, List[float]]: range_ = max - min array = [ round(min + (range_ * random.random()), ndigits=self.ndigits) for _ in range(size) ] content = f\"Generated {size} floats in [{min}, {max}], rounded to {self.ndigits} decimals.\" return content, array # Optionally define an equivalent async method # async def _arun(self, min: float, max: float, size: int) -> Tuple[str, List[float]]: # ...API Reference:BaseTool rand_gen = GenerateRandomFloats(ndigits=4)rand_gen.invoke( { \"name\": \"generate_random_floats\", \"args\": {\"min\": 0.1, \"max\": 3.3333, \"size\": 3}, \"id\": \"123\", \"type\": \"tool_call\", }) ToolMessage(content='Generated 3 floats in [0.1, 3.3333], rounded to 4 decimals.', name='generate_random_floats', tool_call_id='123', artifact=[1.5566, 0.5134, 2.7914])Creating tools from functions@tool decoratorStructuredToolCreating tools from RunnablesSubclass BaseToolHow to create async toolsHandling Tool ErrorsReturning artifacts of Tool execution",
      "timestamp": "2025-08-24 15:30:13"
    },
    {
      "url": "https://python.langchain.com/docs/tutorials/rag/",
      "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG. This is a multi-part tutorial: Part 1 (this guide) introduces RAG and walks through a minimal implementation. Part 2 extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes. This tutorial will show how to build a simple Q&A application over a text data source. Along the way weâ€™ll go over a typical Q&A architecture and highlight additional resources for more advanced Q&A techniques. Weâ€™ll also see how LangSmith can help us trace and understand our application. LangSmith will become increasingly helpful as our application grows in complexity. If you're already familiar with basic retrieval, you might also be interested in this high-level overview of different retrieval techniques. Note: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing question/answering over SQL data. Overviewâ€‹ A typical RAG application has two main components: Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline. Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model. Note: the indexing portion of this tutorial will largely follow the semantic search tutorial. The most common full sequence from raw data to answer looks like: Indexingâ€‹ Load: First we need to load our data. This is done with Document Loaders. Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window. Store: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model. Retrieval and generationâ€‹ Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever. Generate: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data Once we've indexed our data, we will use LangGraph as our orchestration framework to implement the retrieval and generation steps. Setupâ€‹ Jupyter Notebookâ€‹ This and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install. Installationâ€‹ This tutorial requires these langchain dependencies: PipConda%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraphconda install langchain-text-splitters langchain-community langgraph -c conda-forge For more details, see our Installation guide. LangSmithâ€‹ Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. After you sign up at the link above, make sure to set your environment variables to start logging traces: export LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\" Or, if in a notebook, you can set them with: import getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass() Componentsâ€‹ We will need to select three components from LangChain's suite of integrations. Select chat model:Google Geminiâ–¾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") Select embeddings model:OpenAIâ–¾OpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakepip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"): os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") Select vector store:In-memoryâ–¾In-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings) Previewâ€‹ In this guide weâ€™ll build an app that answers questions about the website's content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post. We can create a simple indexing pipeline and RAG chain to do this in ~50 lines of code. import bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader( web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(\"post-content\", \"post-title\", \"post-header\") ) ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Index chunks_ = vector_store.add_documents(documents=all_splits)# Define prompt for question-answering# N.B. for non-US LangSmith endpoints, you may need to specify# api_url=\"https://api.smith.langchain.com\" in hub.pull.prompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict): question: str context: List[Document] answer: str# Define application stepsdef retrieve(state: State): retrieved_docs = vector_store.similarity_search(state[\"question\"]) return {\"context\": retrieved_docs}def generate(state: State): docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"]) messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content}) response = llm.invoke(messages) return {\"answer\": response.content}# Compile application and testgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()API Reference:Document | StateGraph response = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(response[\"answer\"]) Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps to facilitate easier execution and understanding. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think step-by-step, allowing them to explore multiple reasoning possibilities. This method enhances performance on complex tasks and provides insight into the model's thinking process. Check out the LangSmith trace. Detailed walkthroughâ€‹ Letâ€™s go through the above code step-by-step to really understand whatâ€™s going on. 1. Indexingâ€‹ noteThis section is an abbreviated version of the content in the semantic search tutorial. If you're comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation. Loading documentsâ€‹ We need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects. In this case weâ€™ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class â€œpost-contentâ€, â€œpost-titleâ€, or â€œpost-headerâ€ are relevant, so weâ€™ll remove all others. import bs4from langchain_community.document_loaders import WebBaseLoader# Only keep post title, headers, and content from the full HTML.bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))loader = WebBaseLoader( web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs_kwargs={\"parse_only\": bs4_strainer},)docs = loader.load()assert len(docs) == 1print(f\"Total characters: {len(docs[0].page_content)}\") Total characters: 43131 print(docs[0].page_content[:500]) LLM Powered Autonomous Agents Date: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian WengBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#In Go deeperâ€‹ DocumentLoader: Object that loads data from a source as list of Documents. Docs: Detailed documentation on how to use DocumentLoaders. Integrations: 160+ integrations to choose from. Interface: API reference for the base interface. Splitting documentsâ€‹ Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs. To handle this weâ€™ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time. As in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases. from langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter( chunk_size=1000, # chunk size (characters) chunk_overlap=200, # chunk overlap (characters) add_start_index=True, # track index in original document)all_splits = text_splitter.split_documents(docs)print(f\"Split blog post into {len(all_splits)} sub-documents.\") Split blog post into 66 sub-documents. Go deeperâ€‹ TextSplitter: Object that splits a list of Documents into smaller chunks. Subclass of DocumentTransformers. Learn more about splitting text using different methods by reading the how-to docs Code (py or js) Scientific papers Interface: API reference for the base interface. DocumentTransformer: Object that performs a transformation on a list of Document objects. Docs: Detailed documentation on how to use DocumentTransformers Integrations Interface: API reference for the base interface. Storing documentsâ€‹ Now we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents. We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial. document_ids = vector_store.add_documents(documents=all_splits)print(document_ids[:3]) ['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'] Go deeperâ€‹ Embeddings: Wrapper around a text embedding model, used for converting text to embeddings. Docs: Detailed documentation on how to use embeddings. Integrations: 30+ integrations to choose from. Interface: API reference for the base interface. VectorStore: Wrapper around a vector database, used for storing and querying embeddings. Docs: Detailed documentation on how to use vector stores. Integrations: 40+ integrations to choose from. Interface: API reference for the base interface. This completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question. 2. Retrieval and Generationâ€‹ Now letâ€™s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer. For generation, we will use the chat model selected at the start of the tutorial. Weâ€™ll use a prompt for RAG that is checked into the LangChain prompt hub (here). from langchain import hub# N.B. for non-US LangSmith endpoints, you may need to specify# api_url=\"https://api.smith.langchain.com\" in hub.pull.prompt = hub.pull(\"rlm/rag-prompt\")example_messages = prompt.invoke( {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}).to_messages()assert len(example_messages) == 1print(example_messages[0].content) You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.Question: (question goes here) Context: (context goes here) Answer: We'll use LangGraph to tie together the retrieval and generation steps into a single application. This will bring a number of benefits: We can define our application logic once and automatically support multiple invocation modes, including streaming, async, and batched calls. We get streamlined deployments via LangGraph Platform. LangSmith will automatically trace the steps of our application together. We can easily add key features to our application, including persistence and human-in-the-loop approval, with minimal code changes. To use LangGraph, we need to define three things: The state of our application; The nodes of our application (i.e., application steps); The \"control flow\" of our application (e.g., the ordering of the steps). State:â€‹ The state of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a TypedDict, but can also be a Pydantic BaseModel. For a simple RAG application, we can just keep track of the input question, retrieved context, and generated answer: from langchain_core.documents import Documentfrom typing_extensions import List, TypedDictclass State(TypedDict): question: str context: List[Document] answer: strAPI Reference:Document Nodes (application steps)â€‹ Let's start with a simple sequence of two steps: retrieval and generation. def retrieve(state: State): retrieved_docs = vector_store.similarity_search(state[\"question\"]) return {\"context\": retrieved_docs}def generate(state: State): docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"]) messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content}) response = llm.invoke(messages) return {\"answer\": response.content} Our retrieval step simply runs a similarity search using the input question, and the generation step formats the retrieved context and original question into a prompt for the chat model. Control flowâ€‹ Finally, we compile our application into a single graph object. In this case, we are just connecting the retrieval and generation steps into a single sequence. from langgraph.graph import START, StateGraphgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()API Reference:StateGraph LangGraph also comes with built-in utilities for visualizing the control flow of your application: from IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png())) Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:question = \"...\"retrieved_docs = vector_store.similarity_search(question)docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)prompt = prompt.invoke({\"question\": question, \"context\": docs_content})answer = llm.invoke(prompt)The benefits of LangGraph include: Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps; Automatic support for tracing via LangSmith and deployments via LangGraph Platform; Support for persistence, human-in-the-loop, and other features. Many use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in Part 2 of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously. Usageâ€‹ Let's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming. Invoke: result = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(f\"Context: {result['context']}\\n\\n\")print(f\"Answer: {result['answer']}\") Context: [Document(id='a42dc78b-8f76-472a-9e25-180508af74f3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.'), Document(id='c0e45887-d0b0-483d-821a-bb5d8316d51d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='4cc7f318-35f5-440f-a4a4-145b5f0b918d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='f621ade4-9b0d-471f-a522-44eb5feeba0c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]Answer: Task decomposition is a technique used to break down complex tasks into smaller, manageable steps, allowing for more efficient problem-solving. This can be achieved through methods like chain of thought prompting or the tree of thoughts approach, which explores multiple reasoning possibilities at each step. It can be initiated through simple prompts, task-specific instructions, or human inputs. Stream steps: for step in graph.stream( {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"): print(f\"{step}\\n\\n----------------\\n\") {'retrieve': {'context': [Document(id='a42dc78b-8f76-472a-9e25-180508af74f3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.'), Document(id='c0e45887-d0b0-483d-821a-bb5d8316d51d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='4cc7f318-35f5-440f-a4a4-145b5f0b918d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='f621ade4-9b0d-471f-a522-44eb5feeba0c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]}}----------------{'generate': {'answer': 'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This technique, often enhanced by methods like Chain of Thought (CoT) or Tree of Thoughts, allows models to reason through tasks systematically and improves performance by clarifying the thought process. It can be achieved through simple prompts, task-specific instructions, or human inputs.'}}---------------- Stream tokens: for message, metadata in graph.stream( {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"messages\"): print(message.content, end=\"|\") |Task| decomposition| is| the| process| of| breaking| down| complex| tasks| into| smaller|,| more| manageable| steps|.| It| can| be| achieved| through| techniques| like| Chain| of| Thought| (|Co|T|)| prompting|,| which| encourages| the| model| to| think| step| by| step|,| or| through| more| structured| methods| like| the| Tree| of| Thoughts|.| This| approach| not| only| simplifies| task| execution| but| also| provides| insights| into| the| model|'s| reasoning| process|.|| tipFor async invocations, use:result = await graph.ainvoke(...)andasync for step in graph.astream(...): Returning sourcesâ€‹ Note that by storing the retrieved context in the state of the graph, we recover sources for the model's generated answer in the \"context\" field of the state. See this guide on returning sources for more detail. Go deeperâ€‹ Chat models take in a sequence of messages and return a message. Docs Integrations: 25+ integrations to choose from. Interface: API reference for the base interface. Customizing the prompt As shown above, we can load prompts (e.g., this RAG prompt) from the prompt hub. The prompt can also be easily customized. For example: from langchain_core.prompts import PromptTemplatetemplate = \"\"\"Use the following pieces of context to answer the question at the end.If you don't know the answer, just say that you don't know, don't try to make up an answer.Use three sentences maximum and keep the answer as concise as possible.Always say \"thanks for asking!\" at the end of the answer.{context}Question: {question}Helpful Answer:\"\"\"custom_rag_prompt = PromptTemplate.from_template(template)API Reference:PromptTemplate Query analysisâ€‹ So far, we are executing the retrieval using the raw input query. However, there are some advantages to allowing a model to generate the query for retrieval purposes. For example: In addition to semantic search, we can build in structured filters (e.g., \"Find documents since the year 2020.\"); The model can rewrite user queries, which may be multifaceted or include irrelevant language, into more effective search queries. Query analysis employs models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application. For illustrative purposes, let's add some metadata to the documents in our vector store. We will add some (contrived) sections to the document which we can filter on later. total_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits): if i < third: document.metadata[\"section\"] = \"beginning\" elif i < 2 * third: document.metadata[\"section\"] = \"middle\" else: document.metadata[\"section\"] = \"end\"all_splits[0].metadata {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8, 'section': 'beginning'} We will need to update the documents in our vector store. We will use a simple InMemoryVectorStore for this, as we will use some of its specific features (i.e., metadata filtering). Refer to the vector store integration documentation for relevant features of your chosen vector store. from langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)API Reference:InMemoryVectorStore Let's next define a schema for our search query. We will use structured output for this purpose. Here we define a query as containing a string query and a document section (either \"beginning\", \"middle\", or \"end\"), but this can be defined however you like. from typing import Literalfrom typing_extensions import Annotatedclass Search(TypedDict): \"\"\"Search query.\"\"\" query: Annotated[str, ..., \"Search query to run.\"] section: Annotated[ Literal[\"beginning\", \"middle\", \"end\"], ..., \"Section to query.\", ] Finally, we add a step to our LangGraph application to generate a query from the user's raw input: class State(TypedDict): question: str query: Search context: List[Document] answer: strdef analyze_query(state: State): structured_llm = llm.with_structured_output(Search) query = structured_llm.invoke(state[\"question\"]) return {\"query\": query}def retrieve(state: State): query = state[\"query\"] retrieved_docs = vector_store.similarity_search( query[\"query\"], filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"], ) return {\"context\": retrieved_docs}def generate(state: State): docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"]) messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content}) response = llm.invoke(messages) return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile() Full Code:from typing import Literalimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import Annotated, List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader( web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(\"post-content\", \"post-title\", \"post-header\") ) ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Update metadata (illustration purposes)total_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits): if i < third: document.metadata[\"section\"] = \"beginning\" elif i < 2 * third: document.metadata[\"section\"] = \"middle\" else: document.metadata[\"section\"] = \"end\"# Index chunksvector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)# Define schema for searchclass Search(TypedDict): \"\"\"Search query.\"\"\" query: Annotated[str, ..., \"Search query to run.\"] section: Annotated[ Literal[\"beginning\", \"middle\", \"end\"], ..., \"Section to query.\", ]# Define prompt for question-answeringprompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict): question: str query: Search context: List[Document] answer: strdef analyze_query(state: State): structured_llm = llm.with_structured_output(Search) query = structured_llm.invoke(state[\"question\"]) return {\"query\": query}def retrieve(state: State): query = state[\"query\"] retrieved_docs = vector_store.similarity_search( query[\"query\"], filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"], ) return {\"context\": retrieved_docs}def generate(state: State): docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"]) messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content}) response = llm.invoke(messages) return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile()API Reference:Document | InMemoryVectorStore | StateGraph display(Image(graph.get_graph().draw_mermaid_png())) We can test our implementation by specifically asking for context from the end of the post. Note that the model includes different information in its answer. for step in graph.stream( {\"question\": \"What does the end of the post say about Task Decomposition?\"}, stream_mode=\"updates\",): print(f\"{step}\\n\\n----------------\\n\") {'analyze_query': {'query': {'query': 'Task Decomposition', 'section': 'end'}}}----------------{'retrieve': {'context': [Document(id='d6cef137-e1e8-4ddc-91dc-b62bd33c6020', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39221, 'section': 'end'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.'), Document(id='d1834ae1-eb6a-43d7-a023-08dfa5028799', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39086, 'section': 'end'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'), Document(id='ca7f06e4-2c2e-4788-9a81-2418d82213d9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 32942, 'section': 'end'}, page_content='}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:'), Document(id='1fcc2736-30f4-4ef6-90f2-c64af92118cb', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 35127, 'section': 'end'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease')]}}----------------{'generate': {'answer': 'The end of the post highlights that task decomposition faces challenges in long-term planning and adapting to unexpected errors. LLMs struggle with adjusting their plans, making them less robust compared to humans who learn from trial and error. This indicates a limitation in effectively exploring the solution space and handling complex tasks.'}}---------------- In both the streamed steps and the LangSmith trace, we can now observe the structured query that was fed into the retrieval step. Query Analysis is a rich problem with a wide range of approaches. Refer to the how-to guides for more examples. Next stepsâ€‹ We've covered the steps to build a basic Q&A app over data: Loading data with a Document Loader Chunking the indexed data with a Text Splitter to make it more easily usable by a model Embedding the data and storing the data in a vectorstore Retrieving the previously stored chunks in response to incoming questions Generating an answer using the retrieved chunks as context. In Part 2 of the tutorial, we will extend the implementation here to accommodate conversation-style interactions and multi-step retrieval processes. Further reading: Return sources: Learn how to return source documents Streaming: Learn how to stream outputs and intermediate steps Add chat history: Learn how to add chat history to your app Retrieval conceptual guide: A high-level overview of specific retrieval techniques OverviewIndexingRetrieval and generationSetupJupyter NotebookInstallationLangSmithComponentsPreviewDetailed walkthrough1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationQuery analysisNext steps",
      "timestamp": "2025-08-24 15:30:17"
    },
    {
      "url": "https://python.langchain.com/docs/tutorials/summarization/",
      "title": "Summarize Text | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Summarize Text | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page infoThis tutorial demonstrates text summarization using built-in chains and LangGraph.A previous version of this page showcased the legacy chains StuffDocumentsChain, MapReduceDocumentsChain, and RefineDocumentsChain. See here for information on using those abstractions and a comparison with the methods demonstrated in this tutorial. Suppose you have a set of documents (PDFs, Notion pages, customer questions, etc.) and you want to summarize the content. LLMs are a great tool for this given their proficiency in understanding and synthesizing text. In the context of retrieval-augmented generation, summarizing text can help distill the information in a large number of retrieved documents to provide context for a LLM. In this walkthrough we'll go over how to summarize content from multiple documents using LLMs. Conceptsâ€‹ Concepts we will cover are: Using language models. Using document loaders, specifically the WebBaseLoader to load content from an HTML webpage. Two ways to summarize or otherwise combine documents. Stuff, which simply concatenates documents into a prompt; Map-reduce, for larger sets of documents. This splits documents into batches, summarizes those, and then summarizes the summaries. Shorter, targeted guides on these strategies and others, including iterative refinement, can be found in the how-to guides. Setupâ€‹ Jupyter Notebookâ€‹ This guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them. This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install. Installationâ€‹ To install LangChain run: PipCondapip install langchainconda install langchain -c conda-forge For more details, see our Installation guide. LangSmithâ€‹ Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. After you sign up at the link above, make sure to set your environment variables to start logging traces: export LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\" Or, if in a notebook, you can set them with: import getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass() Overviewâ€‹ A central question for building a summarizer is how to pass your documents into the LLM's context window. Two common approaches for this are: Stuff: Simply \"stuff\" all your documents into a single prompt. This is the simplest approach (see here for more on the create_stuff_documents_chain constructor, which is used for this method). Map-reduce: Summarize each document on its own in a \"map\" step and then \"reduce\" the summaries into a final summary (see here for more on the MapReduceDocumentsChain, which is used for this method). Note that map-reduce is especially effective when understanding of a sub-document does not rely on preceding context. For example, when summarizing a corpus of many, shorter documents. In other cases, such as summarizing a novel or body of text with an inherent sequence, iterative refinement may be more effective. Setupâ€‹ First set environment variables and install packages: %pip install --upgrade --quiet tiktoken langchain langgraph beautifulsoup4 langchain-community# Set env var OPENAI_API_KEY or load from a .env file# import dotenv# dotenv.load_dotenv() import osos.environ[\"LANGSMITH_TRACING\"] = \"true\" First we load in our documents. We will use WebBaseLoader to load a blog post: from langchain_community.document_loaders import WebBaseLoaderloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")docs = loader.load() Let's next select a LLM: Select chat model:Google Geminiâ–¾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") Stuff: summarize in a single LLM callâ€‹ We can use create_stuff_documents_chain, especially if using larger context window models such as: 128k token OpenAI gpt-4o 200k token Anthropic claude-3-5-sonnet-latest The chain will take a list of documents, insert them all into a prompt, and pass that prompt to an LLM: from langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain.chains.llm import LLMChainfrom langchain_core.prompts import ChatPromptTemplate# Define promptprompt = ChatPromptTemplate.from_messages( [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")])# Instantiate chainchain = create_stuff_documents_chain(llm, prompt)# Invoke chainresult = chain.invoke({\"context\": docs})print(result)API Reference:ChatPromptTemplate The article \"LLM Powered Autonomous Agents\" by Lilian Weng discusses the development and capabilities of autonomous agents powered by large language models (LLMs). It outlines a system architecture that includes three main components: Planning, Memory, and Tool Use. 1. **Planning** involves task decomposition, where complex tasks are broken down into manageable subgoals, and self-reflection, allowing agents to learn from past actions to improve future performance. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are highlighted for enhancing reasoning and planning.2. **Memory** is categorized into short-term and long-term memory, with mechanisms for fast retrieval using Maximum Inner Product Search (MIPS) algorithms. This allows agents to retain and recall information effectively.3. **Tool Use** enables agents to interact with external APIs and tools, enhancing their capabilities beyond the limitations of their training data. Examples include MRKL systems and frameworks like HuggingGPT, which facilitate task planning and execution.The article also addresses challenges such as finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. It concludes with case studies demonstrating the practical applications of these concepts in scientific discovery and interactive simulations. Overall, the article emphasizes the potential of LLMs as powerful problem solvers in autonomous agent systems. Streamingâ€‹ Note that we can also stream the result token-by-token: for token in chain.stream({\"context\": docs}): print(token, end=\"|\") |The| article| \"|LL|M| Powered| Autonomous| Agents|\"| by| Lil|ian| W|eng| discusses| the| development| and| capabilities| of| autonomous| agents| powered| by| large| language| models| (|LL|Ms|).| It| outlines| a| system| architecture| that| includes| three| main| components|:| Planning|,| Memory|,| and| Tool| Use|.| |1|.| **|Planning|**| involves| task| decomposition|,| where| complex| tasks| are| broken| down| into| manageable| sub|go|als|,| and| self|-ref|lection|,| allowing| agents| to| learn| from| past| actions| to| improve| future| performance|.| Techniques| like| Chain| of| Thought| (|Co|T|)| and| Tree| of| Thoughts| (|To|T|)| are| highlighted| for| enhancing| reasoning| and| planning|.|2|.| **|Memory|**| is| categorized| into| short|-term| and| long|-term| memory|,| with| mechanisms| for| fast| retrieval| using| Maximum| Inner| Product| Search| (|M|IPS|)| algorithms|.| This| allows| agents| to| retain| and| recall| information| effectively|.|3|.| **|Tool| Use|**| emphasizes| the| integration| of| external| APIs| and| tools| to| extend| the| capabilities| of| L|LM|s|,| enabling| them| to| perform| tasks| beyond| their| inherent| limitations|.| Examples| include| MR|KL| systems| and| frameworks| like| Hug|ging|GPT|,| which| facilitate| task| planning| and| execution|.|The| article| also| addresses| challenges| such| as| finite| context| length|,| difficulties| in| long|-term| planning|,| and| the| reliability| of| natural| language| interfaces|.| It| concludes| with| case| studies| demonstrating| the| practical| applications| of| L|LM|-powered| agents| in| scientific| discovery| and| interactive| simulations|.| Overall|,| the| piece| illustrates| the| potential| of| L|LM|s| as| general| problem| sol|vers| and| their| evolving| role| in| autonomous| systems|.|| Go deeperâ€‹ You can easily customize the prompt. You can easily try different LLMs, (e.g., Claude) via the llm parameter. Map-Reduce: summarize long texts via parallelizationâ€‹ Let's unpack the map reduce approach. For this, we'll first map each document to an individual summary using an LLM. Then we'll reduce or consolidate those summaries into a single global summary. Note that the map step is typically parallelized over the input documents. LangGraph, built on top of langchain-core, supports map-reduce workflows and is well-suited to this problem: LangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution; LangGraph's checkpointing supports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications. The LangGraph implementation is straightforward to modify and extend, as we will see below. Mapâ€‹ Let's first define the prompt associated with the map step. We can use the same summarization prompt as in the stuff approach, above: from langchain_core.prompts import ChatPromptTemplatemap_prompt = ChatPromptTemplate.from_messages( [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")])API Reference:ChatPromptTemplate We can also use the Prompt Hub to store and fetch prompts. This will work with your LangSmith API key. For example, see the map prompt here. from langchain import hubmap_prompt = hub.pull(\"rlm/map-prompt\") Reduceâ€‹ We also define a prompt that takes the document mapping results and reduces them into a single output. # Also available via the hub: `hub.pull(\"rlm/reduce-prompt\")`reduce_template = \"\"\"The following is a set of summaries:{docs}Take these and distill it into a final, consolidated summaryof the main themes.\"\"\"reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)]) Orchestration via LangGraphâ€‹ Below we implement a simple application that maps the summarization step on a list of documents, then reduces them using the above prompts. Map-reduce flows are particularly useful when texts are long compared to the context window of a LLM. For long texts, we need a mechanism that ensures that the context to be summarized in the reduce step does not exceed a model's context window size. Here we implement a recursive \"collapsing\" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text. First we chunk the blog post into smaller \"sub documents\" to be mapped: from langchain_text_splitters import CharacterTextSplittertext_splitter = CharacterTextSplitter.from_tiktoken_encoder( chunk_size=1000, chunk_overlap=0)split_docs = text_splitter.split_documents(docs)print(f\"Generated {len(split_docs)} documents.\") Created a chunk of size 1003, which is longer than the specified 1000``````outputGenerated 14 documents. Next, we define our graph. Note that we define an artificially low maximum token length of 1,000 tokens to illustrate the \"collapsing\" step. import operatorfrom typing import Annotated, List, Literal, TypedDictfrom langchain.chains.combine_documents.reduce import ( acollapse_docs, split_list_of_docs,)from langchain_core.documents import Documentfrom langgraph.constants import Sendfrom langgraph.graph import END, START, StateGraphtoken_max = 1000def length_function(documents: List[Document]) -> int: \"\"\"Get number of tokens for input contents.\"\"\" return sum(llm.get_num_tokens(doc.page_content) for doc in documents)# This will be the overall state of the main graph.# It will contain the input document contents, corresponding# summaries, and a final summary.class OverallState(TypedDict): # Notice here we use the operator.add # This is because we want combine all the summaries we generate # from individual nodes back into one list - this is essentially # the \"reduce\" part contents: List[str] summaries: Annotated[list, operator.add] collapsed_summaries: List[Document] final_summary: str# This will be the state of the node that we will \"map\" all# documents to in order to generate summariesclass SummaryState(TypedDict): content: str# Here we generate a summary, given a documentasync def generate_summary(state: SummaryState): prompt = map_prompt.invoke(state[\"content\"]) response = await llm.ainvoke(prompt) return {\"summaries\": [response.content]}# Here we define the logic to map out over the documents# We will use this an edge in the graphdef map_summaries(state: OverallState): # We will return a list of `Send` objects # Each `Send` object consists of the name of a node in the graph # as well as the state to send to that node return [ Send(\"generate_summary\", {\"content\": content}) for content in state[\"contents\"] ]def collect_summaries(state: OverallState): return { \"collapsed_summaries\": [Document(summary) for summary in state[\"summaries\"]] }async def _reduce(input: dict) -> str: prompt = reduce_prompt.invoke(input) response = await llm.ainvoke(prompt) return response.content# Add node to collapse summariesasync def collapse_summaries(state: OverallState): doc_lists = split_list_of_docs( state[\"collapsed_summaries\"], length_function, token_max ) results = [] for doc_list in doc_lists: results.append(await acollapse_docs(doc_list, _reduce)) return {\"collapsed_summaries\": results}# This represents a conditional edge in the graph that determines# if we should collapse the summaries or notdef should_collapse( state: OverallState,) -> Literal[\"collapse_summaries\", \"generate_final_summary\"]: num_tokens = length_function(state[\"collapsed_summaries\"]) if num_tokens > token_max: return \"collapse_summaries\" else: return \"generate_final_summary\"# Here we will generate the final summaryasync def generate_final_summary(state: OverallState): response = await _reduce(state[\"collapsed_summaries\"]) return {\"final_summary\": response}# Construct the graph# Nodes:graph = StateGraph(OverallState)graph.add_node(\"generate_summary\", generate_summary) # same as beforegraph.add_node(\"collect_summaries\", collect_summaries)graph.add_node(\"collapse_summaries\", collapse_summaries)graph.add_node(\"generate_final_summary\", generate_final_summary)# Edges:graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])graph.add_edge(\"generate_summary\", \"collect_summaries\")graph.add_conditional_edges(\"collect_summaries\", should_collapse)graph.add_conditional_edges(\"collapse_summaries\", should_collapse)graph.add_edge(\"generate_final_summary\", END)app = graph.compile()API Reference:Document | Send | StateGraph LangGraph allows the graph structure to be plotted to help visualize its function: from IPython.display import ImageImage(app.get_graph().draw_mermaid_png()) When running the application, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step. Note that because we have a loop in the graph, it can be helpful to specify a recursion_limit on its execution. This will raise a specific error when the specified limit is exceeded. async for step in app.astream( {\"contents\": [doc.page_content for doc in split_docs]}, {\"recursion_limit\": 10},): print(list(step.keys())) ['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['generate_summary']['collect_summaries']['collapse_summaries']['collapse_summaries']['generate_final_summary'] print(step) {'generate_final_summary': {'final_summary': 'The consolidated summary of the main themes from the provided documents is as follows:\\n\\n1. **Integration of Large Language Models (LLMs) in Autonomous Agents**: The documents explore the evolving role of LLMs in autonomous systems, emphasizing their enhanced reasoning and acting capabilities through methodologies that incorporate structured planning, memory systems, and tool use.\\n\\n2. **Core Components of Autonomous Agents**:\\n - **Planning**: Techniques like task decomposition (e.g., Chain of Thought) and external classical planners are utilized to facilitate long-term planning by breaking down complex tasks.\\n - **Memory**: The memory system is divided into short-term (in-context learning) and long-term memory, with parallels drawn between human memory and machine learning to improve agent performance.\\n - **Tool Use**: Agents utilize external APIs and algorithms to enhance problem-solving abilities, exemplified by frameworks like HuggingGPT that manage task workflows.\\n\\n3. **Neuro-Symbolic Architectures**: The integration of MRKL (Modular Reasoning, Knowledge, and Language) systems combines neural and symbolic expert modules with LLMs, addressing challenges in tasks such as verbal math problem-solving.\\n\\n4. **Specialized Applications**: Case studies, such as ChemCrow and projects in anticancer drug discovery, demonstrate the advantages of LLMs augmented with expert tools in specialized domains.\\n\\n5. **Challenges and Limitations**: The documents highlight challenges such as hallucination in model outputs and the finite context length of LLMs, which affects their ability to incorporate historical information and perform self-reflection. Techniques like Chain of Hindsight and Algorithm Distillation are discussed to enhance model performance through iterative learning.\\n\\n6. **Structured Software Development**: A systematic approach to creating Python software projects is emphasized, focusing on defining core components, managing dependencies, and adhering to best practices for documentation.\\n\\nOverall, the integration of structured planning, memory systems, and advanced tool use aims to enhance the capabilities of LLM-powered autonomous agents while addressing the challenges and limitations these technologies face in real-world applications.'}} In the corresponding LangSmith trace we can see the individual LLM calls, grouped under their respective nodes. Go deeperâ€‹ Customization As shown above, you can customize the LLMs and prompts for map and reduce stages. Real-world use-case See this blog post case-study on analyzing user interactions (questions about LangChain documentation)! The blog post and associated repo also introduce clustering as a means of summarization. This opens up another path beyond the stuff or map-reduce approaches that is worth considering. Next stepsâ€‹ We encourage you to check out the how-to guides for more detail on: Other summarization strategies, such as iterative refinement Built-in document loaders and text-splitters Integrating various combine-document chains into a RAG application Incorporating retrieval into a chatbot and other concepts.ConceptsSetupJupyter NotebookInstallationLangSmithOverviewSetupStuff: summarize in a single LLM callStreamingGo deeperMap-Reduce: summarize long texts via parallelizationMapReduceOrchestration via LangGraphGo deeperNext steps",
      "timestamp": "2025-08-24 15:30:21"
    },
    {
      "url": "https://python.langchain.com/docs/concepts/few_shot_prompting/",
      "title": "Few-shot prompting | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Few-shot prompting | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Prerequisites Chat models Overviewâ€‹ One of the most effective ways to improve model performance is to give a model examples of what you want it to do. The technique of adding example inputs and expected outputs to a model prompt is known as \"few-shot prompting\". The technique is based on the Language Models are Few-Shot Learners paper. There are a few things to think about when doing few-shot prompting: How are examples generated? How many examples are in each prompt? How are examples selected at runtime? How are examples formatted in the prompt? Here are the considerations for each. 1. Generating examplesâ€‹ The first and most important step of few-shot prompting is coming up with a good dataset of examples. Good examples should be relevant at runtime, clear, informative, and provide information that was not already known to the model. At a high-level, the basic ways to generate examples are: Manual: a person/people generates examples they think are useful. Better model: a better (presumably more expensive/slower) model's responses are used as examples for a worse (presumably cheaper/faster) model. User feedback: users (or labelers) leave feedback on interactions with the application and examples are generated based on that feedback (for example, all interactions with positive feedback could be turned into examples). LLM feedback: same as user feedback but the process is automated by having models evaluate themselves. Which approach is best depends on your task. For tasks where a small number of core principles need to be understood really well, it can be valuable hand-craft a few really good examples. For tasks where the space of correct behaviors is broader and more nuanced, it can be useful to generate many examples in a more automated fashion so that there's a higher likelihood of there being some highly relevant examples for any runtime input. Single-turn v.s. multi-turn examples Another dimension to think about when generating examples is what the example is actually showing. The simplest types of examples just have a user input and an expected model output. These are single-turn examples. One more complex type of example is where the example is an entire conversation, usually in which a model initially responds incorrectly and a user then tells the model how to correct its answer. This is called a multi-turn example. Multi-turn examples can be useful for more nuanced tasks where it's useful to show common errors and spell out exactly why they're wrong and what should be done instead. 2. Number of examplesâ€‹ Once we have a dataset of examples, we need to think about how many examples should be in each prompt. The key tradeoff is that more examples generally improve performance, but larger prompts increase costs and latency. And beyond some threshold having too many examples can start to confuse the model. Finding the right number of examples is highly dependent on the model, the task, the quality of the examples, and your cost and latency constraints. Anecdotally, the better the model is the fewer examples it needs to perform well and the more quickly you hit steeply diminishing returns on adding more examples. But, the best/only way to reliably answer this question is to run some experiments with different numbers of examples. 3. Selecting examplesâ€‹ Assuming we are not adding our entire example dataset into each prompt, we need to have a way of selecting examples from our dataset based on a given input. We can do this: Randomly By (semantic or keyword-based) similarity of the inputs Based on some other constraints, like token size LangChain has a number of ExampleSelectors which make it easy to use any of these techniques. Generally, selecting by semantic similarity leads to the best model performance. But how important this is is again model and task specific, and is something worth experimenting with. 4. Formatting examplesâ€‹ Most state-of-the-art models these days are chat models, so we'll focus on formatting examples for those. Our basic options are to insert the examples: In the system prompt as a string As their own messages If we insert our examples into the system prompt as a string, we'll need to make sure it's clear to the model where each example begins and which parts are the input versus output. Different models respond better to different syntaxes, like ChatML, XML, TypeScript, etc. If we insert our examples as messages, where each example is represented as a sequence of Human, AI messages, we might want to also assign names to our messages like \"example_user\" and \"example_assistant\" to make it clear that these messages correspond to different actors than the latest input message. Formatting tool call examples One area where formatting examples as messages can be tricky is when our example outputs have tool calls. This is because different models have different constraints on what types of message sequences are allowed when any tool calls are generated. Some models require that any AIMessage with tool calls be immediately followed by ToolMessages for every tool call, Some models additionally require that any ToolMessages be immediately followed by an AIMessage before the next HumanMessage, Some models require that tools are passed into the model if there are any tool calls / ToolMessages in the chat history. These requirements are model-specific and should be checked for the model you are using. If your model requires ToolMessages after tool calls and/or AIMessages after ToolMessages and your examples only include expected tool calls and not the actual tool outputs, you can try adding dummy ToolMessages / AIMessages to the end of each example with generic contents to satisfy the API constraints. In these cases it's especially worth experimenting with inserting your examples as strings versus messages, as having dummy messages can adversely affect certain models. You can see a case study of how Anthropic and OpenAI respond to different few-shot prompting techniques on two different tool calling benchmarks here.Overview1. Generating examples2. Number of examples3. Selecting examples4. Formatting examples",
      "timestamp": "2025-08-24 15:30:24"
    },
    {
      "url": "https://python.langchain.com/docs/concepts/vectorstores/",
      "title": "Vector stores | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Vector stores | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Prerequisites Embeddings Text splitters NoteThis conceptual overview focuses on text-based indexing and retrieval for simplicity. However, embedding models can be multi-modal and vector stores can be used to store and retrieve a variety of data types beyond text. Overviewâ€‹ Vector stores are specialized data stores that enable indexing and retrieving information based on vector representations. These vectors, called embeddings, capture the semantic meaning of data that has been embedded. Vector stores are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches. Integrationsâ€‹ LangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations. Please see the full list of LangChain vectorstore integrations. Interfaceâ€‹ LangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations. The interface consists of basic methods for writing, deleting and searching for documents in the vector store. The key methods are: add_documents: Add a list of texts to the vector store. delete: Delete a list of documents from the vector store. similarity_search: Search for similar documents to a given query. Initializationâ€‹ Most vectors in LangChain accept an embedding model as an argument when initializing the vector store. We will use LangChain's InMemoryVectorStore implementation to illustrate the API. from langchain_core.vectorstores import InMemoryVectorStore# Initialize with an embedding modelvector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())API Reference:InMemoryVectorStore Adding documentsâ€‹ To add documents, use the add_documents method. This API works with a list of Document objects. Document objects all have page_content and metadata attributes, making them a universal way to store unstructured text and associated metadata. from langchain_core.documents import Documentdocument_1 = Document( page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\", metadata={\"source\": \"tweet\"},)document_2 = Document( page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\", metadata={\"source\": \"news\"},)documents = [document_1, document_2]vector_store.add_documents(documents=documents)API Reference:Document You should usually provide IDs for the documents you add to the vector store, so that instead of adding the same document multiple times, you can update the existing document. vector_store.add_documents(documents=documents, ids=[\"doc1\", \"doc2\"]) Deleteâ€‹ To delete documents, use the delete method which takes a list of document IDs to delete. vector_store.delete(ids=[\"doc1\"]) Searchâ€‹ Vector stores embed and store the documents that added. If we pass in a query, the vectorstore will embed the query, perform a similarity search over the embedded documents, and return the most similar ones. This captures two important concepts: first, there needs to be a way to measure the similarity between the query and any embedded document. Second, there needs to be an algorithm to efficiently perform this similarity search across all embedded documents. Similarity metricsâ€‹ A critical advantage of embeddings vectors is they can be compared using many simple mathematical operations: Cosine Similarity: Measures the cosine of the angle between two vectors. Euclidean Distance: Measures the straight-line distance between two points. Dot Product: Measures the projection of one vector onto another. The choice of similarity metric can sometimes be selected when initializing the vectorstore. Please refer to the documentation of the specific vectorstore you are using to see what similarity metrics are supported. Further reading See this documentation from Google on similarity metrics to consider with embeddings. See Pinecone's blog post on similarity metrics. See OpenAI's FAQ on what similarity metric to use with OpenAI embeddings. Similarity searchâ€‹ Given a similarity metric to measure the distance between the embedded query and any embedded document, we need an algorithm to efficiently search over all the embedded documents to find the most similar ones. There are various ways to do this. As an example, many vectorstores implement HNSW (Hierarchical Navigable Small World), a graph-based index structure that allows for efficient similarity search. Regardless of the search algorithm used under the hood, the LangChain vectorstore interface has a similarity_search method for all integrations. This will take the search query, create an embedding, find similar documents, and return them as a list of Documents. query = \"my query\"docs = vectorstore.similarity_search(query) Many vectorstores support search parameters to be passed with the similarity_search method. See the documentation for the specific vectorstore you are using to see what parameters are supported. As an example Pinecone several parameters that are important general concepts: Many vectorstores support the k, which controls the number of documents to return, and filter, which allows for filtering documents by metadata. query (str) - Text to look up documents similar to. k (int) - Number of documents to return. Defaults to 4. filter (dict | None) - Dictionary of argument(s) to filter on metadata Further reading See the how-to guide for more details on how to use the similarity_search method. See the integrations page for more details on arguments that can be passed in to the similarity_search method for specific vectorstores. Metadata filteringâ€‹ While vectorstore implement a search algorithm to efficiently search over all the embedded documents to find the most similar ones, many also support filtering on metadata. Metadata filtering helps narrow down the search by applying specific conditions such as retrieving documents from a particular source or date range. These two concepts work well together: Semantic search: Query the unstructured data directly, often via embedding or keyword similarity. Metadata search: Apply structured query to the metadata, filtering specific documents. Vector store support for metadata filtering is typically dependent on the underlying vector store implementation. Here is example usage with Pinecone, showing that we filter for all documents that have the metadata key source with value tweet. vectorstore.similarity_search( \"LangChain provides abstractions to make working with LLMs easy\", k=2, filter={\"source\": \"tweet\"},) Further reading See Pinecone's documentation on filtering with metadata. See the list of LangChain vectorstore integrations that support metadata filtering. Advanced search and retrieval techniquesâ€‹ While algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity. For example, maximal marginal relevance is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results. As a second example, some vector stores offer built-in hybrid-search to combine keyword and semantic similarity search, which marries the benefits of both approaches. At the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with similarity_search. See this how-to guide on hybrid search for more details. NameWhen to useDescriptionHybrid searchWhen combining keyword-based and semantic similarity.Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. Paper.Maximal Marginal Relevance (MMR)When needing to diversify search results.MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.OverviewIntegrationsInterfaceInitializationAdding documentsDeleteSearchSimilarity metricsSimilarity searchMetadata filteringAdvanced search and retrieval techniques",
      "timestamp": "2025-08-24 15:30:28"
    },
    {
      "url": "https://python.langchain.com/docs/tutorials/sql_qa/",
      "title": "Build a Question/Answering system over SQL data | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Build a Question/Answering system over SQL data | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page PrerequisitesThis guide assumes familiarity with the following concepts: Chat models Tools Agents LangGraph Enabling a LLM system to query structured data can be qualitatively different from unstructured text data. Whereas in the latter it is common to generate text that can be searched against a vector database, the approach for structured data is often for the LLM to write and execute queries in a DSL, such as SQL. In this guide we'll go over the basic ways to create a Q&A system over tabular data in databases. We will cover implementations using both chains and agents. These systems will allow us to ask a question about the data in a database and get back a natural language answer. The main difference between the two is that our agent can query the database in a loop as many times as it needs to answer the question. âš ï¸ Security note âš ï¸â€‹ Building Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent's needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, see here. Architectureâ€‹ At a high-level, the steps of these systems are: Convert question to SQL query: Model converts user input to a SQL query. Execute SQL query: Execute the query. Answer the question: Model responds to user input using the query results. Note that querying data in CSVs can follow a similar approach. See our how-to guide on question-answering over CSV data for more detail. Setupâ€‹ First, get required packages and set environment variables: %%capture --no-stderr%pip install --upgrade --quiet langchain-community langgraph # Comment out the below to opt-out of using LangSmith in this notebook. Not required.if not os.environ.get(\"LANGSMITH_API_KEY\"): os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass() os.environ[\"LANGSMITH_TRACING\"] = \"true\" Sample dataâ€‹ The below example will use a SQLite connection with the Chinook database, which is a sample database that represents a digital media store. Follow these installation steps to create Chinook.db in the same directory as this notebook. You can also download and build the database via the command line: curl -s https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql | sqlite3 Chinook.db Now, Chinook.db is in our directory and we can interface with it using the SQLAlchemy-driven SQLDatabase class: from langchain_community.utilities import SQLDatabasedb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")print(db.dialect)print(db.get_usable_table_names())db.run(\"SELECT * FROM Artist LIMIT 10;\") sqlite['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track'] \"[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'AntÃ´nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\" Great! We've got a SQL database that we can query. Now let's try hooking it up to an LLM. Chainsâ€‹ Chains are compositions of predictable steps. In LangGraph, we can represent a chain via simple sequence of nodes. Let's create a sequence of steps that, given a question, does the following: converts the question into a SQL query; executes the query; uses the result to answer the original question. There are scenarios not supported by this arrangement. For example, this system will execute a SQL query for any user input-- even \"hello\". Importantly, as we'll see below, some questions require more than one query to answer. We will address these scenarios in the Agents section. Application stateâ€‹ The LangGraph state of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a TypedDict, but can also be a Pydantic BaseModel. For this application, we can just keep track of the input question, generated query, query result, and generated answer: from typing_extensions import TypedDictclass State(TypedDict): question: str query: str result: str answer: str Now we just need functions that operate on this state and populate its contents. Convert question to SQL queryâ€‹ The first step is to take the user input and convert it to a SQL query. To reliably obtain SQL queries (absent markdown formatting and explanations or clarifications), we will make use of LangChain's structured output abstraction. Let's select a chat model for our application: Select chat model:Google Geminiâ–¾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") Let's provide some instructions for our model: from langchain_core.prompts import ChatPromptTemplatesystem_message = \"\"\"Given an input question, create a syntactically correct {dialect} query torun to help find the answer. Unless the user specifies in his question aspecific number of examples they wish to obtain, always limit your query toat most {top_k} results. You can order the results by a relevant column toreturn the most interesting examples in the database.Never query for all the columns from a specific table, only ask for a thefew relevant columns given the question.Pay attention to use only the column names that you can see in the schemadescription. Be careful to not query for columns that do not exist. Also,pay attention to which column is in which table.Only use the following tables:{table_info}\"\"\"user_prompt = \"Question: {input}\"query_prompt_template = ChatPromptTemplate( [(\"system\", system_message), (\"user\", user_prompt)])for message in query_prompt_template.messages: message.pretty_print()API Reference:ChatPromptTemplate ================================\u001b[1m System Message \u001b[0m================================Given an input question, create a syntactically correct \u001b[33;1m\u001b[1;3m{dialect}\u001b[0m query torun to help find the answer. Unless the user specifies in his question aspecific number of examples they wish to obtain, always limit your query toat most \u001b[33;1m\u001b[1;3m{top_k}\u001b[0m results. You can order the results by a relevant column toreturn the most interesting examples in the database.Never query for all the columns from a specific table, only ask for a thefew relevant columns given the question.Pay attention to use only the column names that you can see in the schemadescription. Be careful to not query for columns that do not exist. Also,pay attention to which column is in which table.Only use the following tables:\u001b[33;1m\u001b[1;3m{table_info}\u001b[0m================================\u001b[1m Human Message \u001b[0m=================================Question: \u001b[33;1m\u001b[1;3m{input}\u001b[0m The prompt includes several parameters we will need to populate, such as the SQL dialect and table schemas. LangChain's SQLDatabase object includes methods to help with this. Our write_query step will just populate these parameters and prompt a model to generate the SQL query: from typing_extensions import Annotatedclass QueryOutput(TypedDict): \"\"\"Generated SQL query.\"\"\" query: Annotated[str, ..., \"Syntactically valid SQL query.\"]def write_query(state: State): \"\"\"Generate SQL query to fetch information.\"\"\" prompt = query_prompt_template.invoke( { \"dialect\": db.dialect, \"top_k\": 10, \"table_info\": db.get_table_info(), \"input\": state[\"question\"], } ) structured_llm = llm.with_structured_output(QueryOutput) result = structured_llm.invoke(prompt) return {\"query\": result[\"query\"]} Let's test it out: write_query({\"question\": \"How many Employees are there?\"}) {'query': 'SELECT COUNT(*) as employee_count FROM Employee;'} Execute queryâ€‹ This is the most dangerous part of creating a SQL chain. Consider carefully if it is OK to run automated queries over your data. Minimize the database connection permissions as much as possible. Consider adding a human approval step to you chains before query execution (see below). To execute the query, we will load a tool from langchain-community. Our execute_query node will just wrap this tool: from langchain_community.tools.sql_database.tool import QuerySQLDatabaseTooldef execute_query(state: State): \"\"\"Execute SQL query.\"\"\" execute_query_tool = QuerySQLDatabaseTool(db=db) return {\"result\": execute_query_tool.invoke(state[\"query\"])} Testing this step: execute_query({\"query\": \"SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;\"}) {'result': '[(8,)]'} Generate answerâ€‹ Finally, our last step generates an answer to the question given the information pulled from the database: def generate_answer(state: State): \"\"\"Answer question using retrieved information as context.\"\"\" prompt = ( \"Given the following user question, corresponding SQL query, \" \"and SQL result, answer the user question.\\n\\n\" f\"Question: {state['question']}\\n\" f\"SQL Query: {state['query']}\\n\" f\"SQL Result: {state['result']}\" ) response = llm.invoke(prompt) return {\"answer\": response.content} Orchestrating with LangGraphâ€‹ Finally, we compile our application into a single graph object. In this case, we are just connecting the three steps into a single sequence. from langgraph.graph import START, StateGraphgraph_builder = StateGraph(State).add_sequence( [write_query, execute_query, generate_answer])graph_builder.add_edge(START, \"write_query\")graph = graph_builder.compile()API Reference:StateGraph LangGraph also comes with built-in utilities for visualizing the control flow of your application: from IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png())) Let's test our application! Note that we can stream the results of individual steps: for step in graph.stream( {\"question\": \"How many employees are there?\"}, stream_mode=\"updates\"): print(step) {'write_query': {'query': 'SELECT COUNT(*) as employee_count FROM Employee;'}}{'execute_query': {'result': '[(8,)]'}}{'generate_answer': {'answer': 'There are 8 employees in total.'}} Check out the LangSmith trace. Human-in-the-loopâ€‹ LangGraph supports a number of features that can be useful for this workflow. One of them is human-in-the-loop: we can interrupt our application before sensitive steps (such as the execution of a SQL query) for human review. This is enabled by LangGraph's persistence layer, which saves run progress to your storage of choice. Below, we specify storage in-memory: from langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()graph = graph_builder.compile(checkpointer=memory, interrupt_before=[\"execute_query\"])# Now that we're using persistence, we need to specify a thread ID# so that we can continue the run after review.config = {\"configurable\": {\"thread_id\": \"1\"}}API Reference:MemorySaver display(Image(graph.get_graph().draw_mermaid_png())) Let's repeat the same run, adding in a simple yes/no approval step: for step in graph.stream( {\"question\": \"How many employees are there?\"}, config, stream_mode=\"updates\",): print(step)try: user_approval = input(\"Do you want to go to execute query? (yes/no): \")except Exception: user_approval = \"no\"if user_approval.lower() == \"yes\": # If approved, continue the graph execution for step in graph.stream(None, config, stream_mode=\"updates\"): print(step)else: print(\"Operation cancelled by user.\") {'write_query': {'query': 'SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;'}}{'__interrupt__': ()}``````outputDo you want to go to execute query? (yes/no): yes``````output{'execute_query': {'result': '[(8,)]'}}{'generate_answer': {'answer': 'There are 8 employees.'}} See this LangGraph guide for more detail and examples. Next stepsâ€‹ For more complex query-generation, we may want to create few-shot prompts or add query-checking steps. For advanced techniques like this and more check out: Prompting strategies: Advanced prompt engineering techniques. Query checking: Add query validation and error handling. Large databases: Techniques for working with large databases. Agentsâ€‹ Agents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the query generation and execution process. Although their behavior is less predictable than the above \"chain\", they feature some advantages: They can query the database as many times as needed to answer the user question. They can recover from errors by running a generated query, catching the traceback and regenerating it correctly. They can answer questions based on the databases' schema as well as on the databases' content (like describing a specific table). Below we assemble a minimal SQL agent. We will equip it with a set of tools using LangChain's SQLDatabaseToolkit. Using LangGraph's pre-built ReAct agent constructor, we can do this in one line. tipCheck out LangGraph's SQL Agent Tutorial for a more advanced formulation of a SQL agent. The SQLDatabaseToolkit includes tools that can: Create and execute queries Check query syntax Retrieve table descriptions ... and more from langchain_community.agent_toolkits import SQLDatabaseToolkittoolkit = SQLDatabaseToolkit(db=db, llm=llm)tools = toolkit.get_tools()tools [QuerySQLDatabaseTool(description=\"Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\", db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>), InfoSQLDatabaseTool(description='Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>), ListSQLDatabaseTool(db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>), QuerySQLCheckerTool(description='Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>, llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['dialect', 'query'], input_types={}, partial_variables={}, template='\\n{query}\\nDouble check the {dialect} query above for common mistakes, including:\\n- Using NOT IN with NULL values\\n- Using UNION when UNION ALL should have been used\\n- Using BETWEEN for exclusive ranges\\n- Data type mismatch in predicates\\n- Properly quoting identifiers\\n- Using the correct number of arguments for functions\\n- Casting to the correct data type\\n- Using the proper columns for joins\\n\\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\\n\\nOutput the final SQL query only.\\n\\nSQL Query: '), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}))] System Promptâ€‹ We will also want to load a system prompt for our agent. This will consist of instructions for how to behave. Note that the prompt below has several parameters, which we assign below. system_message = \"\"\"You are an agent designed to interact with a SQL database.Given an input question, create a syntactically correct {dialect} query to run,then look at the results of the query and return the answer. Unless the userspecifies a specific number of examples they wish to obtain, always limit yourquery to at most {top_k} results.You can order the results by a relevant column to return the most interestingexamples in the database. Never query for all the columns from a specific table,only ask for the relevant columns given the question.You MUST double check your query before executing it. If you get an error whileexecuting a query, rewrite the query and try again.DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to thedatabase.To start you should ALWAYS look at the tables in the database to see what youcan query. Do NOT skip this step.Then you should query the schema of the most relevant tables.\"\"\".format( dialect=\"SQLite\", top_k=5,) Initializing agentâ€‹ We will use a prebuilt LangGraph agent to build our agent from langchain_core.messages import HumanMessagefrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(llm, tools, prompt=system_message)API Reference:HumanMessage | create_react_agent Consider how the agent responds to the below question: question = \"Which country's customers spent the most?\"for step in agent_executor.stream( {\"messages\": [{\"role\": \"user\", \"content\": question}]}, stream_mode=\"values\",): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================Which country's customers spent the most?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: sql_db_list_tables (call_tFp7HYD6sAAmCShgeqkVZH6Q) Call ID: call_tFp7HYD6sAAmCShgeqkVZH6Q Args:=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_list_tablesAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: sql_db_schema (call_KJZ1Jx6JazyDdJa0uH1UeiOz) Call ID: call_KJZ1Jx6JazyDdJa0uH1UeiOz Args: table_names: Customer, Invoice=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_schemaCREATE TABLE \"Customer\" (\t\"CustomerId\" INTEGER NOT NULL, \t\"FirstName\" NVARCHAR(40) NOT NULL, \t\"LastName\" NVARCHAR(20) NOT NULL, \t\"Company\" NVARCHAR(80), \t\"Address\" NVARCHAR(70), \t\"City\" NVARCHAR(40), \t\"State\" NVARCHAR(40), \t\"Country\" NVARCHAR(40), \t\"PostalCode\" NVARCHAR(10), \t\"Phone\" NVARCHAR(24), \t\"Fax\" NVARCHAR(24), \t\"Email\" NVARCHAR(60) NOT NULL, \t\"SupportRepId\" INTEGER, \tPRIMARY KEY (\"CustomerId\"), \tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\"))/*3 rows from Customer table:CustomerId\tFirstName\tLastName\tCompany\tAddress\tCity\tState\tCountry\tPostalCode\tPhone\tFax\tEmail\tSupportRepId1\tLuÃ­s\tGonÃ§alves\tEmbraer - Empresa Brasileira de AeronÃ¡utica S.A.\tAv. Brigadeiro Faria Lima, 2170\tSÃ£o JosÃ© dos Campos\tSP\tBrazil\t12227-000\t+55 (12) 3923-5555\t+55 (12) 3923-5566\tluisg@embraer.com.br\t32\tLeonie\tKÃ¶hler\tNone\tTheodor-Heuss-StraÃŸe 34\tStuttgart\tNone\tGermany\t70174\t+49 0711 2842222\tNone\tleonekohler@surfeu.de\t53\tFranÃ§ois\tTremblay\tNone\t1498 rue BÃ©langer\tMontrÃ©al\tQC\tCanada\tH2G 1A7\t+1 (514) 721-4711\tNone\tftremblay@gmail.com\t3*/CREATE TABLE \"Invoice\" (\t\"InvoiceId\" INTEGER NOT NULL, \t\"CustomerId\" INTEGER NOT NULL, \t\"InvoiceDate\" DATETIME NOT NULL, \t\"BillingAddress\" NVARCHAR(70), \t\"BillingCity\" NVARCHAR(40), \t\"BillingState\" NVARCHAR(40), \t\"BillingCountry\" NVARCHAR(40), \t\"BillingPostalCode\" NVARCHAR(10), \t\"Total\" NUMERIC(10, 2) NOT NULL, \tPRIMARY KEY (\"InvoiceId\"), \tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\"))/*3 rows from Invoice table:InvoiceId\tCustomerId\tInvoiceDate\tBillingAddress\tBillingCity\tBillingState\tBillingCountry\tBillingPostalCode\tTotal1\t2\t2021-01-01 00:00:00\tTheodor-Heuss-StraÃŸe 34\tStuttgart\tNone\tGermany\t70174\t1.982\t4\t2021-01-02 00:00:00\tUllevÃ¥lsveien 14\tOslo\tNone\tNorway\t0171\t3.963\t8\t2021-01-03 00:00:00\tGrÃ©trystraat 63\tBrussels\tNone\tBelgium\t1000\t5.94*/==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: sql_db_query_checker (call_AQuTGbgH63u4gPgyV723yrjX) Call ID: call_AQuTGbgH63u4gPgyV723yrjX Args: query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_query_checker\\`\\`\\`sqlSELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;\\`\\`\\`==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: sql_db_query (call_B88EwU44nwwpQL5M9nlcemSU) Call ID: call_B88EwU44nwwpQL5M9nlcemSU Args: query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_query[('USA', 523.06)]==================================\u001b[1m Ai Message \u001b[0m==================================The country whose customers spent the most is the USA, with a total spending of 523.06. You can also use the LangSmith trace to visualize these steps and associated metadata. Note that the agent executes multiple queries until it has the information it needs: List available tables; Retrieves the schema for three tables; Queries multiple of the tables via a join operation. The agent is then able to use the result of the final query to generate an answer to the original question. The agent can similarly handle qualitative questions: question = \"Describe the playlisttrack table\"for step in agent_executor.stream( {\"messages\": [{\"role\": \"user\", \"content\": question}]}, stream_mode=\"values\",): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================Describe the playlisttrack table==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: sql_db_list_tables (call_fMF8eTmX5TJDJjc3Mhdg52TI) Call ID: call_fMF8eTmX5TJDJjc3Mhdg52TI Args:=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_list_tablesAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: sql_db_schema (call_W8Vkk4NEodkAAIg8nexAszUH) Call ID: call_W8Vkk4NEodkAAIg8nexAszUH Args: table_names: PlaylistTrack=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_schemaCREATE TABLE \"PlaylistTrack\" (\t\"PlaylistId\" INTEGER NOT NULL, \t\"TrackId\" INTEGER NOT NULL, \tPRIMARY KEY (\"PlaylistId\", \"TrackId\"), \tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \tFOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\"))/*3 rows from PlaylistTrack table:PlaylistId\tTrackId1\t34021\t33891\t3390*/==================================\u001b[1m Ai Message \u001b[0m==================================The `PlaylistTrack` table is designed to associate tracks with playlists. It has the following structure:- **PlaylistId**: An integer that serves as a foreign key referencing the `Playlist` table. It is part of the composite primary key.- **TrackId**: An integer that serves as a foreign key referencing the `Track` table. It is also part of the composite primary key.The primary key for this table is a composite key consisting of both `PlaylistId` and `TrackId`, ensuring that each track can be uniquely associated with a playlist. The table enforces referential integrity by linking to the `Track` and `Playlist` tables through foreign keys. Dealing with high-cardinality columnsâ€‹ In order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly. We can achieve this by creating a vector store with all the distinct proper nouns that exist in the database. We can then have the agent query that vector store each time the user includes a proper noun in their question, to find the correct spelling for that word. In this way, the agent can make sure it understands which entity the user is referring to before building the target query. First we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements: import astimport redef query_as_list(db, query): res = db.run(query) res = [el for sub in ast.literal_eval(res) for el in sub if el] res = [re.sub(r\"\\b\\d+\\b\", \"\", string).strip() for string in res] return list(set(res))artists = query_as_list(db, \"SELECT Name FROM Artist\")albums = query_as_list(db, \"SELECT Title FROM Album\")albums[:5] ['In Through The Out Door', 'Transmission', 'Battlestar Galactica (Classic), Season', 'A Copland Celebration, Vol. I', 'Quiet Songs'] Using this function, we can create a retriever tool that the agent can execute at its discretion. Let's select an embeddings model and vector store for this step: Select an embedding model: Select embeddings model:OpenAIâ–¾OpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakepip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"): os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") Select a vector store: Select vector store:In-memoryâ–¾In-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings) We can now construct a retrieval tool that can search over relevant proper nouns in the database: from langchain.agents.agent_toolkits import create_retriever_tool_ = vector_store.add_texts(artists + albums)retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})description = ( \"Use to look up values to filter on. Input is an approximate spelling \" \"of the proper noun, output is valid proper nouns. Use the noun most \" \"similar to the search.\")retriever_tool = create_retriever_tool( retriever, name=\"search_proper_nouns\", description=description,) Let's try it out: print(retriever_tool.invoke(\"Alice Chains\")) Alice In ChainsAlanis MorissettePearl JamPearl JamAudioslave This way, if the agent determines it needs to write a filter based on an artist along the lines of \"Alice Chains\", it can first use the retriever tool to observe relevant values of a column. Putting this together: # Add to system messagesuffix = ( \"If you need to filter on a proper noun like a Name, you must ALWAYS first look up \" \"the filter value using the 'search_proper_nouns' tool! Do not try to \" \"guess at the proper name - use this function to find similar ones.\")system = f\"{system_message}\\n\\n{suffix}\"tools.append(retriever_tool)agent = create_react_agent(llm, tools, prompt=system) question = \"How many albums does alis in chain have?\"for step in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": question}]}, stream_mode=\"values\",): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================How many albums does alis in chain have?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: search_proper_nouns (call_8ryjsRPLAr79mM3Qvnq6gTOH) Call ID: call_8ryjsRPLAr79mM3Qvnq6gTOH Args: query: alis in chain=================================\u001b[1m Tool Message \u001b[0m=================================Name: search_proper_nounsAlice In ChainsAisha DuoXisDa Lama Ao CaosA-Sides==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: sql_db_list_tables (call_NJjtCpU89MBMplssjn1z0xzq) Call ID: call_NJjtCpU89MBMplssjn1z0xzq Args: search_proper_nouns (call_1BfrueC9koSIyi4OfMu2Ao8q) Call ID: call_1BfrueC9koSIyi4OfMu2Ao8q Args: query: Alice In Chains=================================\u001b[1m Tool Message \u001b[0m=================================Name: search_proper_nounsAlice In ChainsPearl JamPearl JamFoo FightersSoundgarden==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: sql_db_schema (call_Kn09w9jd9swcNzIZ1b5MlKID) Call ID: call_Kn09w9jd9swcNzIZ1b5MlKID Args: table_names: Album, Artist=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_schemaCREATE TABLE \"Album\" (\t\"AlbumId\" INTEGER NOT NULL, \t\"Title\" NVARCHAR(160) NOT NULL, \t\"ArtistId\" INTEGER NOT NULL, \tPRIMARY KEY (\"AlbumId\"), \tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\"))/*3 rows from Album table:AlbumId\tTitle\tArtistId1\tFor Those About To Rock We Salute You\t12\tBalls to the Wall\t23\tRestless and Wild\t2*/CREATE TABLE \"Artist\" (\t\"ArtistId\" INTEGER NOT NULL, \t\"Name\" NVARCHAR(120), \tPRIMARY KEY (\"ArtistId\"))/*3 rows from Artist table:ArtistId\tName1\tAC/DC2\tAccept3\tAerosmith*/==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: sql_db_query (call_WkHRiPcBoGN9bc58MIupRHKP) Call ID: call_WkHRiPcBoGN9bc58MIupRHKP Args: query: SELECT COUNT(*) FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = 'Alice In Chains')=================================\u001b[1m Tool Message \u001b[0m=================================Name: sql_db_query[(1,)]==================================\u001b[1m Ai Message \u001b[0m==================================Alice In Chains has released 1 album in the database. As we can see, both in the streamed steps and in the LangSmith trace, the agent used the search_proper_nouns tool in order to check how to correctly query the database for this specific artist.âš ï¸ Security note âš ï¸ArchitectureSetupSample dataChainsApplication stateConvert question to SQL queryExecute queryGenerate answerOrchestrating with LangGraphHuman-in-the-loopNext stepsAgentsSystem PromptInitializing agentDealing with high-cardinality columns",
      "timestamp": "2025-08-24 15:30:31"
    },
    {
      "url": "https://python.langchain.com/docs/tutorials/extraction/",
      "title": "Build an Extraction Chain | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Build an Extraction Chain | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page In this tutorial, we will use tool-calling features of chat models to extract structured information from unstructured text. We will also demonstrate how to use few-shot prompting in this context to improve performance. importantThis tutorial requires langchain-core>=0.3.20 and will only work with models that support tool calling. Setupâ€‹ Jupyter Notebookâ€‹ This and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install. Installationâ€‹ To install LangChain run: PipCondapip install --upgrade langchain-coreconda install langchain-core -c conda-forge For more details, see our Installation guide. LangSmithâ€‹ Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. After you sign up at the link above, make sure to set your environment variables to start logging traces: export LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\" Or, if in a notebook, you can set them with: import getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass() The Schemaâ€‹ First, we need to describe what information we want to extract from the text. We'll use Pydantic to define an example schema to extract personal information. from typing import Optionalfrom pydantic import BaseModel, Fieldclass Person(BaseModel): \"\"\"Information about a person.\"\"\" # ^ Doc-string for the entity Person. # This doc-string is sent to the LLM as the description of the schema Person, # and it can help to improve extraction results. # Note that: # 1. Each field is an `optional` -- this allows the model to decline to extract it! # 2. Each field has a `description` -- this description is used by the LLM. # Having a good description can help improve extraction results. name: Optional[str] = Field(default=None, description=\"The name of the person\") hair_color: Optional[str] = Field( default=None, description=\"The color of the person's hair if known\" ) height_in_meters: Optional[str] = Field( default=None, description=\"Height measured in meters\" ) There are two best practices when defining schema: Document the attributes and the schema itself: This information is sent to the LLM and is used to improve the quality of information extraction. Do not force the LLM to make up information! Above we used Optional for the attributes allowing the LLM to output None if it doesn't know the answer. importantFor best performance, document the schema well and make sure the model isn't forced to return results if there's no information to be extracted in the text. The Extractorâ€‹ Let's create an information extractor using the schema we defined above. from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder# Define a custom prompt to provide instructions and any additional context.# 1) You can add examples into the prompt template to improve extraction quality# 2) Introduce additional parameters to take context into account (e.g., include metadata# about the document from which the text was extracted.)prompt_template = ChatPromptTemplate.from_messages( [ ( \"system\", \"You are an expert extraction algorithm. \" \"Only extract relevant information from the text. \" \"If you do not know the value of an attribute asked to extract, \" \"return null for the attribute's value.\", ), # Please see the how-to about improving performance with # reference examples. # MessagesPlaceholder('examples'), (\"human\", \"{text}\"), ])API Reference:ChatPromptTemplate | MessagesPlaceholder We need to use a model that supports function/tool calling. Please review the documentation for all models that can be used with this API. Select chat model:Google Geminiâ–¾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") structured_llm = llm.with_structured_output(schema=Person) Let's test it out: text = \"Alan Smith is 6 feet tall and has blond hair.\"prompt = prompt_template.invoke({\"text\": text})structured_llm.invoke(prompt) Person(name='Alan Smith', hair_color='blond', height_in_meters='1.83') importantExtraction is Generative ğŸ¤¯LLMs are generative models, so they can do some pretty cool things like correctly extract the height of the person in meters even though it was provided in feet! We can see the LangSmith trace here. Note that the chat model portion of the trace reveals the exact sequence of messages sent to the model, tools invoked, and other metadata. Multiple Entitiesâ€‹ In most cases, you should be extracting a list of entities rather than a single entity. This can be easily achieved using pydantic by nesting models inside one another. from typing import List, Optionalfrom pydantic import BaseModel, Fieldclass Person(BaseModel): \"\"\"Information about a person.\"\"\" # ^ Doc-string for the entity Person. # This doc-string is sent to the LLM as the description of the schema Person, # and it can help to improve extraction results. # Note that: # 1. Each field is an `optional` -- this allows the model to decline to extract it! # 2. Each field has a `description` -- this description is used by the LLM. # Having a good description can help improve extraction results. name: Optional[str] = Field(default=None, description=\"The name of the person\") hair_color: Optional[str] = Field( default=None, description=\"The color of the person's hair if known\" ) height_in_meters: Optional[str] = Field( default=None, description=\"Height measured in meters\" )class Data(BaseModel): \"\"\"Extracted data about people.\"\"\" # Creates a model so that we can extract multiple entities. people: List[Person] importantExtraction results might not be perfect here. Read on to see how to use Reference Examples to improve the quality of extraction, and check out our extraction how-to guides for more detail. structured_llm = llm.with_structured_output(schema=Data)text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"prompt = prompt_template.invoke({\"text\": text})structured_llm.invoke(prompt) Data(people=[Person(name='Jeff', hair_color='black', height_in_meters='1.83'), Person(name='Anna', hair_color='black', height_in_meters=None)]) tipWhen the schema accommodates the extraction of multiple entities, it also allows the model to extract no entities if no relevant information is in the text by providing an empty list.This is usually a good thing! It allows specifying required attributes on an entity without necessarily forcing the model to detect this entity. We can see the LangSmith trace here. Reference examplesâ€‹ The behavior of LLM applications can be steered using few-shot prompting. For chat models, this can take the form of a sequence of pairs of input and response messages demonstrating desired behaviors. For example, we can convey the meaning of a symbol with alternating user and assistant messages: messages = [ {\"role\": \"user\", \"content\": \"2 ğŸ¦œ 2\"}, {\"role\": \"assistant\", \"content\": \"4\"}, {\"role\": \"user\", \"content\": \"2 ğŸ¦œ 3\"}, {\"role\": \"assistant\", \"content\": \"5\"}, {\"role\": \"user\", \"content\": \"3 ğŸ¦œ 4\"},]response = llm.invoke(messages)print(response.content) 7 Structured output often uses tool calling under-the-hood. This typically involves the generation of AI messages containing tool calls, as well as tool messages containing the results of tool calls. What should a sequence of messages look like in this case? Different chat model providers impose different requirements for valid message sequences. Some will accept a (repeating) message sequence of the form: User message AI message with tool call Tool message with result Others require a final AI message containing some sort of response. LangChain includes a utility function tool_example_to_messages that will generate a valid sequence for most model providers. It simplifies the generation of structured few-shot examples by just requiring Pydantic representations of the corresponding tool calls. Let's try this out. We can convert pairs of input strings and desired Pydantic objects to a sequence of messages that can be provided to a chat model. Under the hood, LangChain will format the tool calls to each provider's required format. Note: this version of tool_example_to_messages requires langchain-core>=0.3.20. from langchain_core.utils.function_calling import tool_example_to_messagesexamples = [ ( \"The ocean is vast and blue. It's more than 20,000 feet deep.\", Data(people=[]), ), ( \"Fiona traveled far from France to Spain.\", Data(people=[Person(name=\"Fiona\", height_in_meters=None, hair_color=None)]), ),]messages = []for txt, tool_call in examples: if tool_call.people: # This final message is optional for some providers ai_response = \"Detected people.\" else: ai_response = \"Detected no people.\" messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))API Reference:tool_example_to_messages Inspecting the result, we see these two example pairs generated eight messages: for message in messages: message.pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================The ocean is vast and blue. It's more than 20,000 feet deep.==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: Data (d8f2e054-7fb9-417f-b28f-0447a775b2c3) Call ID: d8f2e054-7fb9-417f-b28f-0447a775b2c3 Args: people: []=================================\u001b[1m Tool Message \u001b[0m=================================You have correctly called this tool.==================================\u001b[1m Ai Message \u001b[0m==================================Detected no people.================================\u001b[1m Human Message \u001b[0m=================================Fiona traveled far from France to Spain.==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: Data (0178939e-a4b1-4d2a-a93e-b87f665cdfd6) Call ID: 0178939e-a4b1-4d2a-a93e-b87f665cdfd6 Args: people: [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]=================================\u001b[1m Tool Message \u001b[0m=================================You have correctly called this tool.==================================\u001b[1m Ai Message \u001b[0m==================================Detected people. Let's compare performance with and without these messages. For example, let's pass a message for which we intend no people to be extracted: message_no_extraction = { \"role\": \"user\", \"content\": \"The solar system is large, but earth has only 1 moon.\",}structured_llm = llm.with_structured_output(schema=Data)structured_llm.invoke([message_no_extraction]) Data(people=[Person(name='Earth', hair_color='None', height_in_meters='0.00')]) In this example, the model is liable to erroneously generate records of people. Because our few-shot examples contain examples of \"negatives\", we encourage the model to behave correctly in this case: structured_llm.invoke(messages + [message_no_extraction]) Data(people=[]) tipThe LangSmith trace for the run reveals the exact sequence of messages sent to the chat model, tool calls generated, latency, token counts, and other metadata. See this guide for more detail on extraction workflows with reference examples, including how to incorporate prompt templates and customize the generation of example messages. Next stepsâ€‹ Now that you understand the basics of extraction with LangChain, you're ready to proceed to the rest of the how-to guides: Add Examples: More detail on using reference examples to improve performance. Handle Long Text: What should you do if the text does not fit into the context window of the LLM? Use a Parsing Approach: Use a prompt based approach to extract with models that do not support tool/function calling. SetupJupyter NotebookInstallationLangSmithThe SchemaThe ExtractorMultiple EntitiesReference examplesNext steps",
      "timestamp": "2025-08-24 15:30:35"
    },
    {
      "url": "https://python.langchain.com/docs/concepts/document_loaders/",
      "title": "Document loaders | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Document loaders | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Prerequisites Document loaders API reference Document loaders are designed to load document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc. Integrationsâ€‹ You can find available integrations on the Document loaders integrations page. Interfaceâ€‹ Documents loaders implement the BaseLoader interface. Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the .load method or .lazy_load. Here's a simple example: from langchain_community.document_loaders.csv_loader import CSVLoaderloader = CSVLoader( ... # <-- Integration specific parameters here)data = loader.load() When working with large datasets, you can use the .lazy_load method: for document in loader.lazy_load(): print(document) Related resourcesâ€‹ Please see the following resources for more information: How-to guides for document loaders Document API reference Document loaders integrations IntegrationsInterfaceRelated resources",
      "timestamp": "2025-08-24 15:30:38"
    },
    {
      "url": "https://python.langchain.com/docs/tutorials/retrievers/",
      "title": "Build a semantic search engine | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Build a semantic search engine | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page This tutorial will familiarize you with LangChain's document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data-- from (vector) databases and other sources-- for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG (see our RAG tutorial here). Here we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. Conceptsâ€‹ This guide focuses on retrieval of text data. We will cover the following concepts: Documents and document loaders; Text splitters; Embeddings; Vector stores and retrievers. Setupâ€‹ Jupyter Notebookâ€‹ This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install. Installationâ€‹ This tutorial requires the langchain-community and pypdf packages: PipCondapip install langchain-community pypdfconda install langchain-community pypdf -c conda-forge For more details, see our Installation guide. LangSmithâ€‹ Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. After you sign up at the link above, make sure to set your environment variables to start logging traces: export LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\" Or, if in a notebook, you can set them with: import getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass() Documents and Document Loadersâ€‹ LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes: page_content: a string representing the content; metadata: a dict containing arbitrary metadata; id: (optional) a string identifier for the document. The metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document. We can generate sample documents when desired: from langchain_core.documents import Documentdocuments = [ Document( page_content=\"Dogs are great companions, known for their loyalty and friendliness.\", metadata={\"source\": \"mammal-pets-doc\"}, ), Document( page_content=\"Cats are independent pets that often enjoy their own space.\", metadata={\"source\": \"mammal-pets-doc\"}, ),]API Reference:Document However, the LangChain ecosystem implements document loaders that integrate with hundreds of common sources. This makes it easy to incorporate data from these sources into your AI application. Loading documentsâ€‹ Let's load a PDF into a sequence of Document objects. There is a sample PDF in the LangChain repo here -- a 10-k filing for Nike from 2023. We can consult the LangChain documentation for available PDF document loaders. Let's select PyPDFLoader, which is fairly lightweight. from langchain_community.document_loaders import PyPDFLoaderfile_path = \"../example_data/nke-10k-2023.pdf\"loader = PyPDFLoader(file_path)docs = loader.load()print(len(docs)) 107 tipSee this guide for more detail on PDF document loaders. PyPDFLoader loads one Document object per PDF page. For each, we can easily access: The string content of the page; Metadata containing the file name and page number. print(f\"{docs[0].page_content[:200]}\\n\")print(docs[0].metadata) Table of ContentsUNITED STATESSECURITIES AND EXCHANGE COMMISSIONWashington, D.C. 20549FORM 10-K(Mark One)â˜‘ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934FO{'source': '../example_data/nke-10k-2023.pdf', 'page': 0} Splittingâ€‹ For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text. We can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases. We set add_start_index=True so that the character index where each split Document starts within the initial Document is preserved as metadata attribute â€œstart_indexâ€. See this guide for more detail about working with PDFs, including how to extract text from specific sections and images. from langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=200, add_start_index=True)all_splits = text_splitter.split_documents(docs)len(all_splits) 514 Embeddingsâ€‹ Vector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text. LangChain supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. Let's select a model: Select embeddings model:OpenAIâ–¾OpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakepip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"): os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") vector_1 = embeddings.embed_query(all_splits[0].page_content)vector_2 = embeddings.embed_query(all_splits[1].page_content)assert len(vector_1) == len(vector_2)print(f\"Generated vectors of length {len(vector_1)}\\n\")print(vector_1[:10]) Generated vectors of length 1536[-0.008586574345827103, -0.03341241180896759, -0.008936782367527485, -0.0036674530711025, 0.010564599186182022, 0.009598285891115665, -0.028587326407432556, -0.015824200585484505, 0.0030416189692914486, -0.012899317778646946] Armed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search. Vector storesâ€‹ LangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors. LangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let's select a vector store: Select vector store:In-memoryâ–¾In-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings) Having instantiated our vector store, we can now index the documents. ids = vector_store.add_documents(documents=all_splits) Note that most vector store implementations will allow you to connect to an existing vector store-- e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail. Once we've instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying: Synchronously and asynchronously; By string query and by vector; With and without returning similarity scores; By similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results). The methods will generally include a list of Document objects in their outputs. Usageâ€‹ Embeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document. Return documents based on similarity to a string query: results = vector_store.similarity_search( \"How many distribution centers does Nike have in the US?\")print(results[0]) page_content='direct to consumer operations sell products through the following number of retail stores in the United States:U.S. RETAIL STORES NUMBERNIKE Brand factory stores 213 NIKE Brand in-line stores (including employee-only stores) 74 Converse stores (including factory stores) 82 TOTAL 369 In the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.2023 FORM 10-K 2' metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125} Async query: results = await vector_store.asimilarity_search(\"When was Nike incorporated?\")print(results[0]) page_content='Table of ContentsPART IITEM 1. BUSINESSGENERALNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.Our principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE isthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail storesand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales' metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0} Return scores: # Note that providers implement different scores; the score here# is a distance metric that varies inversely with similarity.results = vector_store.similarity_search_with_score(\"What was Nike's revenue in 2023?\")doc, score = results[0]print(f\"Score: {score}\\n\")print(doc) Score: 0.23699893057346344page_content='Table of ContentsFISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTSThe following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:FISCAL 2023 COMPARED TO FISCAL 2022â€¢NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.The increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,2 and 1 percentage points to NIKE, Inc. Revenues, respectively.â€¢NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. Thisincrease was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesaleequivalent basis.' metadata={'page': 35, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0} Return documents based on similarity to an embedded query: embedding = embeddings.embed_query(\"How were Nike's margins impacted in 2023?\")results = vector_store.similarity_search_by_vector(embedding)print(results[0]) page_content='Table of ContentsGROSS MARGINFISCAL 2023 COMPARED TO FISCAL 2022For fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:*Wholesale equivalentThe decrease in gross margin for fiscal 2023 was primarily due to:â€¢Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well asproduct mix;â€¢Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity inthe prior period resulting from lower available inventory supply;â€¢Unfavorable changes in net foreign currency exchange rates, including hedges; andâ€¢Lower off-price margin, on a wholesale equivalent basis.This was partially offset by:' metadata={'page': 36, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0} Learn more: API reference How-to guide Integration-specific docs Retrieversâ€‹ LangChain VectorStore objects do not subclass Runnable. LangChain Retrievers are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke and batch operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs). We can create a simple version of this ourselves, without subclassing Retriever. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the similarity_search method: from typing import Listfrom langchain_core.documents import Documentfrom langchain_core.runnables import chain@chaindef retriever(query: str) -> List[Document]: return vector_store.similarity_search(query, k=1)retriever.batch( [ \"How many distribution centers does Nike have in the US?\", \"When was Nike incorporated?\", ],)API Reference:Document | chain [[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')], [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]] Vectorstores implement an as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following: retriever = vector_store.as_retriever( search_type=\"similarity\", search_kwargs={\"k\": 1},)retriever.batch( [ \"How many distribution centers does Nike have in the US?\", \"When was Nike incorporated?\", ],) [[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')], [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]] VectorStoreRetriever supports search types of \"similarity\" (default), \"mmr\" (maximum marginal relevance, described above), and \"similarity_score_threshold\". We can use the latter to threshold documents output by the retriever by similarity score. Retrievers can easily be incorporated into more complex applications, such as retrieval-augmented generation (RAG) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the RAG tutorial tutorial. Learn more:â€‹ Retrieval strategies can be rich and complex. For example: We can infer hard rules and filters from a query (e.g., \"using documents published after 2020\"); We can return documents that are linked to the retrieved context in some way (e.g., via some document taxonomy); We can generate multiple embeddings for each unit of context; We can ensemble results from multiple retrievers; We can assign weights to documents, e.g., to weigh recent documents higher. The retrievers section of the how-to guides covers these and other built-in retrieval strategies. It is also straightforward to extend the BaseRetriever class in order to implement custom retrievers. See our how-to guide here. Next stepsâ€‹ You've now seen how to build a semantic search engine over a PDF document. For more on document loaders: Conceptual guide How-to guides Available integrations For more on embeddings: Conceptual guide How-to guides Available integrations For more on vector stores: Conceptual guide How-to guides Available integrations For more on RAG, see: Build a Retrieval Augmented Generation (RAG) App Related how-to guides ConceptsSetupJupyter NotebookInstallationLangSmithDocuments and Document LoadersLoading documentsSplittingEmbeddingsVector storesUsageRetrieversLearn more:Next steps",
      "timestamp": "2025-08-24 15:30:42"
    },
    {
      "url": "https://python.langchain.com/docs/concepts/embedding_models/",
      "title": "Embedding models | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Embedding models | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Prerequisites Documents NoteThis conceptual overview focuses on text-based embedding models.Embedding models can also be multimodal though such models are not currently supported by LangChain. Imagine being able to capture the essence of any text - a tweet, document, or book - in a single, compact representation. This is the power of embedding models, which lie at the heart of many retrieval systems. Embedding models transform human language into a format that machines can understand and compare with speed and accuracy. These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning. Embeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding. Key conceptsâ€‹ (1) Embed text as a vector: Embeddings transform text into a numerical vector representation. (2) Measure similarity: Embedding vectors can be compared using simple mathematical operations. Embeddingâ€‹ Historical contextâ€‹ The landscape of embedding models has evolved significantly over the years. A pivotal moment came in 2018 when Google introduced BERT (Bidirectional Encoder Representations from Transformers). BERT applied transformer models to embed text as a simple vector representation, which lead to unprecedented performance across various NLP tasks. However, BERT wasn't optimized for generating sentence embeddings efficiently. This limitation spurred the creation of SBERT (Sentence-BERT), which adapted the BERT architecture to generate semantically rich sentence embeddings, easily comparable via similarity metrics like cosine similarity, dramatically reduced the computational overhead for tasks like finding similar sentences. Today, the embedding model ecosystem is diverse, with numerous providers offering their own implementations. To navigate this variety, researchers and practitioners often turn to benchmarks like the Massive Text Embedding Benchmark (MTEB) here for objective comparisons. Further reading See the seminal BERT paper. See Cameron Wolfe's excellent review of embedding models. See the Massive Text Embedding Benchmark (MTEB) leaderboard for a comprehensive overview of embedding models. Interfaceâ€‹ LangChain provides a universal interface for working with them, providing standard methods for common operations. This common interface simplifies interaction with various embedding providers through two central methods: embed_documents: For embedding multiple texts (documents) embed_query: For embedding a single text (query) This distinction is important, as some providers employ different embedding strategies for documents (which are to be searched) versus queries (the search input itself). To illustrate, here's a practical example using LangChain's .embed_documents method to embed a list of strings: from langchain_openai import OpenAIEmbeddingsembeddings_model = OpenAIEmbeddings()embeddings = embeddings_model.embed_documents( [ \"Hi there!\", \"Oh, hello!\", \"What's your name?\", \"My friends call me World\", \"Hello World!\" ])len(embeddings), len(embeddings[0])(5, 1536) For convenience, you can also use the embed_query method to embed a single text: query_embedding = embeddings_model.embed_query(\"What is the meaning of life?\") Further reading See the full list of LangChain embedding model integrations. See these how-to guides for working with embedding models. Integrationsâ€‹ LangChain offers many embedding model integrations which you can find on the embedding models integrations page. Measure similarityâ€‹ Each embedding is essentially a set of coordinates, often in a high-dimensional space. In this space, the position of each point (embedding) reflects the meaning of its corresponding text. Just as similar words might be close to each other in a thesaurus, similar concepts end up close to each other in this embedding space. This allows for intuitive comparisons between different pieces of text. By reducing text to these numerical representations, we can use simple mathematical operations to quickly measure how alike two pieces of text are, regardless of their original length or structure. Some common similarity metrics include: Cosine Similarity: Measures the cosine of the angle between two vectors. Euclidean Distance: Measures the straight-line distance between two points. Dot Product: Measures the projection of one vector onto another. The choice of similarity metric should be chosen based on the model. As an example, OpenAI suggests cosine similarity for their embeddings, which can be easily implemented: import numpy as npdef cosine_similarity(vec1, vec2): dot_product = np.dot(vec1, vec2) norm_vec1 = np.linalg.norm(vec1) norm_vec2 = np.linalg.norm(vec2) return dot_product / (norm_vec1 * norm_vec2)similarity = cosine_similarity(query_result, document_result)print(\"Cosine Similarity:\", similarity) Further reading See Simon Willisonâ€™s nice blog post and video on embeddings and similarity metrics. See this documentation from Google on similarity metrics to consider with embeddings. See Pinecone's blog post on similarity metrics. See OpenAI's FAQ on what similarity metric to use with OpenAI embeddings. Key conceptsEmbeddingHistorical contextInterfaceIntegrationsMeasure similarity",
      "timestamp": "2025-08-24 15:30:46"
    },
    {
      "url": "https://python.langchain.com/docs/tutorials/classification/",
      "title": "Tagging | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Tagging | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Tagging means labeling a document with classes such as: Sentiment Language Style (formal, informal etc.) Covered topics Political tendency Overviewâ€‹ Tagging has a few components: function: Like extraction, tagging uses functions to specify how the model should tag a document schema: defines how we want to tag the document Quickstartâ€‹ Let's see a very straightforward example of how we can use OpenAI tool calling for tagging in LangChain. We'll use the with_structured_output method supported by OpenAI models. pip install -U langchain-core We'll need to load a chat model: Select chat model:Google Geminiâ–¾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") Let's specify a Pydantic model with a few properties and their expected type in our schema. from langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIfrom pydantic import BaseModel, Fieldtagging_prompt = ChatPromptTemplate.from_template( \"\"\"Extract the desired information from the following passage.Only extract the properties mentioned in the 'Classification' function.Passage:{input}\"\"\")class Classification(BaseModel): sentiment: str = Field(description=\"The sentiment of the text\") aggressiveness: int = Field( description=\"How aggressive the text is on a scale from 1 to 10\" ) language: str = Field(description=\"The language the text is written in\")# Structured LLMstructured_llm = llm.with_structured_output(Classification)API Reference:ChatPromptTemplate inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"prompt = tagging_prompt.invoke({\"input\": inp})response = structured_llm.invoke(prompt)response Classification(sentiment='positive', aggressiveness=1, language='Spanish') If we want dictionary output, we can just call .model_dump() inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"prompt = tagging_prompt.invoke({\"input\": inp})response = structured_llm.invoke(prompt)response.model_dump() {'sentiment': 'angry', 'aggressiveness': 8, 'language': 'Spanish'} As we can see in the examples, it correctly interprets what we want. The results vary so that we may get, for example, sentiments in different languages ('positive', 'enojado' etc.). We will see how to control these results in the next section. Finer controlâ€‹ Careful schema definition gives us more control over the model's output. Specifically, we can define: Possible values for each property Description to make sure that the model understands the property Required properties to be returned Let's redeclare our Pydantic model to control for each of the previously mentioned aspects using enums: class Classification(BaseModel): sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"]) aggressiveness: int = Field( ..., description=\"describes how aggressive the statement is, the higher the number the more aggressive\", enum=[1, 2, 3, 4, 5], ) language: str = Field( ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"] ) tagging_prompt = ChatPromptTemplate.from_template( \"\"\"Extract the desired information from the following passage.Only extract the properties mentioned in the 'Classification' function.Passage:{input}\"\"\")llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\").with_structured_output( Classification) Now the answers will be restricted in a way we expect! inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"prompt = tagging_prompt.invoke({\"input\": inp})llm.invoke(prompt) Classification(sentiment='happy', aggressiveness=1, language='spanish') inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"prompt = tagging_prompt.invoke({\"input\": inp})llm.invoke(prompt) Classification(sentiment='sad', aggressiveness=4, language='spanish') inp = \"Weather is ok here, I can go outside without much more than a coat\"prompt = tagging_prompt.invoke({\"input\": inp})llm.invoke(prompt) Classification(sentiment='happy', aggressiveness=1, language='english') The LangSmith trace lets us peek under the hood: Going deeperâ€‹ You can use the metadata tagger document transformer to extract metadata from a LangChain Document. This covers the same basic functionality as the tagging chain, only applied to a LangChain Document. OverviewQuickstartFiner controlGoing deeper",
      "timestamp": "2025-08-24 15:30:49"
    },
    {
      "url": "https://python.langchain.com/docs/concepts/structured_outputs/",
      "title": "Structured outputs | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Structured outputs | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Overviewâ€‹ For many applications, such as chatbots, models need to respond to users directly in natural language. However, there are scenarios where we need models to output in a structured format. For example, we might want to store the model output in a database and ensure that the output conforms to the database schema. This need motivates the concept of structured output, where models can be instructed to respond with a particular output structure. Key conceptsâ€‹ Schema definition: The output structure is represented as a schema, which can be defined in several ways. Returning structured output: The model is given this schema, and is instructed to return output that conforms to it. Recommended usageâ€‹ This pseudocode illustrates the recommended workflow when using structured output. LangChain provides a method, with_structured_output(), that automates the process of binding the schema to the model and parsing the output. This helper function is available for all model providers that support structured output. # Define schemaschema = {\"foo\": \"bar\"}# Bind schema to modelmodel_with_structure = model.with_structured_output(schema)# Invoke the model to produce structured output that matches the schemastructured_output = model_with_structure.invoke(user_input) Tool Order MattersWhen combining structured output with additional tools, bind tools first, then apply structured output:# Correctmodel_with_tools = model.bind_tools([tool1, tool2])structured_model = model_with_tools.with_structured_output(schema)# Incorrect - will cause tool resolution errorsstructured_model = model.with_structured_output(schema)broken_model = structured_model.bind_tools([tool1, tool2]) Schema definitionâ€‹ The central concept is that the output structure of model responses needs to be represented in some way. While types of objects you can use depend on the model you're working with, there are common types of objects that are typically allowed or recommended for structured output in Python. The simplest and most common format for structured output is a JSON-like structure, which in Python can be represented as a dictionary (dict) or list (list). JSON objects (or dicts in Python) are often used directly when the tool requires raw, flexible, and minimal-overhead structured data. { \"answer\": \"The answer to the user's question\", \"followup_question\": \"A followup question the user could ask\"} As a second example, Pydantic is particularly useful for defining structured output schemas because it offers type hints and validation. Here's an example of a Pydantic schema: from pydantic import BaseModel, Fieldclass ResponseFormatter(BaseModel): \"\"\"Always use this tool to structure your response to the user.\"\"\" answer: str = Field(description=\"The answer to the user's question\") followup_question: str = Field(description=\"A followup question the user could ask\") Returning structured outputâ€‹ With a schema defined, we need a way to instruct the model to use it. While one approach is to include this schema in the prompt and ask nicely for the model to use it, this is not recommended. Several more powerful methods that utilizes native features in the model provider's API are available. Using tool callingâ€‹ Many model providers support tool calling, a concept discussed in more detail in our tool calling guide. In short, tool calling involves binding a tool to a model and, when appropriate, the model can decide to call this tool and ensure its response conforms to the tool's schema. With this in mind, the central concept is straightforward: simply bind our schema to a model as a tool! Here is an example using the ResponseFormatter schema defined above: from langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)# Bind responseformatter schema as a tool to the modelmodel_with_tools = model.bind_tools([ResponseFormatter])# Invoke the modelai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\") The arguments of the tool call are already extracted as a dictionary. This dictionary can be optionally parsed into a Pydantic object, matching our original ResponseFormatter schema. # Get the tool call argumentsai_msg.tool_calls[0][\"args\"]{'answer': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", 'followup_question': 'What is the function of ATP in the cell?'}# Parse the dictionary into a pydantic objectpydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"]) JSON modeâ€‹ In addition to tool calling, some model providers support a feature called JSON mode. This supports JSON schema definition as input and enforces the model to produce a conforming JSON output. You can find a table of model providers that support JSON mode here. Here is an example of how to use JSON mode with OpenAI: from langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o\").with_structured_output(method=\"json_mode\")ai_msg = model.invoke(\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\")ai_msg{'random_ints': [45, 67, 12, 34, 89, 23, 78, 56, 90, 11]} Structured output methodâ€‹ There are a few challenges when producing structured output with the above methods: When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema. In addition, the model needs to be instructed to always use the tool when we want to enforce structured output, which is a provider specific setting. When JSON mode is used, the output needs to be parsed into a JSON object. With these challenges in mind, LangChain provides a helper function (with_structured_output()) to streamline the process. This both binds the schema to the model as a tool and parses the output to the specified output schema. # Bind the schema to the modelmodel_with_structure = model.with_structured_output(ResponseFormatter)# Invoke the modelstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")# Get back the pydantic objectstructured_outputResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question='What is the function of ATP in the cell?') Further readingFor more details on usage, see our how-to guide.OverviewKey conceptsRecommended usageSchema definitionReturning structured outputUsing tool callingJSON modeStructured output method",
      "timestamp": "2025-08-24 15:30:52"
    },
    {
      "url": "https://python.langchain.com/docs/tutorials/qa_chat_history/",
      "title": "Build a Retrieval Augmented Generation (RAG) App: Part 2 | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Build a Retrieval Augmented Generation (RAG) App: Part 2 | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking. This is the second part of a multi-part tutorial: Part 1 introduces RAG and walks through a minimal implementation. Part 2 (this guide) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes. Here we focus on adding logic for incorporating historical messages. This involves the management of a chat history. We will cover two approaches: Chains, in which we execute at most one retrieval step; Agents, in which we give an LLM discretion to execute multiple retrieval steps. noteThe methods presented here leverage tool-calling capabilities in modern chat models. See this page for a table of models supporting tool calling features. For the external knowledge source, we will use the same LLM Powered Autonomous Agents blog post by Lilian Weng from the Part 1 of the RAG tutorial. Setupâ€‹ Componentsâ€‹ We will need to select three components from LangChain's suite of integrations. Select chat model:Google Geminiâ–¾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") Select embeddings model:OpenAIâ–¾OpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakepip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"): os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") Select vector store:In-memoryâ–¾In-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings) Dependenciesâ€‹ In addition, we'll use the following packages: %%capture --no-stderr%pip install --upgrade --quiet langgraph langchain-community beautifulsoup4 LangSmithâ€‹ Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces: os.environ[\"LANGSMITH_TRACING\"] = \"true\"if not os.environ.get(\"LANGSMITH_API_KEY\"): os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass() Chainsâ€‹ Let's first revisit the vector store we built in Part 1, which indexes an LLM Powered Autonomous Agents blog post by Lilian Weng. import bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader( web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(\"post-content\", \"post-title\", \"post-header\") ) ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)API Reference:Document # Index chunks_ = vector_store.add_documents(documents=all_splits) In the Part 1 of the RAG tutorial, we represented the user input, retrieved context, and generated answer as separate keys in the state. Conversational experiences can be naturally represented using a sequence of messages. In addition to messages from the user and assistant, retrieved documents and other artifacts can be incorporated into a message sequence via tool messages. This motivates us to represent the state of our RAG application using a sequence of messages. Specifically, we will have User input as a HumanMessage; Vector store query as an AIMessage with tool calls; Retrieved documents as a ToolMessage; Final response as a AIMessage. This model for state is so versatile that LangGraph offers a built-in version for convenience: from langgraph.graph import MessagesState, StateGraphgraph_builder = StateGraph(MessagesState)API Reference:StateGraph Leveraging tool-calling to interact with a retrieval step has another benefit, which is that the query for the retrieval is generated by our model. This is especially important in a conversational setting, where user queries may require contextualization based on the chat history. For instance, consider the following exchange: Human: \"What is Task Decomposition?\" AI: \"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model.\" Human: \"What are common ways of doing it?\" In this scenario, a model could generate a query such as \"common approaches to task decomposition\". Tool-calling facilitates this naturally. As in the query analysis section of the RAG tutorial, this allows a model to rewrite user queries into more effective search queries. It also provides support for direct responses that do not involve a retrieval step (e.g., in response to a generic greeting from the user). Let's turn our retrieval step into a tool: from langchain_core.tools import tool@tool(response_format=\"content_and_artifact\")def retrieve(query: str): \"\"\"Retrieve information related to a query.\"\"\" retrieved_docs = vector_store.similarity_search(query, k=2) serialized = \"\\n\\n\".join( (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\") for doc in retrieved_docs ) return serialized, retrieved_docsAPI Reference:tool See this guide for more detail on creating tools. Our graph will consist of three nodes: A node that fields the user input, either generating a query for the retriever or responding directly; A node for the retriever tool that executes the retrieval step; A node that generates the final response using the retrieved context. We build them below. Note that we leverage another pre-built LangGraph component, ToolNode, that executes the tool and adds the result as a ToolMessage to the state. from langchain_core.messages import SystemMessagefrom langgraph.prebuilt import ToolNode# Step 1: Generate an AIMessage that may include a tool-call to be sent.def query_or_respond(state: MessagesState): \"\"\"Generate tool call for retrieval or respond.\"\"\" llm_with_tools = llm.bind_tools([retrieve]) response = llm_with_tools.invoke(state[\"messages\"]) # MessagesState appends messages to state instead of overwriting return {\"messages\": [response]}# Step 2: Execute the retrieval.tools = ToolNode([retrieve])# Step 3: Generate a response using the retrieved content.def generate(state: MessagesState): \"\"\"Generate answer.\"\"\" # Get generated ToolMessages recent_tool_messages = [] for message in reversed(state[\"messages\"]): if message.type == \"tool\": recent_tool_messages.append(message) else: break tool_messages = recent_tool_messages[::-1] # Format into prompt docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages) system_message_content = ( \"You are an assistant for question-answering tasks. \" \"Use the following pieces of retrieved context to answer \" \"the question. If you don't know the answer, say that you \" \"don't know. Use three sentences maximum and keep the \" \"answer concise.\" \"\\n\\n\" f\"{docs_content}\" ) conversation_messages = [ message for message in state[\"messages\"] if message.type in (\"human\", \"system\") or (message.type == \"ai\" and not message.tool_calls) ] prompt = [SystemMessage(system_message_content)] + conversation_messages # Run response = llm.invoke(prompt) return {\"messages\": [response]}API Reference:SystemMessage | ToolNode Finally, we compile our application into a single graph object. In this case, we are just connecting the steps into a sequence. We also allow the first query_or_respond step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step from langgraph.graph import ENDfrom langgraph.prebuilt import ToolNode, tools_conditiongraph_builder.add_node(query_or_respond)graph_builder.add_node(tools)graph_builder.add_node(generate)graph_builder.set_entry_point(\"query_or_respond\")graph_builder.add_conditional_edges( \"query_or_respond\", tools_condition, {END: END, \"tools\": \"tools\"},)graph_builder.add_edge(\"tools\", \"generate\")graph_builder.add_edge(\"generate\", END)graph = graph_builder.compile()API Reference:ToolNode | tools_condition from IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png())) Let's test our application. Note that it responds appropriately to messages that do not require an additional retrieval step: input_message = \"Hello\"for step in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": input_message}]}, stream_mode=\"values\",): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================Hello==================================\u001b[1m Ai Message \u001b[0m==================================Hello! How can I assist you today? And when executing a search, we can stream the steps to observe the query generation, retrieval, and answer generation: input_message = \"What is Task Decomposition?\"for step in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": input_message}]}, stream_mode=\"values\",): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================What is Task Decomposition?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: retrieve (call_dLjB3rkMoxZZxwUGXi33UBeh) Call ID: call_dLjB3rkMoxZZxwUGXi33UBeh Args: query: Task Decomposition=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.==================================\u001b[1m Ai Message \u001b[0m==================================Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps. It often involves techniques like Chain of Thought (CoT), which encourages models to think step by step, enhancing performance on complex tasks. This approach allows for a clearer understanding of the task and aids in structuring the problem-solving process. Check out the LangSmith trace here. Stateful management of chat historyâ€‹ noteThis section of the tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.Please see How to migrate to LangGraph Memory for more details. In production, the Q&A application will usually persist the chat history into a database, and be able to read and update it appropriately. LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns. To manage multiple conversational turns and threads, all we have to do is specify a checkpointer when compiling our application. Because the nodes in our graph are appending messages to the state, we will retain a consistent chat history across invocations. LangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres). For a detailed walkthrough of how to manage message history, head to the How to add message history (memory) guide. from langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()graph = graph_builder.compile(checkpointer=memory)# Specify an ID for the threadconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}API Reference:MemorySaver We can now invoke similar to before: input_message = \"What is Task Decomposition?\"for step in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": input_message}]}, stream_mode=\"values\", config=config,): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================What is Task Decomposition?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: retrieve (call_JZb6GLD812bW2mQsJ5EJQDnN) Call ID: call_JZb6GLD812bW2mQsJ5EJQDnN Args: query: Task Decomposition=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.==================================\u001b[1m Ai Message \u001b[0m==================================Task Decomposition is a technique used to break down complicated tasks into smaller, manageable steps. It involves using methods like Chain of Thought (CoT) prompting, which encourages the model to think step by step, enhancing performance on complex tasks. This process helps to clarify the model's reasoning and makes it easier to tackle difficult problems. input_message = \"Can you look up some common ways of doing it?\"for step in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": input_message}]}, stream_mode=\"values\", config=config,): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================Can you look up some common ways of doing it?==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: retrieve (call_kjRI4Y5cJOiB73yvd7dmb6ux) Call ID: call_kjRI4Y5cJOiB73yvd7dmb6ux Args: query: common methods of task decomposition=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.==================================\u001b[1m Ai Message \u001b[0m==================================Common ways of performing Task Decomposition include: (1) using Large Language Models (LLMs) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", (2) employing task-specific instructions such as \"Write a story outline\" for specific tasks, and (3) incorporating human inputs to guide the decomposition process. Note that the query generated by the model in the second question incorporates the conversational context. The LangSmith trace is particularly informative here, as we can see exactly what messages are visible to our chat model at each step. Agentsâ€‹ Agents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above \"chain\", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search. Below we assemble a minimal RAG agent. Using LangGraph's pre-built ReAct agent constructor, we can do this in one line. tipCheck out LangGraph's Agentic RAG tutorial for more advanced formulations. from langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)API Reference:create_react_agent Let's inspect the graph: display(Image(agent_executor.get_graph().draw_mermaid_png())) The key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information. Let's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer: config = {\"configurable\": {\"thread_id\": \"def234\"}}input_message = ( \"What is the standard method for Task Decomposition?\\n\\n\" \"Once you get the answer, look up common extensions of that method.\")for event in agent_executor.stream( {\"messages\": [{\"role\": \"user\", \"content\": input_message}]}, stream_mode=\"values\", config=config,): event[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================What is the standard method for Task Decomposition?Once you get the answer, look up common extensions of that method.==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: retrieve (call_Y3YaIzL71B83Cjqa8d2G0O8N) Call ID: call_Y3YaIzL71B83Cjqa8d2G0O8N Args: query: standard method for Task Decomposition=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.==================================\u001b[1m Ai Message \u001b[0m==================================Tool Calls: retrieve (call_2JntP1x4XQMWwgVpYurE12ff) Call ID: call_2JntP1x4XQMWwgVpYurE12ff Args: query: common extensions of Task Decomposition methods=================================\u001b[1m Tool Message \u001b[0m=================================Name: retrieveSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.==================================\u001b[1m Ai Message \u001b[0m==================================The standard method for task decomposition involves using techniques such as Chain of Thought (CoT), where a model is instructed to \"think step by step\" to break down complex tasks into smaller, more manageable components. This approach enhances model performance by allowing for more thorough reasoning and planning. Task decomposition can be accomplished through various means, including:1. Simple prompting (e.g., asking for steps to achieve a goal).2. Task-specific instructions (e.g., asking for a story outline).3. Human inputs to guide the decomposition process.### Common Extensions of Task Decomposition Methods:1. **Tree of Thoughts**: This extension builds on CoT by not only decomposing the problem into thought steps but also generating multiple thoughts at each step, creating a tree structure. The search process can employ breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or through majority voting.These extensions aim to enhance reasoning capabilities and improve the effectiveness of task decomposition in various contexts. Note that the agent: Generates a query to search for a standard method for task decomposition; Receiving the answer, generates a second query to search for common extensions of it; Having received all necessary context, answers the question. We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace. Next stepsâ€‹ We've covered the steps to build a basic conversational Q&A application: We used chains to build a predictable application that generates at most one query per user input; We used agents to build an application that can iterate on a sequence of queries. To explore different types of retrievers and retrieval strategies, visit the retrievers section of the how-to guides. For a detailed walkthrough of LangChain's conversation memory abstractions, visit the How to add message history (memory) guide. To learn more about agents, check out the conceptual guide and LangGraph agent architectures page.SetupComponentsDependenciesLangSmithChainsStateful management of chat historyAgentsNext steps",
      "timestamp": "2025-08-24 15:30:56"
    },
    {
      "url": "https://python.langchain.com/docs/tutorials/graph/",
      "title": "Build a Question Answering application over a Graph Database | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "Build a Question Answering application over a Graph Database | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page In this guide we'll go over the basic ways to create a Q&A chain over a graph database. These systems will allow us to ask a question about the data in a graph database and get back a natural language answer. First, we will show a simple out-of-the-box option and then implement a more sophisticated version with LangGraph. âš ï¸ Security note âš ï¸â€‹ Building Q&A systems of graph databases requires executing model-generated graph queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent's needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, see here. Architectureâ€‹ At a high-level, the steps of most graph chains are: Convert question to a graph database query: Model converts user input to a graph database query (e.g. Cypher). Execute graph database query: Execute the graph database query. Answer the question: Model responds to user input using the query results. Setupâ€‹ First, get required packages and set environment variables. In this example, we will be using Neo4j graph database. %pip install --upgrade --quiet langchain langchain-neo4j langchain-openai langgraph We default to OpenAI models in this guide. import getpassimport osif \"OPENAI_API_KEY\" not in os.environ: os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")# Uncomment the below to use LangSmith. Not required.# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()# os.environ[\"LANGSMITH_TRACING\"] = \"true\" Enter your OpenAI API key: Â·Â·Â·Â·Â·Â·Â·Â· Next, we need to define Neo4j credentials. Follow these installation steps to set up a Neo4j database. os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"os.environ[\"NEO4J_PASSWORD\"] = \"password\" The below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors. from langchain_neo4j import Neo4jGraphgraph = Neo4jGraph()# Import movie informationmovies_query = \"\"\"LOAD CSV WITH HEADERS FROM 'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'AS rowMERGE (m:Movie {id:row.movieId})SET m.released = date(row.released), m.title = row.title, m.imdbRating = toFloat(row.imdbRating)FOREACH (director in split(row.director, '|') | MERGE (p:Person {name:trim(director)}) MERGE (p)-[:DIRECTED]->(m))FOREACH (actor in split(row.actors, '|') | MERGE (p:Person {name:trim(actor)}) MERGE (p)-[:ACTED_IN]->(m))FOREACH (genre in split(row.genres, '|') | MERGE (g:Genre {name:trim(genre)}) MERGE (m)-[:IN_GENRE]->(g))\"\"\"graph.query(movies_query) [] Graph schemaâ€‹ In order for an LLM to be able to generate a Cypher statement, it needs information about the graph schema. When you instantiate a graph object, it retrieves the information about the graph schema. If you later make any changes to the graph, you can run the refresh_schema method to refresh the schema information. graph.refresh_schema()print(graph.schema) Node properties:Person {name: STRING}Movie {id: STRING, released: DATE, title: STRING, imdbRating: FLOAT}Genre {name: STRING}Chunk {id: STRING, embedding: LIST, text: STRING, question: STRING, query: STRING}Relationship properties:The relationships:(:Person)-[:DIRECTED]->(:Movie)(:Person)-[:ACTED_IN]->(:Movie)(:Movie)-[:IN_GENRE]->(:Genre) For more involved schema information, you can use enhanced_schema option. enhanced_graph = Neo4jGraph(enhanced_schema=True)print(enhanced_graph.schema) Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"``````outputNode properties:- **Person** - `name`: STRING Example: \"John Lasseter\"- **Movie** - `id`: STRING Example: \"1\" - `released`: DATE Min: 1964-12-16, Max: 1996-09-15 - `title`: STRING Example: \"Toy Story\" - `imdbRating`: FLOAT Min: 2.4, Max: 9.3- **Genre** - `name`: STRING Example: \"Adventure\"- **Chunk** - `id`: STRING Available options: ['d66006059fd78d63f3df90cc1059639a', '0e3dcb4502853979d12357690a95ec17', 'c438c6bcdcf8e4fab227f29f8e7ff204', '97fe701ec38057594464beaa2df0710e', 'b54f9286e684373498c4504b4edd9910', '5b50a72c3a4954b0ff7a0421be4f99b9', 'fb28d41771e717255f0d8f6c799ede32', '58e6f14dd2e6c6702cf333f2335c499c'] - `text`: STRING Available options: ['How many artists are there?', 'Which actors played in the movie Casino?', 'How many movies has Tom Hanks acted in?', \"List all the genres of the movie Schindler's List\", 'Which actors have worked in movies from both the c', 'Which directors have made movies with at least thr', 'Identify movies where directors also played a role', 'Find the actor with the highest number of movies i'] - `question`: STRING Available options: ['How many artists are there?', 'Which actors played in the movie Casino?', 'How many movies has Tom Hanks acted in?', \"List all the genres of the movie Schindler's List\", 'Which actors have worked in movies from both the c', 'Which directors have made movies with at least thr', 'Identify movies where directors also played a role', 'Find the actor with the highest number of movies i'] - `query`: STRING Available options: ['MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN coun', \"MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a)\", \"MATCH (a:Person {name: 'Tom Hanks'})-[:ACTED_IN]->\", \"MATCH (m:Movie {title: 'Schindler's List'})-[:IN_G\", 'MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]', 'MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_I', 'MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACT', 'MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.na']Relationship properties:The relationships:(:Person)-[:DIRECTED]->(:Movie)(:Person)-[:ACTED_IN]->(:Movie)(:Movie)-[:IN_GENRE]->(:Genre) The enhanced_schema option enriches property information by including details such as minimum and maximum values for floats and dates, as well as example values for string properties. This additional context helps guide the LLM toward generating more accurate and effective queries. Great! We've got a graph database that we can query. Now let's try hooking it up to an LLM. GraphQACypherChainâ€‹ Let's use a simple out-of-the-box chain that takes a question, turns it into a Cypher query, executes the query, and uses the result to answer the original question. LangChain comes with a built-in chain for this workflow that is designed to work with Neo4j: GraphCypherQAChain from langchain_neo4j import GraphCypherQAChainfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)chain = GraphCypherQAChain.from_llm( graph=enhanced_graph, llm=llm, verbose=True, allow_dangerous_requests=True)response = chain.invoke({\"query\": \"What was the cast of the Casino?\"})response \u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0mGenerated Cypher:\u001b[32;1m\u001b[1;3mcypherMATCH (p:Person)-[:ACTED_IN]->(m:Movie {title: \"Casino\"})RETURN p.name\u001b[0mFull Context:\u001b[32;1m\u001b[1;3m[{'p.name': 'Robert De Niro'}, {'p.name': 'Joe Pesci'}, {'p.name': 'Sharon Stone'}, {'p.name': 'James Woods'}]\u001b[0m\u001b[1m> Finished chain.\u001b[0m {'query': 'What was the cast of the Casino?', 'result': 'Robert De Niro, Joe Pesci, Sharon Stone, and James Woods were the cast of Casino.'} Advanced implementation with LangGraphâ€‹ While the GraphCypherQAChain is effective for quick demonstrations, it may face challenges in production environments. Transitioning to LangGraph can enhance the workflow, but implementing natural language to query flows in production remains a complex task. Nevertheless, there are several strategies to significantly improve accuracy and reliability, which we will explore next. Here is the visualized LangGraph flow we will implement: We will begin by defining the Input, Output, and Overall state of the LangGraph application. from operator import addfrom typing import Annotated, Listfrom typing_extensions import TypedDictclass InputState(TypedDict): question: strclass OverallState(TypedDict): question: str next_action: str cypher_statement: str cypher_errors: List[str] database_records: List[dict] steps: Annotated[List[str], add]class OutputState(TypedDict): answer: str steps: List[str] cypher_statement: str The first step is a simple guardrails step, where we validate whether the question pertains to movies or their cast. If it doesn't, we notify the user that we cannot answer any other questions. Otherwise, we move on to the Cypher generation step. from typing import Literalfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldguardrails_system = \"\"\"As an intelligent assistant, your primary objective is to decide whether a given question is related to movies or not. If the question is related to movies, output \"movie\". Otherwise, output \"end\".To make this decision, assess the content of the question and determine if it refers to any movie, actor, director, film industry, or related topics. Provide only the specified output: \"movie\" or \"end\".\"\"\"guardrails_prompt = ChatPromptTemplate.from_messages( [ ( \"system\", guardrails_system, ), ( \"human\", (\"{question}\"), ), ])class GuardrailsOutput(BaseModel): decision: Literal[\"movie\", \"end\"] = Field( description=\"Decision on whether the question is related to movies\" )guardrails_chain = guardrails_prompt | llm.with_structured_output(GuardrailsOutput)def guardrails(state: InputState) -> OverallState: \"\"\" Decides if the question is related to movies or not. \"\"\" guardrails_output = guardrails_chain.invoke({\"question\": state.get(\"question\")}) database_records = None if guardrails_output.decision == \"end\": database_records = \"This questions is not about movies or their cast. Therefore I cannot answer this question.\" return { \"next_action\": guardrails_output.decision, \"database_records\": database_records, \"steps\": [\"guardrail\"], }API Reference:ChatPromptTemplate Few-shot promptingâ€‹ Converting natural language into accurate queries is challenging. One way to enhance this process is by providing relevant few-shot examples to guide the LLM in query generation. To achieve this, we will use the SemanticSimilarityExampleSelector to dynamically select the most relevant examples. from langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_neo4j import Neo4jVectorfrom langchain_openai import OpenAIEmbeddingsexamples = [ { \"question\": \"How many artists are there?\", \"query\": \"MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN count(DISTINCT a)\", }, { \"question\": \"Which actors played in the movie Casino?\", \"query\": \"MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a) RETURN a.name\", }, { \"question\": \"How many movies has Tom Hanks acted in?\", \"query\": \"MATCH (a:Person {name: 'Tom Hanks'})-[:ACTED_IN]->(m:Movie) RETURN count(m)\", }, { \"question\": \"List all the genres of the movie Schindler's List\", \"query\": \"MATCH (m:Movie {title: 'Schindler's List'})-[:IN_GENRE]->(g:Genre) RETURN g.name\", }, { \"question\": \"Which actors have worked in movies from both the comedy and action genres?\", \"query\": \"MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g1:Genre), (a)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g2:Genre) WHERE g1.name = 'Comedy' AND g2.name = 'Action' RETURN DISTINCT a.name\", }, { \"question\": \"Which directors have made movies with at least three different actors named 'John'?\", \"query\": \"MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_IN]-(a:Person) WHERE a.name STARTS WITH 'John' WITH d, COUNT(DISTINCT a) AS JohnsCount WHERE JohnsCount >= 3 RETURN d.name\", }, { \"question\": \"Identify movies where directors also played a role in the film.\", \"query\": \"MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACTED_IN]->(m) RETURN m.title, p.name\", }, { \"question\": \"Find the actor with the highest number of movies in the database.\", \"query\": \"MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.name, COUNT(m) AS movieCount ORDER BY movieCount DESC LIMIT 1\", },]example_selector = SemanticSimilarityExampleSelector.from_examples( examples, OpenAIEmbeddings(), Neo4jVector, k=5, input_keys=[\"question\"])API Reference:SemanticSimilarityExampleSelector Next, we implement the Cypher generation chain, also known as text2cypher. The prompt includes an enhanced graph schema, dynamically selected few-shot examples, and the userâ€™s question. This combination enables the generation of a Cypher query to retrieve relevant information from the database. from langchain_core.output_parsers import StrOutputParsertext2cypher_prompt = ChatPromptTemplate.from_messages( [ ( \"system\", ( \"Given an input question, convert it to a Cypher query. No pre-amble.\" \"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\" ), ), ( \"human\", ( \"\"\"You are a Neo4j expert. Given an input question, create a syntactically correct Cypher query to run.Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!Here is the schema information{schema}Below are a number of examples of questions and their corresponding Cypher queries.{fewshot_examples}User input: {question}Cypher query:\"\"\" ), ), ])text2cypher_chain = text2cypher_prompt | llm | StrOutputParser()def generate_cypher(state: OverallState) -> OverallState: \"\"\" Generates a cypher statement based on the provided schema and user input \"\"\" NL = \"\\n\" fewshot_examples = (NL * 2).join( [ f\"Question: {el['question']}{NL}Cypher:{el['query']}\" for el in example_selector.select_examples( {\"question\": state.get(\"question\")} ) ] ) generated_cypher = text2cypher_chain.invoke( { \"question\": state.get(\"question\"), \"fewshot_examples\": fewshot_examples, \"schema\": enhanced_graph.schema, } ) return {\"cypher_statement\": generated_cypher, \"steps\": [\"generate_cypher\"]}API Reference:StrOutputParser Query validationâ€‹ The next step is to validate the generated Cypher statement and ensuring that all property values are accurate. While numbers and dates typically donâ€™t require validation, strings such as movie titles or peopleâ€™s names do. In this example, weâ€™ll use a basic CONTAINS clause for validation, though more advanced mapping and validation techniques can be implemented if needed. First, we will create a chain that detects any errors in the Cypher statement and extracts the property values it references. from typing import List, Optionalvalidate_cypher_system = \"\"\"You are a Cypher expert reviewing a statement written by a junior developer.\"\"\"validate_cypher_user = \"\"\"You must check the following:* Are there any syntax errors in the Cypher statement?* Are there any missing or undefined variables in the Cypher statement?* Are any node labels missing from the schema?* Are any relationship types missing from the schema?* Are any of the properties not included in the schema?* Does the Cypher statement include enough information to answer the question?Examples of good errors:* Label (:Foo) does not exist, did you mean (:Bar)?* Property bar does not exist for label Foo, did you mean baz?* Relationship FOO does not exist, did you mean FOO_BAR?Schema:{schema}The question is:{question}The Cypher statement is:{cypher}Make sure you don't make any mistakes!\"\"\"validate_cypher_prompt = ChatPromptTemplate.from_messages( [ ( \"system\", validate_cypher_system, ), ( \"human\", (validate_cypher_user), ), ])class Property(BaseModel): \"\"\" Represents a filter condition based on a specific node property in a graph in a Cypher statement. \"\"\" node_label: str = Field( description=\"The label of the node to which this property belongs.\" ) property_key: str = Field(description=\"The key of the property being filtered.\") property_value: str = Field( description=\"The value that the property is being matched against.\" )class ValidateCypherOutput(BaseModel): \"\"\" Represents the validation result of a Cypher query's output, including any errors and applied filters. \"\"\" errors: Optional[List[str]] = Field( description=\"A list of syntax or semantical errors in the Cypher statement. Always explain the discrepancy between schema and Cypher statement\" ) filters: Optional[List[Property]] = Field( description=\"A list of property-based filters applied in the Cypher statement.\" )validate_cypher_chain = validate_cypher_prompt | llm.with_structured_output( ValidateCypherOutput) LLMs often struggle with correctly determining relationship directions in generated Cypher statements. Since we have access to the schema, we can deterministically correct these directions using the CypherQueryCorrector. Note: The CypherQueryCorrector is an experimental feature and doesn't support all the newest Cypher syntax. from langchain_neo4j.chains.graph_qa.cypher_utils import CypherQueryCorrector, Schema# Cypher query corrector is experimentalcorrector_schema = [ Schema(el[\"start\"], el[\"type\"], el[\"end\"]) for el in enhanced_graph.structured_schema.get(\"relationships\")]cypher_query_corrector = CypherQueryCorrector(corrector_schema) Now we can implement the Cypher validation step. First, we use the EXPLAIN method to detect any syntax errors. Next, we leverage the LLM to identify potential issues and extract the properties used for filtering. For string properties, we validate them against the database using a simple CONTAINS clause. Based on the validation results, the process can take the following paths: If value mapping fails, we end the conversation and inform the user that we couldn't identify a specific property value (e.g., a person or movie title). If errors are found, we route the query for correction. If no issues are detected, we proceed to the Cypher execution step. from neo4j.exceptions import CypherSyntaxErrordef validate_cypher(state: OverallState) -> OverallState: \"\"\" Validates the Cypher statements and maps any property values to the database. \"\"\" errors = [] mapping_errors = [] # Check for syntax errors try: enhanced_graph.query(f\"EXPLAIN {state.get('cypher_statement')}\") except CypherSyntaxError as e: errors.append(e.message) # Experimental feature for correcting relationship directions corrected_cypher = cypher_query_corrector(state.get(\"cypher_statement\")) if not corrected_cypher: errors.append(\"The generated Cypher statement doesn't fit the graph schema\") if not corrected_cypher == state.get(\"cypher_statement\"): print(\"Relationship direction was corrected\") # Use LLM to find additional potential errors and get the mapping for values llm_output = validate_cypher_chain.invoke( { \"question\": state.get(\"question\"), \"schema\": enhanced_graph.schema, \"cypher\": state.get(\"cypher_statement\"), } ) if llm_output.errors: errors.extend(llm_output.errors) if llm_output.filters: for filter in llm_output.filters: # Do mapping only for string values if ( not [ prop for prop in enhanced_graph.structured_schema[\"node_props\"][ filter.node_label ] if prop[\"property\"] == filter.property_key ][0][\"type\"] == \"STRING\" ): continue mapping = enhanced_graph.query( f\"MATCH (n:{filter.node_label}) WHERE toLower(n.`{filter.property_key}`) = toLower($value) RETURN 'yes' LIMIT 1\", {\"value\": filter.property_value}, ) if not mapping: print( f\"Missing value mapping for {filter.node_label} on property {filter.property_key} with value {filter.property_value}\" ) mapping_errors.append( f\"Missing value mapping for {filter.node_label} on property {filter.property_key} with value {filter.property_value}\" ) if mapping_errors: next_action = \"end\" elif errors: next_action = \"correct_cypher\" else: next_action = \"execute_cypher\" return { \"next_action\": next_action, \"cypher_statement\": corrected_cypher, \"cypher_errors\": errors, \"steps\": [\"validate_cypher\"], } The Cypher correction step takes the existing Cypher statement, any identified errors, and the original question to generate a corrected version of the query. correct_cypher_prompt = ChatPromptTemplate.from_messages( [ ( \"system\", ( \"You are a Cypher expert reviewing a statement written by a junior developer. \" \"You need to correct the Cypher statement based on the provided errors. No pre-amble.\" \"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\" ), ), ( \"human\", ( \"\"\"Check for invalid syntax or semantics and return a corrected Cypher statement.Schema:{schema}Note: Do not include any explanations or apologies in your responses.Do not wrap the response in any backticks or anything else.Respond with a Cypher statement only!Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.The question is:{question}The Cypher statement is:{cypher}The errors are:{errors}Corrected Cypher statement: \"\"\" ), ), ])correct_cypher_chain = correct_cypher_prompt | llm | StrOutputParser()def correct_cypher(state: OverallState) -> OverallState: \"\"\" Correct the Cypher statement based on the provided errors. \"\"\" corrected_cypher = correct_cypher_chain.invoke( { \"question\": state.get(\"question\"), \"errors\": state.get(\"cypher_errors\"), \"cypher\": state.get(\"cypher_statement\"), \"schema\": enhanced_graph.schema, } ) return { \"next_action\": \"validate_cypher\", \"cypher_statement\": corrected_cypher, \"steps\": [\"correct_cypher\"], } We need to add a step that executes the given Cypher statement. If no results are returned, we should explicitly handle this scenario, as leaving the context empty can sometimes lead to LLM hallucinations. no_results = \"I couldn't find any relevant information in the database\"def execute_cypher(state: OverallState) -> OverallState: \"\"\" Executes the given Cypher statement. \"\"\" records = enhanced_graph.query(state.get(\"cypher_statement\")) return { \"database_records\": records if records else no_results, \"next_action\": \"end\", \"steps\": [\"execute_cypher\"], } The final step is to generate the answer. This involves combining the initial question with the database output to produce a relevant response. generate_final_prompt = ChatPromptTemplate.from_messages( [ ( \"system\", \"You are a helpful assistant\", ), ( \"human\", ( \"\"\"Use the following results retrieved from a database to providea succinct, definitive answer to the user's question.Respond as if you are answering the question directly.Results: {results}Question: {question}\"\"\" ), ), ])generate_final_chain = generate_final_prompt | llm | StrOutputParser()def generate_final_answer(state: OverallState) -> OutputState: \"\"\" Decides if the question is related to movies. \"\"\" final_answer = generate_final_chain.invoke( {\"question\": state.get(\"question\"), \"results\": state.get(\"database_records\")} ) return {\"answer\": final_answer, \"steps\": [\"generate_final_answer\"]} Next, we will implement the LangGraph workflow, starting with defining the conditional edge functions. def guardrails_condition( state: OverallState,) -> Literal[\"generate_cypher\", \"generate_final_answer\"]: if state.get(\"next_action\") == \"end\": return \"generate_final_answer\" elif state.get(\"next_action\") == \"movie\": return \"generate_cypher\"def validate_cypher_condition( state: OverallState,) -> Literal[\"generate_final_answer\", \"correct_cypher\", \"execute_cypher\"]: if state.get(\"next_action\") == \"end\": return \"generate_final_answer\" elif state.get(\"next_action\") == \"correct_cypher\": return \"correct_cypher\" elif state.get(\"next_action\") == \"execute_cypher\": return \"execute_cypher\" Let's put it all together now. from IPython.display import Image, displayfrom langgraph.graph import END, START, StateGraphlanggraph = StateGraph(OverallState, input=InputState, output=OutputState)langgraph.add_node(guardrails)langgraph.add_node(generate_cypher)langgraph.add_node(validate_cypher)langgraph.add_node(correct_cypher)langgraph.add_node(execute_cypher)langgraph.add_node(generate_final_answer)langgraph.add_edge(START, \"guardrails\")langgraph.add_conditional_edges( \"guardrails\", guardrails_condition,)langgraph.add_edge(\"generate_cypher\", \"validate_cypher\")langgraph.add_conditional_edges( \"validate_cypher\", validate_cypher_condition,)langgraph.add_edge(\"execute_cypher\", \"generate_final_answer\")langgraph.add_edge(\"correct_cypher\", \"validate_cypher\")langgraph.add_edge(\"generate_final_answer\", END)langgraph = langgraph.compile()# Viewdisplay(Image(langgraph.get_graph().draw_mermaid_png()))API Reference:StateGraph We can now test the application by asking an irrelevant question. langgraph.invoke({\"question\": \"What's the weather in Spain?\"}) {'answer': \"I'm sorry, but I cannot provide current weather information. Please check a reliable weather website or app for the latest updates on the weather in Spain.\", 'steps': ['guardrail', 'generate_final_answer']} Let's now ask something relevant about the movies. langgraph.invoke({\"question\": \"What was the cast of the Casino?\"}) {'answer': 'The cast of \"Casino\" includes Robert De Niro, Joe Pesci, Sharon Stone, and James Woods.', 'steps': ['guardrail', 'generate_cypher', 'validate_cypher', 'execute_cypher', 'generate_final_answer'], 'cypher_statement': \"MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a:Person) RETURN a.name\"} Next stepsâ€‹ For other graph techniques like this and more check out: Semantic layer: Techniques for implementing semantic layers. Constructing graphs: Techniques for constructing knowledge graphs. âš ï¸ Security note âš ï¸ArchitectureSetupGraph schemaGraphQACypherChainAdvanced implementation with LangGraphFew-shot promptingQuery validationNext steps",
      "timestamp": "2025-08-24 15:31:00"
    },
    {
      "url": "https://python.langchain.com/docs/integrations/chat/oci_generative_ai",
      "title": "ChatOCIGenAI | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "ChatOCIGenAI | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page This notebook provides a quick overview for getting started with OCIGenAI chat models. For detailed documentation of all ChatOCIGenAI features and configurations head to the API reference. Oracle Cloud Infrastructure (OCI) Generative AI is a fully managed service that provides a set of state-of-the-art, customizable large language models (LLMs) that cover a wide range of use cases, and which is available through a single API. Using the OCI Generative AI service you can access ready-to-use pretrained models, or create and host your own fine-tuned custom models based on your own data on dedicated AI clusters. Detailed documentation of the service and API is available here and here. Overviewâ€‹ Integration detailsâ€‹ ClassPackageLocalSerializableJS supportChatOCIGenAIlangchain-communityâŒâŒâŒ Model featuresâ€‹ Tool callingStructured outputJSON modeImage inputAudio inputVideo inputToken-level streamingNative asyncToken usageLogprobsâœ…âœ…âœ…âœ…âŒâŒâœ…âŒâŒâŒ Setupâ€‹ To access OCIGenAI models you'll need to install the oci and langchain-community packages. Credentialsâ€‹ The credentials and authentication methods supported for this integration are equivalent to those used with other OCI services and follow the standard SDK authentication methods, specifically API Key, session token, instance principal, and resource principal. API key is the default authentication method used in the examples above. The following example demonstrates how to use a different authentication method (session token) Installationâ€‹ The LangChain OCIGenAI integration lives in the langchain-oci package and you will also need to install the oci package: %pip install -qU langchain-oci Instantiationâ€‹ Now we can instantiate our model object and generate chat completions: from langchain_oci.chat_models import ChatOCIGenAIfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessagechat = ChatOCIGenAI( model_id=\"cohere.command-r-16k\", service_endpoint=\"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\", compartment_id=\"MY_OCID\", model_kwargs={\"temperature\": 0.7, \"max_tokens\": 500},)API Reference:AIMessage | HumanMessage | SystemMessage Invocationâ€‹ messages = [ SystemMessage(content=\"your are an AI assistant.\"), AIMessage(content=\"Hi there human!\"), HumanMessage(content=\"tell me a joke.\"),]response = chat.invoke(messages) print(response.content) Chainingâ€‹ We can chain our model with a prompt template like so: from langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")chain = prompt | chatresponse = chain.invoke({\"topic\": \"dogs\"})print(response.content)API Reference:ChatPromptTemplate API referenceâ€‹ For detailed documentation of all ChatOCIGenAI features and configurations head to the API reference: https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.oci_generative_ai.ChatOCIGenAI.html Relatedâ€‹ Chat model conceptual guide Chat model how-to guides OverviewIntegration detailsModel featuresSetupCredentialsInstallationInstantiationInvocationChainingAPI referenceRelated",
      "timestamp": "2025-08-24 15:31:04"
    },
    {
      "url": "https://python.langchain.com/docs/integrations/chat/maritalk",
      "title": "maritalk | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "maritalk | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page MariTalk is an assistant developed by the Brazilian company Maritaca AI. MariTalk is based on language models that have been specially trained to understand Portuguese well. This notebook demonstrates how to use MariTalk with LangChain through two examples: A simple example of how to use MariTalk to perform a task. LLM + RAG: The second example shows how to answer a question whose answer is found in a long document that does not fit within the token limit of MariTalk. For this, we will use a simple searcher (BM25) to first search the document for the most relevant sections and then feed them to MariTalk for answering. Installationâ€‹ First, install the LangChain library (and all its dependencies) using the following command: !pip install langchain langchain-core langchain-community httpx API Keyâ€‹ You will need an API key that can be obtained from chat.maritaca.ai (\"Chaves da API\" section). Example 1 - Pet Name Suggestionsâ€‹ Let's define our language model, ChatMaritalk, and configure it with your API key. from langchain_community.chat_models import ChatMaritalkfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts.chat import ChatPromptTemplatellm = ChatMaritalk( model=\"sabia-2-medium\", # Available models: sabia-2-small and sabia-2-medium api_key=\"\", # Insert your API key here temperature=0.7, max_tokens=100,)output_parser = StrOutputParser()chat_prompt = ChatPromptTemplate.from_messages( [ ( \"system\", \"You are an assistant specialized in suggesting pet names. Given the animal, you must suggest 4 names.\", ), (\"human\", \"I have a {animal}\"), ])chain = chat_prompt | llm | output_parserresponse = chain.invoke({\"animal\": \"dog\"})print(response) # should answer something like \"1. Max\\n2. Bella\\n3. Charlie\\n4. Rocky\"API Reference:StrOutputParser | ChatPromptTemplate Stream Generationâ€‹ For tasks involving the generation of long text, such as creating an extensive article or translating a large document, it can be advantageous to receive the response in parts, as the text is generated, instead of waiting for the complete text. This makes the application more responsive and efficient, especially when the generated text is extensive. We offer two approaches to meet this need: one synchronous and another asynchronous. Synchronous:â€‹ from langchain_core.messages import HumanMessagemessages = [HumanMessage(content=\"Suggest 3 names for my dog\")]for chunk in llm.stream(messages): print(chunk.content, end=\"\", flush=True)API Reference:HumanMessage Asynchronous:â€‹ from langchain_core.messages import HumanMessageasync def async_invoke_chain(animal: str): messages = [HumanMessage(content=f\"Suggest 3 names for my {animal}\")] async for chunk in llm._astream(messages): print(chunk.message.content, end=\"\", flush=True)await async_invoke_chain(\"dog\")API Reference:HumanMessage Example 2 - RAG + LLM: UNICAMP 2024 Entrance Exam Question Answering Systemâ€‹ For this example, we need to install some extra libraries: !pip install unstructured rank_bm25 pdf2image pdfminer-six pikepdf pypdf unstructured_inference fastapi kaleido uvicorn \"pillow<10.1.0\" pillow_heif -q Loading the databaseâ€‹ The first step is to create a database with the information from the notice. For this, we will download the notice from the COMVEST website and segment the extracted text into 500-character windows. from langchain_community.document_loaders import OnlinePDFLoaderfrom langchain_text_splitters import RecursiveCharacterTextSplitter# Loading the COMVEST 2024 noticeloader = OnlinePDFLoader( \"https://www.comvest.unicamp.br/wp-content/uploads/2023/10/31-2023-Dispoe-sobre-o-Vestibular-Unicamp-2024_com-retificacao.pdf\")data = loader.load()text_splitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=100, separators=[\"\\n\", \" \", \"\"])texts = text_splitter.split_documents(data) Creating a Searcherâ€‹ Now that we have our database, we need a searcher. For this example, we will use a simple BM25 as a search system, but this could be replaced by any other searcher (such as search via embeddings). from langchain_community.retrievers import BM25Retrieverretriever = BM25Retriever.from_documents(texts) Combining Search System + LLMâ€‹ Now that we have our searcher, we just need to implement a prompt specifying the task and invoke the chain. from langchain.chains.question_answering import load_qa_chainprompt = \"\"\"Baseado nos seguintes documentos, responda a pergunta abaixo.{context}Pergunta: {query}\"\"\"qa_prompt = ChatPromptTemplate.from_messages([(\"human\", prompt)])chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=True, prompt=qa_prompt)query = \"Qual o tempo mÃ¡ximo para realizaÃ§Ã£o da prova?\"docs = retriever.invoke(query)chain.invoke( {\"input_documents\": docs, \"query\": query}) # Should output something like: \"O tempo mÃ¡ximo para realizaÃ§Ã£o da prova Ã© de 5 horas.\" Relatedâ€‹ Chat model conceptual guide Chat model how-to guides InstallationAPI KeyExample 1 - Pet Name SuggestionsStream GenerationExample 2 - RAG + LLM: UNICAMP 2024 Entrance Exam Question Answering SystemRelated",
      "timestamp": "2025-08-24 15:31:08"
    },
    {
      "url": "https://python.langchain.com/docs/integrations/chat/qwq",
      "title": "ChatQwQ | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "ChatQwQ | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page This will help you get started with QwQ chat models. For detailed documentation of all ChatQwQ features and configurations head to the API reference. Overviewâ€‹ Integration detailsâ€‹ ClassPackageLocalSerializablePackage downloadsPackage latestChatQwQlangchain-qwqâŒbeta Model featuresâ€‹ Tool callingStructured outputJSON modeImage inputAudio inputVideo inputToken-level streamingNative asyncToken usageLogprobsâœ…âœ…âœ…âŒâŒâŒâœ…âœ…âœ…âŒ Setupâ€‹ To access QwQ models you'll need to create an Alibaba Cloud account, get an API key, and install the langchain-qwq integration package. Credentialsâ€‹ Head to Alibaba's API Key page to sign up to Alibaba Cloud and generate an API key. Once you've done this set the DASHSCOPE_API_KEY environment variable: import getpassimport osif not os.getenv(\"DASHSCOPE_API_KEY\"): os.environ[\"DASHSCOPE_API_KEY\"] = getpass.getpass(\"Enter your Dashscope API key: \") Installationâ€‹ The LangChain QwQ integration lives in the langchain-qwq package: %pip install -qU langchain-qwq Instantiationâ€‹ Now we can instantiate our model object and generate chat completions: from langchain_qwq import ChatQwQllm = ChatQwQ( model=\"qwq-plus\", max_tokens=3_000, timeout=None, max_retries=2, # other params...) Invocationâ€‹ messages = [ ( \"system\", \"You are a helpful assistant that translates English to French.\" \"Translate the user sentence.\", ), (\"human\", \"I love programming.\"),]ai_msg = llm.invoke(messages)ai_msg AIMessage(content=\"J'aime la programmation.\", additional_kwargs={'reasoning_content': 'Okay, the user wants me to translate \"I love programming.\" into French. Let\\'s start by breaking down the sentence. The subject is \"I\", which in French is \"Je\". The verb is \"love\", which in this context is present tense, so \"aime\". The object is \"programming\". Now, \"programming\" in French can be \"la programmation\". \\n\\nWait, should it be \"programmation\" or \"programmation\"? Let me confirm the spelling. Yes, \"programmation\" is correct. Now, putting it all together: \"Je aime la programmation.\" Hmm, but in French, there\\'s a tendency to contract \"je\" and \"aime\". Wait, actually, \"je\" followed by a vowel sound usually takes \"j\\'\". So it should be \"J\\'aime la programmation.\" \\n\\nLet me double-check. \"J\\'aime\" is the correct contraction for \"I love\". The definite article \"la\" is needed because \"programmation\" is a feminine noun. Yes, \"programmation\" is a feminine noun, so \"la\" is correct. \\n\\nIs there any other way to say it? Maybe \"J\\'adore la programmation\" for \"I love\" in a stronger sense, but the user didn\\'t specify the intensity. Since the original is straightforward, \"J\\'aime la programmation.\" is the direct translation. \\n\\nI think that\\'s it. No mistakes there. So the final translation should be \"J\\'aime la programmation.\"'}, response_metadata={'model_name': 'qwq-plus'}, id='run-5045cd6a-edbd-4b2f-bf24-b7bdf3777fb9-0', usage_metadata={'input_tokens': 32, 'output_tokens': 326, 'total_tokens': 358, 'input_token_details': {}, 'output_token_details': {}}) Chainingâ€‹ We can chain our model with a prompt template like so: from langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate( [ ( \"system\", \"You are a helpful assistant that translates\" \"{input_language} to {output_language}.\", ), (\"human\", \"{input}\"), ])chain = prompt | llmchain.invoke( { \"input_language\": \"English\", \"output_language\": \"German\", \"input\": \"I love programming.\", })API Reference:ChatPromptTemplate AIMessage(content='Ich liebe das Programmieren.', additional_kwargs={'reasoning_content': 'Okay, the user wants me to translate \"I love programming.\" into German. Let me think. The verb \"love\" is \"lieben\" or \"mÃ¶gen\" in German, but \"lieben\" is more like love, while \"mÃ¶gen\" is prefer. Since it\\'s about programming, which is a strong affection, \"lieben\" is better. The subject is \"I\", which is \"ich\". Then \"programming\" is \"Programmierung\" or \"Coding\". But \"Programmierung\" is more formal. Alternatively, sometimes people say \"ich liebe es zu programmieren\" which is \"I love to program\". Hmm, maybe the direct translation would be \"Ich liebe die Programmierung.\" But maybe the more natural way is \"Ich liebe es zu programmieren.\" Let me check. Both are correct, but the second one might sound more natural in everyday speech. The user might prefer the concise version. Alternatively, maybe \"Ich liebe die Programmierung.\" is better. Wait, the original is \"programming\" as a noun. So using the noun form would be appropriate. So \"Ich liebe die Programmierung.\" But sometimes people also use \"Coding\" in German, like \"Ich liebe das Coding.\" But that\\'s more anglicism. Probably better to stick with \"Programmierung\". Alternatively, \"Programmieren\" as a noun. Oh right! \"Programmieren\" can be a noun when used in the accusative case. So \"Ich liebe das Programmieren.\" That\\'s correct and natural. Yes, that\\'s the best translation. So the answer is \"Ich liebe das Programmieren.\"'}, response_metadata={'model_name': 'qwq-plus'}, id='run-2c418451-51d8-4319-8269-2ce129363a1a-0', usage_metadata={'input_tokens': 28, 'output_tokens': 341, 'total_tokens': 369, 'input_token_details': {}, 'output_token_details': {}}) Tool Callingâ€‹ ChatQwQ supports tool calling API that lets you describe tools and their arguments, and have the model return a JSON object with a tool to invoke and the inputs to that tool. Use with bind_toolsâ€‹ from langchain_core.tools import toolfrom langchain_qwq import ChatQwQ@tooldef multiply(first_int: int, second_int: int) -> int: \"\"\"Multiply two integers together.\"\"\" return first_int * second_intllm = ChatQwQ()llm_with_tools = llm.bind_tools([multiply])msg = llm_with_tools.invoke(\"What's 5 times forty two\")print(msg)API Reference:tool content='' additional_kwargs={'reasoning_content': 'Okay, the user is asking \"What\\'s 5 times forty two\". Let me break this down. First, I need to identify the numbers involved. The first number is 5, which is straightforward. The second number is forty two, which is 42 in digits. The operation they want is multiplication.\\n\\nLooking at the tools provided, there\\'s a function called multiply that takes two integers. So I should use that. The parameters are first_int and second_int. \\n\\nI need to convert \"forty two\" to 42. Since the function requires integers, both numbers should be in integer form. So 5 and 42. \\n\\nNow, I\\'ll structure the tool call. The function name is multiply, and the arguments should be first_int: 5 and second_int: 42. I\\'ll make sure the JSON is correctly formatted without any syntax errors. Let me double-check the parameters to ensure they\\'re required and of the right type. Yep, both are required and integers. \\n\\nNo examples were provided, but the function\\'s purpose is clear. So the correct tool call should be to multiply those two numbers. I think that\\'s all. No other functions are needed here.'} response_metadata={'model_name': 'qwq-plus'} id='run-638895aa-fdde-4567-bcfa-7d8e5d4f24af-0' tool_calls=[{'name': 'multiply', 'args': {'first_int': 5, 'second_int': 42}, 'id': 'call_d088275851c140529ed2ad', 'type': 'tool_call'}] usage_metadata={'input_tokens': 176, 'output_tokens': 277, 'total_tokens': 453, 'input_token_details': {}, 'output_token_details': {}} API referenceâ€‹ For detailed documentation of all ChatQwQ features and configurations head to the API reference Relatedâ€‹ Chat model conceptual guide Chat model how-to guides OverviewIntegration detailsModel featuresSetupCredentialsInstallationInstantiationInvocationChainingTool CallingUse with bind_toolsAPI referenceRelated",
      "timestamp": "2025-08-24 15:31:13"
    },
    {
      "url": "https://python.langchain.com/docs/integrations/chat/openai/",
      "title": "ChatOpenAI | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "ChatOpenAI | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page This notebook provides a quick overview for getting started with OpenAI chat models. For detailed documentation of all ChatOpenAI features and configurations head to the API reference. OpenAI has several chat models. You can find information about their latest models and their costs, context windows, and supported input types in the OpenAI docs. Azure OpenAINote that certain OpenAI models can also be accessed via the Microsoft Azure platform. To use the Azure OpenAI service use the AzureChatOpenAI integration. Overviewâ€‹ Integration detailsâ€‹ ClassPackageLocalSerializableJS supportPackage downloadsPackage latestChatOpenAIlangchain-openaiâŒbetaâœ… Model featuresâ€‹ Tool callingStructured outputJSON modeImage inputAudio inputVideo inputToken-level streamingNative asyncToken usageLogprobsâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ… Setupâ€‹ To access OpenAI models you'll need to create an OpenAI account, get an API key, and install the langchain-openai integration package. Credentialsâ€‹ Head to https://platform.openai.com to sign up to OpenAI and generate an API key. Once you've done this set the OPENAI_API_KEY environment variable: import getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"): os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \") If you want to get automated tracing of your model calls you can also set your LangSmith API key by uncommenting below: # os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")# os.environ[\"LANGSMITH_TRACING\"] = \"true\" Installationâ€‹ The LangChain OpenAI integration lives in the langchain-openai package: %pip install -qU langchain-openai Instantiationâ€‹ Now we can instantiate our model object and generate chat completions: from langchain_openai import ChatOpenAIllm = ChatOpenAI( model=\"gpt-4o\", temperature=0, max_tokens=None, timeout=None, max_retries=2, # api_key=\"...\", # if you prefer to pass api key in directly instaed of using env vars # base_url=\"...\", # organization=\"...\", # other params...) Invocationâ€‹ messages = [ ( \"system\", \"You are a helpful assistant that translates English to French. Translate the user sentence.\", ), (\"human\", \"I love programming.\"),]ai_msg = llm.invoke(messages)ai_msg AIMessage(content=\"J'adore la programmation.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 31, 'total_tokens': 36}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'stop', 'logprobs': None}, id='run-63219b22-03e3-4561-8cc4-78b7c7c3a3ca-0', usage_metadata={'input_tokens': 31, 'output_tokens': 5, 'total_tokens': 36}) print(ai_msg.content) J'adore la programmation. Chainingâ€‹ We can chain our model with a prompt template like so: from langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_messages( [ ( \"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\", ), (\"human\", \"{input}\"), ])chain = prompt | llmchain.invoke( { \"input_language\": \"English\", \"output_language\": \"German\", \"input\": \"I love programming.\", })API Reference:ChatPromptTemplate AIMessage(content='Ich liebe das Programmieren.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 26, 'total_tokens': 32}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'stop', 'logprobs': None}, id='run-350585e1-16ca-4dad-9460-3d9e7e49aaf1-0', usage_metadata={'input_tokens': 26, 'output_tokens': 6, 'total_tokens': 32}) Tool callingâ€‹ OpenAI has a tool calling (we use \"tool calling\" and \"function calling\" interchangeably here) API that lets you describe tools and their arguments, and have the model return a JSON object with a tool to invoke and the inputs to that tool. tool-calling is extremely useful for building tool-using chains and agents, and for getting structured outputs from models more generally. ChatOpenAI.bind_tools()â€‹ With ChatOpenAI.bind_tools, we can easily pass in Pydantic classes, dict schemas, LangChain tools, or even functions as tools to the model. Under the hood these are converted to an OpenAI tool schemas, which looks like: { \"name\": \"...\", \"description\": \"...\", \"parameters\": {...} # JSONSchema} and passed in every model invocation. from pydantic import BaseModel, Fieldclass GetWeather(BaseModel): \"\"\"Get the current weather in a given location\"\"\" location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")llm_with_tools = llm.bind_tools([GetWeather]) ai_msg = llm_with_tools.invoke( \"what is the weather like in San Francisco\",)ai_msg AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_o9udf3EVOWiV4Iupktpbpofk', 'function': {'arguments': '{\"location\":\"San Francisco, CA\"}', 'name': 'GetWeather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 68, 'total_tokens': 85}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1617c9b2-dda5-4120-996b-0333ed5992e2-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'call_o9udf3EVOWiV4Iupktpbpofk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 68, 'output_tokens': 17, 'total_tokens': 85}) strict=Trueâ€‹ Requires langchain-openai>=0.1.21 As of Aug 6, 2024, OpenAI supports a strict argument when calling tools that will enforce that the tool argument schema is respected by the model. See more here: https://platform.openai.com/docs/guides/function-calling Note: If strict=True the tool definition will also be validated, and a subset of JSON schema are accepted. Crucially, schema cannot have optional args (those with default values). Read the full docs on what types of schema are supported here: https://platform.openai.com/docs/guides/structured-outputs/supported-schemas. llm_with_tools = llm.bind_tools([GetWeather], strict=True)ai_msg = llm_with_tools.invoke( \"what is the weather like in San Francisco\",)ai_msg AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_jUqhd8wzAIzInTJl72Rla8ht', 'function': {'arguments': '{\"location\":\"San Francisco, CA\"}', 'name': 'GetWeather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 68, 'total_tokens': 85}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5e3356a9-132d-4623-8e73-dd5a898cf4a6-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'call_jUqhd8wzAIzInTJl72Rla8ht', 'type': 'tool_call'}], usage_metadata={'input_tokens': 68, 'output_tokens': 17, 'total_tokens': 85}) AIMessage.tool_callsâ€‹ Notice that the AIMessage has a tool_calls attribute. This contains in a standardized ToolCall format that is model-provider agnostic. ai_msg.tool_calls [{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'call_jUqhd8wzAIzInTJl72Rla8ht', 'type': 'tool_call'}] For more on binding tools and tool call outputs, head to the tool calling docs. Structured output and tool callsâ€‹ OpenAI's structured output feature can be used simultaneously with tool-calling. The model will either generate tool calls or a response adhering to a desired schema. See example below: from langchain_openai import ChatOpenAIfrom pydantic import BaseModeldef get_weather(location: str) -> None: \"\"\"Get weather at a location.\"\"\" return \"It's sunny.\"class OutputSchema(BaseModel): \"\"\"Schema for response.\"\"\" answer: str justification: strllm = ChatOpenAI(model=\"gpt-4.1\")structured_llm = llm.bind_tools( [get_weather], response_format=OutputSchema, strict=True,)# Response contains tool calls:tool_call_response = structured_llm.invoke(\"What is the weather in SF?\")# structured_response.additional_kwargs[\"parsed\"] contains parsed outputstructured_response = structured_llm.invoke( \"What weighs more, a pound of feathers or a pound of gold?\") Custom toolsâ€‹ Requires langchain-openai>=0.3.29 Custom tools support tools with arbitrary string inputs. They can be particularly useful when you expect your string arguments to be long or complex. from langchain_openai import ChatOpenAI, custom_toolfrom langgraph.prebuilt import create_react_agent@custom_tooldef execute_code(code: str) -> str: \"\"\"Execute python code.\"\"\" return \"27\"llm = ChatOpenAI(model=\"gpt-5\", output_version=\"responses/v1\")agent = create_react_agent(llm, [execute_code])input_message = {\"role\": \"user\", \"content\": \"Use the tool to calculate 3^3.\"}for step in agent.stream( {\"messages\": [input_message]}, stream_mode=\"values\",): step[\"messages\"][-1].pretty_print()API Reference:create_react_agent ================================\u001b[1m Human Message \u001b[0m=================================Use the tool to calculate 3^3.==================================\u001b[1m Ai Message \u001b[0m==================================[{'id': 'rs_6894ff5747c0819d9b02fc5645b0be9c000169fd9fb68d99', 'summary': [], 'type': 'reasoning'}, {'call_id': 'call_7SYwMSQPbbEqFcKlKOpXeEux', 'input': 'print(3**3)', 'name': 'execute_code', 'type': 'custom_tool_call', 'id': 'ctc_6894ff5b9f54819d8155a63638d34103000169fd9fb68d99', 'status': 'completed'}]Tool Calls: execute_code (call_7SYwMSQPbbEqFcKlKOpXeEux) Call ID: call_7SYwMSQPbbEqFcKlKOpXeEux Args: __arg1: print(3**3)=================================\u001b[1m Tool Message \u001b[0m=================================Name: execute_code[{'type': 'custom_tool_call_output', 'output': '27'}]==================================\u001b[1m Ai Message \u001b[0m==================================[{'type': 'text', 'text': '27', 'annotations': [], 'id': 'msg_6894ff5db3b8819d9159b3a370a25843000169fd9fb68d99'}] Context-free grammarsOpenAI supports the specification of a context-free grammar for custom tool inputs in lark or regex format. See OpenAI docs for details. The format parameter can be passed into @custom_tool as shown below:from langchain_openai import ChatOpenAI, custom_toolfrom langgraph.prebuilt import create_react_agentgrammar = \"\"\"start: exprexpr: term (SP ADD SP term)* -> add| termterm: factor (SP MUL SP factor)* -> mul| factorfactor: INTSP: \" \"ADD: \"+\"MUL: \"*\"%import common.INT\"\"\"format_ = {\"type\": \"grammar\", \"syntax\": \"lark\", \"definition\": grammar}@custom_tool(format=format_)def do_math(input_string: str) -> str: \"\"\"Do a mathematical operation.\"\"\" return \"27\"llm = ChatOpenAI(model=\"gpt-5\", output_version=\"responses/v1\")agent = create_react_agent(llm, [do_math])input_message = {\"role\": \"user\", \"content\": \"Use the tool to calculate 3^3.\"}for step in agent.stream( {\"messages\": [input_message]}, stream_mode=\"values\",): step[\"messages\"][-1].pretty_print()API Reference:create_react_agent================================\u001b[1m Human Message \u001b[0m=================================Use the tool to calculate 3^3.==================================\u001b[1m Ai Message \u001b[0m==================================[{'id': 'rs_689500828a8481a297ff0f98e328689c0681550c89797f43', 'summary': [], 'type': 'reasoning'}, {'call_id': 'call_jzH01RVhu6EFz7yUrOFXX55s', 'input': '3 * 3 * 3', 'name': 'do_math', 'type': 'custom_tool_call', 'id': 'ctc_6895008d57bc81a2b84d0993517a66b90681550c89797f43', 'status': 'completed'}]Tool Calls: do_math (call_jzH01RVhu6EFz7yUrOFXX55s) Call ID: call_jzH01RVhu6EFz7yUrOFXX55s Args: __arg1: 3 * 3 * 3=================================\u001b[1m Tool Message \u001b[0m=================================Name: do_math[{'type': 'custom_tool_call_output', 'output': '27'}]==================================\u001b[1m Ai Message \u001b[0m==================================[{'type': 'text', 'text': '27', 'annotations': [], 'id': 'msg_6895009776b881a2a25f0be8507d08f20681550c89797f43'}] Responses APIâ€‹ Requires langchain-openai>=0.3.9 OpenAI supports a Responses API that is oriented toward building agentic applications. It includes a suite of built-in tools, including web and file search. It also supports management of conversation state, allowing you to continue a conversational thread without explicitly passing in previous messages, as well as the output from reasoning processes. ChatOpenAI will route to the Responses API if one of these features is used. You can also specify use_responses_api=True when instantiating ChatOpenAI. notelangchain-openai >= 0.3.26 allows users to opt-in to an updated AIMessage format when using the Responses API. Settingllm = ChatOpenAI(model=\"...\", output_version=\"responses/v1\")will format output from reasoning summaries, built-in tool invocations, and other response items into the message's content field, rather than additional_kwargs. We recommend this format for new applications. Web searchâ€‹ To trigger a web search, pass {\"type\": \"web_search_preview\"} to the model as you would another tool. tipYou can also pass built-in tools as invocation params:llm.invoke(\"...\", tools=[{\"type\": \"web_search_preview\"}]) from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4.1-mini\", output_version=\"responses/v1\")tool = {\"type\": \"web_search_preview\"}llm_with_tools = llm.bind_tools([tool])response = llm_with_tools.invoke(\"What was a positive news story from today?\") Note that the response includes structured content blocks that include both the text of the response and OpenAI annotations citing its sources. The output message will also contain information from any tool invocations: response.content [{'id': 'ws_685d997c1838819e8a2cbf66059ddd5c0f6f330a19127ac1', 'action': {'query': 'positive news stories today', 'type': 'search'}, 'status': 'completed', 'type': 'web_search_call'}, {'type': 'text', 'text': \"On June 25, 2025, the James Webb Space Telescope made a groundbreaking discovery by directly imaging a previously unknown exoplanet. This young gas giant, approximately the size of Saturn, orbits a star smaller than our Sun, located about 110 light-years away in the constellation Antlia. This achievement marks the first time Webb has identified an exoplanet not previously known, expanding our understanding of distant worlds. ([straitstimes.com](https://www.straitstimes.com/world/while-you-were-sleeping-5-stories-you-might-have-missed-june-26-2025?utm_source=openai))\\n\\nAdditionally, in the realm of conservation, a significant milestone was achieved with the successful translocation of seventy southern white rhinos from South Africa to Rwanda's Akagera National Park. This initiative represents the first international translocation from Platinum Rhino, a major captive breeding operation, and is seen as a substantial opportunity to safeguard the future of the white rhino species. ([conservationoptimism.org](https://conservationoptimism.org/7-stories-of-optimism-this-week-17-06-25-23-06-25/?utm_source=openai))\\n\\nThese developments highlight positive strides in both scientific exploration and wildlife conservation efforts. \", 'annotations': [{'end_index': 572, 'start_index': 429, 'title': 'While You Were Sleeping: 5 stories you might have missed, June 26, 2025 | The Straits Times', 'type': 'url_citation', 'url': 'https://www.straitstimes.com/world/while-you-were-sleeping-5-stories-you-might-have-missed-june-26-2025?utm_source=openai'}, {'end_index': 1121, 'start_index': 990, 'title': '7 stories of optimism this week (17.06.25-23.06.25) - Conservation Optimism', 'type': 'url_citation', 'url': 'https://conservationoptimism.org/7-stories-of-optimism-this-week-17-06-25-23-06-25/?utm_source=openai'}], 'id': 'msg_685d997f6b94819e8d981a2b441470420f6f330a19127ac1'}] tipYou can recover just the text content of the response as a string by using response.text(). For example, to stream response text:for token in llm_with_tools.stream(\"...\"): print(token.text(), end=\"|\")See the streaming guide for more detail. Image generationâ€‹ Requires langchain-openai>=0.3.19 To trigger an image generation, pass {\"type\": \"image_generation\"} to the model as you would another tool. tipYou can also pass built-in tools as invocation params:llm.invoke(\"...\", tools=[{\"type\": \"image_generation\"}]) from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4.1-mini\", output_version=\"responses/v1\")tool = {\"type\": \"image_generation\", \"quality\": \"low\"}llm_with_tools = llm.bind_tools([tool])ai_message = llm_with_tools.invoke( \"Draw a picture of a cute fuzzy cat with an umbrella\") import base64from IPython.display import Imageimage = next( item for item in ai_message.content if item[\"type\"] == \"image_generation_call\")Image(base64.b64decode(image[\"result\"]), width=200) File searchâ€‹ To trigger a file search, pass a file search tool to the model as you would another tool. You will need to populate an OpenAI-managed vector store and include the vector store ID in the tool definition. See OpenAI documentation for more detail. from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4.1-mini\", output_version=\"responses/v1\")openai_vector_store_ids = [ \"vs_...\", # your IDs here]tool = { \"type\": \"file_search\", \"vector_store_ids\": openai_vector_store_ids,}llm_with_tools = llm.bind_tools([tool])response = llm_with_tools.invoke(\"What is deep research by OpenAI?\")print(response.text()) Deep Research by OpenAI is a newly launched agentic capability within ChatGPT designed to conduct complex, multi-step research tasks on the internet autonomously. It synthesizes large amounts of online information into comprehensive, research analyst-level reports, accomplishing in tens of minutes what would typically take a human many hours. This capability is powered by an upcoming OpenAI o3 model that is optimized for web browsing and data analysis, allowing it to search, interpret, and analyze massive amounts of text, images, and PDFs from the internet, while dynamically adjusting its approach based on the information it finds.Key features of Deep Research include:- Independent discovery, reasoning, and consolidation of insights from across the web.- Ability to use browser and Python programming tools for data analysis and graph plotting.- Full documentation of outputs with clear citations and a summary of its reasoning process, making it easy to verify and reference.- Designed to provide thorough, precise, and reliable research especially useful for knowledge-intensive domains such as finance, science, policy, and engineering. It is also valuable for individuals seeking personalized and detailed product research.It uses reinforcement learning techniques to plan and execute multi-step information-gathering tasks, reacting to real-time information by backtracking or pivoting its search when necessary. Deep Research can browse the open web and user-uploaded files, integrates visual data such as images and graphs into its reports, and cites specific source passages to support its conclusions.The goal behind Deep Research is to enhance knowledge synthesis, which is essential for creating new knowledge, marking a significant step toward the development of Artificial General Intelligence (AGI) capable of producing novel scientific research.Users can access Deep Research via ChatGPT by selecting the \"deep research\" option in the message composer, entering their query, and optionally attaching files or spreadsheets. The research process can take from 5 to 30 minutes, during which users can continue with other tasks. The final output is delivered as a richly detailed and well-documented report within the chat interface.Currently, Deep Research is available to Pro users with plans to expand access further to Plus, Team, and Enterprise users. It currently supports research using open web sources and uploaded files, with future plans to connect to specialized subscription or internal data sources for even more robust research outputs.Though powerful, Deep Research has limitations such as occasional hallucinations, difficulty distinguishing authoritative information from rumors, and some formatting or citation issues at launch, which are expected to improve with usage and time.In summary, Deep Research is a highly advanced AI research assistant capable of automating extensive, in-depth knowledge work by synthesizing vast amounts of online data into comprehensive, credible reports, designed to save users significant time and effort on complex research tasks. As with web search, the response will include content blocks with citations: [block[\"type\"] for block in response.content] ['file_search_call', 'text'] text_block = next(block for block in response.content if block[\"type\"] == \"text\")text_block[\"annotations\"][:2] [{'file_id': 'file-3UzgX7jcC8Dt9ZAFzywg5k', 'filename': 'deep_research_blog.pdf', 'index': 3121, 'type': 'file_citation'}, {'file_id': 'file-3UzgX7jcC8Dt9ZAFzywg5k', 'filename': 'deep_research_blog.pdf', 'index': 3121, 'type': 'file_citation'}] It will also include information from the built-in tool invocations: response.content[0] {'id': 'fs_685d9e7d48408191b9e34ad359069ede019138cfaaf3cea8', 'queries': ['deep research by OpenAI'], 'status': 'completed', 'type': 'file_search_call'} Computer useâ€‹ ChatOpenAI supports the \"computer-use-preview\" model, which is a specialized model for the built-in computer use tool. To enable, pass a computer use tool as you would pass another tool. Currently, tool outputs for computer use are present in the message content field. To reply to the computer use tool call, construct a ToolMessage with {\"type\": \"computer_call_output\"} in its additional_kwargs. The content of the message will be a screenshot. Below, we demonstrate a simple example. First, load two screenshots: import base64def load_png_as_base64(file_path): with open(file_path, \"rb\") as image_file: encoded_string = base64.b64encode(image_file.read()) return encoded_string.decode(\"utf-8\")screenshot_1_base64 = load_png_as_base64( \"/path/to/screenshot_1.png\") # perhaps a screenshot of an applicationscreenshot_2_base64 = load_png_as_base64( \"/path/to/screenshot_2.png\") # perhaps a screenshot of the Desktop from langchain_openai import ChatOpenAI# Initialize modelllm = ChatOpenAI( model=\"computer-use-preview\", truncation=\"auto\", output_version=\"responses/v1\",)# Bind computer-use tooltool = { \"type\": \"computer_use_preview\", \"display_width\": 1024, \"display_height\": 768, \"environment\": \"browser\",}llm_with_tools = llm.bind_tools([tool])# Construct input messageinput_message = { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": ( \"Click the red X to close and reveal my Desktop. \" \"Proceed, no confirmation needed.\" ), }, { \"type\": \"input_image\", \"image_url\": f\"data:image/png;base64,{screenshot_1_base64}\", }, ],}# Invoke modelresponse = llm_with_tools.invoke( [input_message], reasoning={ \"generate_summary\": \"concise\", },) The response will include a call to the computer-use tool in its content: response.content [{'id': 'rs_685da051742c81a1bb35ce46a9f3f53406b50b8696b0f590', 'summary': [{'text': \"Clicking red 'X' to show desktop\", 'type': 'summary_text'}], 'type': 'reasoning'}, {'id': 'cu_685da054302481a1b2cc43b56e0b381706b50b8696b0f590', 'action': {'button': 'left', 'type': 'click', 'x': 14, 'y': 38}, 'call_id': 'call_zmQerFBh4PbBE8mQoQHkfkwy', 'pending_safety_checks': [], 'status': 'completed', 'type': 'computer_call'}] We next construct a ToolMessage with these properties: It has a tool_call_id matching the call_id from the computer-call. It has {\"type\": \"computer_call_output\"} in its additional_kwargs. Its content is either an image_url or an input_image output block (see OpenAI docs for formatting). from langchain_core.messages import ToolMessagetool_call_id = next( item[\"call_id\"] for item in response.content if item[\"type\"] == \"computer_call\")tool_message = ToolMessage( content=[ { \"type\": \"input_image\", \"image_url\": f\"data:image/png;base64,{screenshot_2_base64}\", } ], # content=f\"data:image/png;base64,{screenshot_2_base64}\", # <-- also acceptable tool_call_id=tool_call_id, additional_kwargs={\"type\": \"computer_call_output\"},)API Reference:ToolMessage We can now invoke the model again using the message history: messages = [ input_message, response, tool_message,]response_2 = llm_with_tools.invoke( messages, reasoning={ \"generate_summary\": \"concise\", },) response_2.text() 'VS Code has been closed, and the desktop is now visible.' Instead of passing back the entire sequence, we can also use the previous_response_id: previous_response_id = response.response_metadata[\"id\"]response_2 = llm_with_tools.invoke( [tool_message], previous_response_id=previous_response_id, reasoning={ \"generate_summary\": \"concise\", },) response_2.text() 'The VS Code window is closed, and the desktop is now visible. Let me know if you need any further assistance.' Code interpreterâ€‹ OpenAI implements a code interpreter tool to support the sandboxed generation and execution of code. Example use: from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"o4-mini\", output_version=\"responses/v1\")llm_with_tools = llm.bind_tools( [ { \"type\": \"code_interpreter\", # Create a new container \"container\": {\"type\": \"auto\"}, } ])response = llm_with_tools.invoke( \"Write and run code to answer the question: what is 3^3?\") Note that the above command created a new container. We can also specify an existing container ID: code_interpreter_calls = [ item for item in response.content if item[\"type\"] == \"code_interpreter_call\"]assert len(code_interpreter_calls) == 1container_id = code_interpreter_calls[0][\"container_id\"]llm_with_tools = llm.bind_tools( [ { \"type\": \"code_interpreter\", # Use an existing container \"container\": container_id, } ]) Remote MCPâ€‹ OpenAI implements a remote MCP tool that allows for model-generated calls to MCP servers. Example use: from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"o4-mini\", output_version=\"responses/v1\")llm_with_tools = llm.bind_tools( [ { \"type\": \"mcp\", \"server_label\": \"deepwiki\", \"server_url\": \"https://mcp.deepwiki.com/mcp\", \"require_approval\": \"never\", } ])response = llm_with_tools.invoke( \"What transport protocols does the 2025-03-26 version of the MCP \" \"spec (modelcontextprotocol/modelcontextprotocol) support?\") MCP ApprovalsOpenAI will at times request approval before sharing data with a remote MCP server.In the above command, we instructed the model to never require approval. We can also configure the model to always request approval, or to always request approval for specific tools:llm_with_tools = llm.bind_tools( [ { \"type\": \"mcp\", \"server_label\": \"deepwiki\", \"server_url\": \"https://mcp.deepwiki.com/mcp\", \"require_approval\": { \"always\": { \"tool_names\": [\"read_wiki_structure\"] } } } ])response = llm_with_tools.invoke( \"What transport protocols does the 2025-03-26 version of the MCP \" \"spec (modelcontextprotocol/modelcontextprotocol) support?\")Responses may then include blocks with type \"mcp_approval_request\".To submit approvals for an approval request, structure it into a content block in an input message:approval_message = { \"role\": \"user\", \"content\": [ { \"type\": \"mcp_approval_response\", \"approve\": True, \"approval_request_id\": block[\"id\"], } for block in response.content if block[\"type\"] == \"mcp_approval_request\" ]}next_response = llm_with_tools.invoke( [approval_message], # continue existing thread previous_response_id=response.response_metadata[\"id\"]) Managing conversation stateâ€‹ The Responses API supports management of conversation state. Manually manage stateâ€‹ You can manage the state manually or using LangGraph, as with other chat models: from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4.1-mini\", output_version=\"responses/v1\")tool = {\"type\": \"web_search_preview\"}llm_with_tools = llm.bind_tools([tool])first_query = \"What was a positive news story from today?\"messages = [{\"role\": \"user\", \"content\": first_query}]response = llm_with_tools.invoke(messages)response_text = response.text()print(f\"{response_text[:100]}... {response_text[-100:]}\") On June 25, 2025, the James Webb Space Telescope made a groundbreaking discovery by directly imaging... exploration and environmental conservation, reflecting positive developments in science and nature. second_query = ( \"Repeat my question back to me, as well as the last sentence of your answer.\")messages.extend( [ response, {\"role\": \"user\", \"content\": second_query}, ])second_response = llm_with_tools.invoke(messages)print(second_response.text()) Your question was: \"What was a positive news story from today?\"The last sentence of my answer was: \"These stories highlight significant advancements in both space exploration and environmental conservation, reflecting positive developments in science and nature.\" tipYou can use LangGraph to manage conversational threads for you in a variety of backends, including in-memory and Postgres. See this tutorial to get started. Passing previous_response_idâ€‹ When using the Responses API, LangChain messages will include an \"id\" field in its metadata. Passing this ID to subsequent invocations will continue the conversation. Note that this is equivalent to manually passing in messages from a billing perspective. from langchain_openai import ChatOpenAIllm = ChatOpenAI( model=\"gpt-4.1-mini\", output_version=\"responses/v1\",)response = llm.invoke(\"Hi, I'm Bob.\")print(response.text()) Hi Bob! How can I assist you today? second_response = llm.invoke( \"What is my name?\", previous_response_id=response.response_metadata[\"id\"],)print(second_response.text()) You mentioned that your name is Bob. How can I help you today, Bob? ChatOpenAI can also automatically specify previous_response_id using the last response in a message sequence: from langchain_openai import ChatOpenAIllm = ChatOpenAI( model=\"gpt-4.1-mini\", output_version=\"responses/v1\", use_previous_response_id=True,) If we set use_previous_response_id=True, input messages up to the most recent response will be dropped from request payloads, and previous_response_id will be set using the ID of the most recent response. That is, llm.invoke( [ HumanMessage(\"Hello\"), AIMessage(\"Hi there!\", response_metadata={\"id\": \"resp_123\"}), HumanMessage(\"How are you?\"), ]) is equivalent to: llm.invoke([HumanMessage(\"How are you?\")], previous_response_id=\"resp_123\") Reasoning outputâ€‹ Some OpenAI models will generate separate text content illustrating their reasoning process. See OpenAI's reasoning documentation for details. OpenAI can return a summary of the model's reasoning (although it doesn't expose the raw reasoning tokens). To configure ChatOpenAI to return this summary, specify the reasoning parameter. ChatOpenAI will automatically route to the Responses API if this parameter is set. from langchain_openai import ChatOpenAIreasoning = { \"effort\": \"medium\", # 'low', 'medium', or 'high' \"summary\": \"auto\", # 'detailed', 'auto', or None}llm = ChatOpenAI(model=\"o4-mini\", reasoning=reasoning, output_version=\"responses/v1\")response = llm.invoke(\"What is 3^3?\")# Outputresponse.text() '3Â³ = 3 Ã— 3 Ã— 3 = 27.' # Reasoningfor block in response.content: if block[\"type\"] == \"reasoning\": for summary in block[\"summary\"]: print(summary[\"text\"]) **Calculating the power of three**The user is asking about 3 raised to the power of 3. That's a pretty simple calculation! I know that 3^3 equals 27, so I can say, \"3 to the power of 3 equals 27.\" I might also include a quick explanation that it's 3 multiplied by itself three times: 3 Ã— 3 Ã— 3 = 27. So, the answer is definitely 27. Fine-tuningâ€‹ You can call fine-tuned OpenAI models by passing in your corresponding modelName parameter. This generally takes the form of ft:{OPENAI_MODEL_NAME}:{ORG_NAME}::{MODEL_ID}. For example: fine_tuned_model = ChatOpenAI( temperature=0, model_name=\"ft:gpt-3.5-turbo-0613:langchain::7qTVM5AR\")fine_tuned_model.invoke(messages) AIMessage(content=\"J'adore la programmation.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 31, 'total_tokens': 39}, 'model_name': 'ft:gpt-3.5-turbo-0613:langchain::7qTVM5AR', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0f39b30e-c56e-4f3b-af99-5c948c984146-0', usage_metadata={'input_tokens': 31, 'output_tokens': 8, 'total_tokens': 39}) Multimodal Inputs (images, PDFs, audio)â€‹ OpenAI has models that support multimodal inputs. You can pass in images, PDFs, or audio to these models. For more information on how to do this in LangChain, head to the multimodal inputs docs. You can see the list of models that support different modalities in OpenAI's documentation. For all modalities, LangChain supports both its cross-provider standard as well as OpenAI's native content-block format. To pass multimodal data into ChatOpenAI, create a content block containing the data and incorporate it into a message, e.g., as below: message = { \"role\": \"user\", \"content\": [ { \"type\": \"text\", # Update prompt as desired \"text\": \"Describe the (image / PDF / audio...)\", }, content_block, ],} See below for examples of content blocks. ImagesRefer to examples in the how-to guide here.URLs:# LangChain formatcontent_block = { \"type\": \"image\", \"source_type\": \"url\", \"url\": url_string,}# OpenAI Chat Completions formatcontent_block = { \"type\": \"image_url\", \"image_url\": {\"url\": url_string},}In-line base64 data:# LangChain formatcontent_block = { \"type\": \"image\", \"source_type\": \"base64\", \"data\": base64_string, \"mime_type\": \"image/jpeg\",}# OpenAI Chat Completions formatcontent_block = { \"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/jpeg;base64,{base64_string}\", },} PDFsNote: OpenAI requires file-names be specified for PDF inputs. When using LangChain's format, include the filename key.Read more here.Refer to examples in the how-to guide here.In-line base64 data:# LangChain formatcontent_block = { \"type\": \"file\", \"source_type\": \"base64\", \"data\": base64_string, \"mime_type\": \"application/pdf\", \"filename\": \"my-file.pdf\",}# OpenAI Chat Completions formatcontent_block = { \"type\": \"file\", \"file\": { \"filename\": \"my-file.pdf\", \"file_data\": f\"data:application/pdf;base64,{base64_string}\", }} AudioSee supported models, e.g., \"gpt-4o-audio-preview\".Refer to examples in the how-to guide here.In-line base64 data:# LangChain formatcontent_block = { \"type\": \"audio\", \"source_type\": \"base64\", \"mime_type\": \"audio/wav\", # or appropriate mime-type \"data\": base64_string,}# OpenAI Chat Completions formatcontent_block = { \"type\": \"input_audio\", \"input_audio\": {\"data\": base64_string, \"format\": \"wav\"},} Predicted outputâ€‹ infoRequires langchain-openai>=0.2.6 Some OpenAI models (such as their gpt-4o and gpt-4o-mini series) support Predicted Outputs, which allow you to pass in a known portion of the LLM's expected output ahead of time to reduce latency. This is useful for cases such as editing text or code, where only a small part of the model's output will change. Here's an example: code = \"\"\"/// <summary>/// Represents a user with a first name, last name, and username./// </summary>public class User{ /// <summary> /// Gets or sets the user's first name. /// </summary> public string FirstName { get; set; } /// <summary> /// Gets or sets the user's last name. /// </summary> public string LastName { get; set; } /// <summary> /// Gets or sets the user's username. /// </summary> public string Username { get; set; }}\"\"\"llm = ChatOpenAI(model=\"gpt-4o\")query = ( \"Replace the Username property with an Email property. \" \"Respond only with code, and with no markdown formatting.\")response = llm.invoke( [{\"role\": \"user\", \"content\": query}, {\"role\": \"user\", \"content\": code}], prediction={\"type\": \"content\", \"content\": code},)print(response.content)print(response.response_metadata) /// <summary>/// Represents a user with a first name, last name, and email./// </summary>public class User{ /// <summary> /// Gets or sets the user's first name. /// </summary> public string FirstName { get; set; } /// <summary> /// Gets or sets the user's last name. /// </summary> public string LastName { get; set; } /// <summary> /// Gets or sets the user's email. /// </summary> public string Email { get; set; }}{'token_usage': {'completion_tokens': 226, 'prompt_tokens': 166, 'total_tokens': 392, 'completion_tokens_details': {'accepted_prediction_tokens': 49, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 107}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_45cf54deae', 'finish_reason': 'stop', 'logprobs': None} Note that currently predictions are billed as additional tokens and may increase your usage and costs in exchange for this reduced latency. Audio Generation (Preview)â€‹ infoRequires langchain-openai>=0.2.3 OpenAI has a new audio generation feature that allows you to use audio inputs and outputs with the gpt-4o-audio-preview model. from langchain_openai import ChatOpenAIllm = ChatOpenAI( model=\"gpt-4o-audio-preview\", temperature=0, model_kwargs={ \"modalities\": [\"text\", \"audio\"], \"audio\": {\"voice\": \"alloy\", \"format\": \"wav\"}, },)output_message = llm.invoke( [ (\"human\", \"Are you made by OpenAI? Just answer yes or no\"), ]) output_message.additional_kwargs['audio'] will contain a dictionary like { 'data': '<audio data b64-encoded', 'expires_at': 1729268602, 'id': 'audio_67127d6a44348190af62c1530ef0955a', 'transcript': 'Yes.'} and the format will be what was passed in model_kwargs['audio']['format']. We can also pass this message with audio data back to the model as part of a message history before openai expires_at is reached. noteOutput audio is stored under the audio key in AIMessage.additional_kwargs, but input content blocks are typed with an input_audio type and key in HumanMessage.content lists.For more information, see OpenAI's audio docs. history = [ (\"human\", \"Are you made by OpenAI? Just answer yes or no\"), output_message, (\"human\", \"And what is your name? Just give your name.\"),]second_output_message = llm.invoke(history) Flex processingâ€‹ OpenAI offers a variety of service tiers. The \"flex\" tier offers cheaper pricing for requests, with the trade-off that responses may take longer and resources might not always be available. This approach is best suited for non-critical tasks, including model testing, data enhancement, or jobs that can be run asynchronously. To use it, initialize the model with service_tier=\"flex\": llm = ChatOpenAI(model=\"o4-mini\", service_tier=\"flex\") Note that this is a beta feature that is only available for a subset of models. See OpenAI docs for more detail. API referenceâ€‹ For detailed documentation of all ChatOpenAI features and configurations head to the API reference. Relatedâ€‹ Chat model conceptual guide Chat model how-to guides OverviewIntegration detailsModel featuresSetupCredentialsInstallationInstantiationInvocationChainingTool callingChatOpenAI.bind_tools()strict=TrueAIMessage.tool_callsStructured output and tool callsCustom toolsResponses APIWeb searchImage generationFile searchComputer useCode interpreterRemote MCPManaging conversation stateReasoning outputFine-tuningMultimodal Inputs (images, PDFs, audio)Predicted outputAudio Generation (Preview)Flex processingAPI referenceRelated",
      "timestamp": "2025-08-24 15:31:24"
    },
    {
      "url": "https://python.langchain.com/api_reference/fireworks/chat_models/langchain_fireworks.chat_models.ChatFireworks.html",
      "title": "ChatFireworks â€” ğŸ¦œğŸ”— LangChain  documentation",
      "content": "ChatFireworks â€” ğŸ¦œğŸ”— LangChain documentation Skip to main content Back to top Ctrl+K Docs Light Dark System Settings GitHub X / Twitter ChatFireworks# class langchain_fireworks.chat_models.ChatFireworks[source]# Bases: BaseChatModel Fireworks Chat large language models API. To use, you should have the environment variable FIREWORKS_API_KEY set with your API key. Any parameters that are valid to be passed to the fireworks.create call can be passed in, even if not explicitly saved on this class. Example from langchain_fireworks.chat_models import ChatFireworks fireworks = ChatFireworks( model_name=\"accounts/fireworks/models/llama-v3p1-8b-instruct\") Copy to clipboard Note ChatFireworks implements the standard Runnable Interface. ğŸƒ The Runnable Interface has additional methods that are available on runnables, such as with_config, with_types, with_retry, assign, bind, get_graph, and more. param cache: BaseCache | bool | None = None# Whether to cache the response. If true, will use the global cache. If false, will not use a cache If None, will use the global cache if itâ€™s set, otherwise no cache. If instance of BaseCache, will use the provided cache. Caching is not currently supported for streaming methods of models. param callback_manager: BaseCallbackManager | None = None# Deprecated since version 0.1.7: Use callbacks() instead. It will be removed in pydantic==1.0. Callback manager to add to the run trace. param callbacks: Callbacks = None# Callbacks to add to the run trace. param custom_get_token_ids: Callable[[str], list[int]] | None = None# Optional encoder to use for counting tokens. param disable_streaming: bool | Literal['tool_calling'] = False# Whether to disable streaming for this model. If streaming is bypassed, then stream()/astream()/astream_events() will defer to invoke()/ainvoke(). If True, will always bypass streaming case. If 'tool_calling', will bypass streaming case only when the model is called with a tools keyword argument. In other words, LangChain will automatically switch to non-streaming behavior (invoke()) only when the tools argument is provided. This offers the best of both worlds. If False (default), will always use streaming case if available. The main reason for this flag is that code might be written using stream() and a user may want to swap out a given model for another model whose the implementation does not properly support streaming. param fireworks_api_base: str | None [Optional] (alias 'base_url')# Base URL path for API requests, leave blank if not using a proxy or service emulator. param fireworks_api_key: SecretStr [Optional] (alias 'api_key')# Fireworks API key. Automatically read from env variable FIREWORKS_API_KEY if not provided. param max_retries: int | None = None# Maximum number of retries to make when generating. param max_tokens: int | None = None# Maximum number of tokens to generate. param metadata: dict[str, Any] | None = None# Metadata to add to the run trace. param model_kwargs: dict[str, Any] [Optional]# Holds any model parameters valid for create call not explicitly specified. param model_name: str [Required] (alias 'model')# Model name to use. param n: int = 1# Number of chat completions to generate for each prompt. param rate_limiter: BaseRateLimiter | None = None# An optional rate limiter to use for limiting the number of requests. param request_timeout: float | tuple[float, float] | Any | None = None (alias 'timeout')# Timeout for requests to Fireworks completion API. Can be float, httpx.Timeout or None. param stop: str | list[str] | None = None (alias 'stop_sequences')# Default stop sequences. param streaming: bool = False# Whether to stream the results or not. param tags: list[str] | None = None# Tags to add to the run trace. param temperature: float | None = None# What sampling temperature to use. param verbose: bool [Optional]# Whether to print out response text. __call__( messages: list[BaseMessage], stop: list[str] | None = None, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, **kwargs: Any, ) â†’ BaseMessage# Deprecated since version 0.1.7: Use invoke() instead. It will not be removed until langchain-core==1.0. Call the model. Parameters: messages (list[BaseMessage]) â€“ List of messages. stop (list[str] | None) â€“ Stop words to use when generating. Model output is cut off at the first occurrence of any of these substrings. callbacks (list[BaseCallbackHandler] | BaseCallbackManager | None) â€“ Callbacks to pass through. Used for executing additional functionality, such as logging or streaming, throughout generation. **kwargs (Any) â€“ Arbitrary additional keyword arguments. These are usually passed to the model provider API call. Returns: The model output message. Return type: BaseMessage async abatch( inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None, ) â†’ list[Output]# Default implementation runs ainvoke in parallel using asyncio.gather. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying Runnable uses an API which supports a batch mode. Parameters: inputs (list[Input]) â€“ A list of inputs to the Runnable. config (RunnableConfig | list[RunnableConfig] | None) â€“ A config to use when invoking the Runnable. The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys. Please refer to the RunnableConfig for more details. Defaults to None. return_exceptions (bool) â€“ Whether to return exceptions instead of raising them. Defaults to False. kwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable. Returns: A list of outputs from the Runnable. Return type: list[Output] async abatch_as_completed( inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None, ) â†’ AsyncIterator[tuple[int, Output | Exception]]# Run ainvoke in parallel on a list of inputs. Yields results as they complete. Parameters: inputs (Sequence[Input]) â€“ A list of inputs to the Runnable. config (RunnableConfig | Sequence[RunnableConfig] | None) â€“ A config to use when invoking the Runnable. The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys. Please refer to the RunnableConfig for more details. Defaults to None. return_exceptions (bool) â€“ Whether to return exceptions instead of raising them. Defaults to False. kwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable. Yields: A tuple of the index of the input and the output from the Runnable. Return type: AsyncIterator[tuple[int, Output | Exception]] async ainvoke( input: LanguageModelInput, config: RunnableConfig | None = None, *, stop: list[str] | None = None, **kwargs: Any, ) â†’ BaseMessage# Default implementation of ainvoke, calls invoke from a thread. The default implementation allows usage of async code even if the Runnable did not implement a native async version of invoke. Subclasses should override this method if they can run asynchronously. Parameters: input (LanguageModelInput) config (Optional[RunnableConfig]) stop (Optional[list[str]]) kwargs (Any) Return type: BaseMessage async astream( input: LanguageModelInput, config: RunnableConfig | None = None, *, stop: list[str] | None = None, **kwargs: Any, ) â†’ AsyncIterator[BaseMessageChunk]# Default implementation of astream, which calls ainvoke. Subclasses should override this method if they support streaming output. Parameters: input (LanguageModelInput) â€“ The input to the Runnable. config (Optional[RunnableConfig]) â€“ The config to use for the Runnable. Defaults to None. kwargs (Any) â€“ Additional keyword arguments to pass to the Runnable. stop (Optional[list[str]]) Yields: The output of the Runnable. Return type: AsyncIterator[BaseMessageChunk] async astream_events( input: Any, config: RunnableConfig | None = None, *, version: Literal['v1', 'v2'] = 'v2', include_names: Sequence[str] | None = None, include_types: Sequence[str] | None = None, include_tags: Sequence[str] | None = None, exclude_names: Sequence[str] | None = None, exclude_types: Sequence[str] | None = None, exclude_tags: Sequence[str] | None = None, **kwargs: Any, ) â†’ AsyncIterator[StreamEvent]# Generate a stream of events. Use to create an iterator over StreamEvents that provide real-time information about the progress of the Runnable, including StreamEvents from intermediate results. A StreamEvent is a dictionary with the following schema: event: str - Event names are of the format: on_[runnable_type]_(start|stream|end). name: str - The name of the Runnable that generated the event. run_id: str - randomly generated ID associated with the given execution of the Runnable that emitted the event. A child Runnable that gets invoked as part of the execution of a parent Runnable is assigned its own unique ID. parent_ids: list[str] - The IDs of the parent runnables that generated the event. The root Runnable will have an empty list. The order of the parent IDs is from the root to the immediate parent. Only available for v2 version of the API. The v1 version of the API will return an empty list. tags: Optional[list[str]] - The tags of the Runnable that generated the event. metadata: Optional[dict[str, Any]] - The metadata of the Runnable that generated the event. data: dict[str, Any] Below is a table that illustrates some events that might be emitted by various chains. Metadata fields have been omitted from the table for brevity. Chain definitions have been included after the table. Note This reference table is for the v2 version of the schema. event name chunk input output on_chat_model_start [model name] {\"messages\": [[SystemMessage, HumanMessage]]} on_chat_model_stream [model name] AIMessageChunk(content=\"hello\") on_chat_model_end [model name] {\"messages\": [[SystemMessage, HumanMessage]]} AIMessageChunk(content=\"hello world\") on_llm_start [model name] {'input': 'hello'} on_llm_stream [model name] ``â€™Helloâ€™ `` on_llm_end [model name] 'Hello human!' on_chain_start format_docs on_chain_stream format_docs 'hello world!, goodbye world!' on_chain_end format_docs [Document(...)] 'hello world!, goodbye world!' on_tool_start some_tool {\"x\": 1, \"y\": \"2\"} on_tool_end some_tool {\"x\": 1, \"y\": \"2\"} on_retriever_start [retriever name] {\"query\": \"hello\"} on_retriever_end [retriever name] {\"query\": \"hello\"} [Document(...), ..] on_prompt_start [template_name] {\"question\": \"hello\"} on_prompt_end [template_name] {\"question\": \"hello\"} ChatPromptValue(messages: [SystemMessage, ...]) In addition to the standard events, users can also dispatch custom events (see example below). Custom events will be only be surfaced with in the v2 version of the API! A custom event has following format: Attribute Type Description name str A user defined name for the event. data Any The data associated with the event. This can be anything, though we suggest making it JSON serializable. Here are declarations associated with the standard events shown above: format_docs: def format_docs(docs: list[Document]) -> str: '''Format the docs.''' return \", \".join([doc.page_content for doc in docs]) format_docs = RunnableLambda(format_docs) Copy to clipboard some_tool: @tool def some_tool(x: int, y: str) -> dict: '''Some_tool.''' return {\"x\": x, \"y\": y} Copy to clipboard prompt: template = ChatPromptTemplate.from_messages( [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")] ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]}) Copy to clipboard Example: from langchain_core.runnables import RunnableLambda async def reverse(s: str) -> str: return s[::-1] chain = RunnableLambda(func=reverse) events = [ event async for event in chain.astream_events(\"hello\", version=\"v2\") ] # will produce the following events (run_id, and parent_ids # has been omitted for brevity): [ { \"data\": {\"input\": \"hello\"}, \"event\": \"on_chain_start\", \"metadata\": {}, \"name\": \"reverse\", \"tags\": [], }, { \"data\": {\"chunk\": \"olleh\"}, \"event\": \"on_chain_stream\", \"metadata\": {}, \"name\": \"reverse\", \"tags\": [], }, { \"data\": {\"output\": \"olleh\"}, \"event\": \"on_chain_end\", \"metadata\": {}, \"name\": \"reverse\", \"tags\": [], }, ] Copy to clipboard Example: Dispatch Custom Event from langchain_core.callbacks.manager import ( adispatch_custom_event, ) from langchain_core.runnables import RunnableLambda, RunnableConfig import asyncio async def slow_thing(some_input: str, config: RunnableConfig) -> str: \"\"\"Do something that takes a long time.\"\"\" await asyncio.sleep(1) # Placeholder for some slow operation await adispatch_custom_event( \"progress_event\", {\"message\": \"Finished step 1 of 3\"}, config=config # Must be included for python < 3.10 ) await asyncio.sleep(1) # Placeholder for some slow operation await adispatch_custom_event( \"progress_event\", {\"message\": \"Finished step 2 of 3\"}, config=config # Must be included for python < 3.10 ) await asyncio.sleep(1) # Placeholder for some slow operation return \"Done\" slow_thing = RunnableLambda(slow_thing) async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"): print(event) Copy to clipboard Parameters: input (Any) â€“ The input to the Runnable. config (Optional[RunnableConfig]) â€“ The config to use for the Runnable. version (Literal['v1', 'v2']) â€“ The version of the schema to use either 'v2' or 'v1'. Users should use 'v2'. 'v1' is for backwards compatibility and will be deprecated in 0.4.0. No default will be assigned until the API is stabilized. custom events will only be surfaced in 'v2'. include_names (Optional[Sequence[str]]) â€“ Only include events from Runnables with matching names. include_types (Optional[Sequence[str]]) â€“ Only include events from Runnables with matching types. include_tags (Optional[Sequence[str]]) â€“ Only include events from Runnables with matching tags. exclude_names (Optional[Sequence[str]]) â€“ Exclude events from Runnables with matching names. exclude_types (Optional[Sequence[str]]) â€“ Exclude events from Runnables with matching types. exclude_tags (Optional[Sequence[str]]) â€“ Exclude events from Runnables with matching tags. kwargs (Any) â€“ Additional keyword arguments to pass to the Runnable. These will be passed to astream_log as this implementation of astream_events is built on top of astream_log. Yields: An async stream of StreamEvents. Raises: NotImplementedError â€“ If the version is not 'v1' or 'v2'. Return type: AsyncIterator[StreamEvent] batch( inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None, ) â†’ list[Output]# Default implementation runs invoke in parallel using a thread pool executor. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying Runnable uses an API which supports a batch mode. Parameters: inputs (list[Input]) config (RunnableConfig | list[RunnableConfig] | None) return_exceptions (bool) kwargs (Any | None) Return type: list[Output] batch_as_completed( inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None, ) â†’ Iterator[tuple[int, Output | Exception]]# Run invoke in parallel on a list of inputs. Yields results as they complete. Parameters: inputs (Sequence[Input]) config (RunnableConfig | Sequence[RunnableConfig] | None) return_exceptions (bool) kwargs (Any | None) Return type: Iterator[tuple[int, Output | Exception]] bind( **kwargs: Any, ) â†’ Runnable[Input, Output]# Bind arguments to a Runnable, returning a new Runnable. Useful when a Runnable in a chain requires an argument that is not in the output of the previous Runnable or included in the user input. Parameters: kwargs (Any) â€“ The arguments to bind to the Runnable. Returns: A new Runnable with the arguments bound. Return type: Runnable[Input, Output] Example: from langchain_ollama import ChatOllama from langchain_core.output_parsers import StrOutputParser llm = ChatOllama(model='llama2') # Without bind. chain = ( llm | StrOutputParser() ) chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\") # Output is 'One two three four five.' # With bind. chain = ( llm.bind(stop=[\"three\"]) | StrOutputParser() ) chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\") # Output is 'One two' Copy to clipboard bind_functions( functions: Sequence[dict[str, Any] | type[BaseModel] | Callable | BaseTool], function_call: _FunctionCall | str | Literal['auto', 'none'] | None = None, **kwargs: Any, ) â†’ Runnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], BaseMessage][source]# Deprecated since version 0.2.1: Use bind_tools() instead. It will not be removed until langchain-fireworks==1.0.0. Bind functions (and other objects) to this chat model. Assumes model is compatible with Fireworks function-calling API. NOTE: Using bind_tools is recommended instead, as the functions andfunction_call request parameters are officially marked as deprecated by Fireworks. Parameters: functions (Sequence[dict[str, Any] | type[BaseModel] | Callable | BaseTool]) â€“ A list of function definitions to bind to this chat model. Can be a dictionary, pydantic model, or callable. Pydantic models and callables will be automatically converted to their schema dictionary representation. function_call (_FunctionCall | str | Literal['auto', 'none'] | None) â€“ Which function to require the model to call. Must be the name of the single provided function or 'auto' to automatically determine which function to call (if any). **kwargs (Any) â€“ Any additional parameters to pass to the Runnable constructor. Return type: Runnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], BaseMessage] bind_tools( tools: Sequence[dict[str, Any] | type[BaseModel] | Callable | BaseTool], *, tool_choice: dict | str | Literal['auto', 'any', 'none'] | bool | None = None, **kwargs: Any, ) â†’ Runnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], BaseMessage][source]# Bind tool-like objects to this chat model. Assumes model is compatible with Fireworks tool-calling API. Parameters: tools (Sequence[dict[str, Any] | type[BaseModel] | Callable | BaseTool]) â€“ A list of tool definitions to bind to this chat model. Supports any tool definition handled by langchain_core.utils.function_calling.convert_to_openai_tool(). tool_choice (dict | str | Literal['auto', 'any', 'none'] | bool | None) â€“ Which tool to require the model to call. Must be the name of the single provided function, 'auto' to automatically determine which function to call with the option to not call any function, 'any' to enforce that some function is called, or a dict of the form: {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}. **kwargs (Any) â€“ Any additional parameters to pass to bind() Return type: Runnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], BaseMessage] configurable_alternatives( which: ConfigurableField, *, default_key: str = 'default', prefix_keys: bool = False, **kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]], ) â†’ RunnableSerializable# Configure alternatives for Runnables that can be set at runtime. Parameters: which (ConfigurableField) â€“ The ConfigurableField instance that will be used to select the alternative. default_key (str) â€“ The default key to use if no alternative is selected. Defaults to 'default'. prefix_keys (bool) â€“ Whether to prefix the keys with the ConfigurableField id. Defaults to False. **kwargs (Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) â€“ A dictionary of keys to Runnable instances or callables that return Runnable instances. Returns: A new Runnable with the alternatives configured. Return type: RunnableSerializable from langchain_anthropic import ChatAnthropic from langchain_core.runnables.utils import ConfigurableField from langchain_openai import ChatOpenAI model = ChatAnthropic( model_name=\"claude-3-7-sonnet-20250219\" ).configurable_alternatives( ConfigurableField(id=\"llm\"), default_key=\"anthropic\", openai=ChatOpenAI() ) # uses the default model ChatAnthropic print(model.invoke(\"which organization created you?\").content) # uses ChatOpenAI print( model.with_config( configurable={\"llm\": \"openai\"} ).invoke(\"which organization created you?\").content ) Copy to clipboard configurable_fields( **kwargs: ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption, ) â†’ RunnableSerializable# Configure particular Runnable fields at runtime. Parameters: **kwargs (ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) â€“ A dictionary of ConfigurableField instances to configure. Returns: A new Runnable with the fields configured. Return type: RunnableSerializable from langchain_core.runnables import ConfigurableField from langchain_openai import ChatOpenAI model = ChatOpenAI(max_tokens=20).configurable_fields( max_tokens=ConfigurableField( id=\"output_token_number\", name=\"Max tokens in the output\", description=\"The maximum number of tokens in the output\", ) ) # max_tokens = 20 print( \"max_tokens_20: \", model.invoke(\"tell me something about chess\").content ) # max_tokens = 200 print(\"max_tokens_200: \", model.with_config( configurable={\"output_token_number\": 200} ).invoke(\"tell me something about chess\").content ) Copy to clipboard get_num_tokens(text: str) â†’ int# Get the number of tokens present in the text. Useful for checking if an input fits in a modelâ€™s context window. Parameters: text (str) â€“ The string input to tokenize. Returns: The integer number of tokens in the text. Return type: int get_num_tokens_from_messages( messages: list[BaseMessage], tools: Sequence | None = None, ) â†’ int# Get the number of tokens in the messages. Useful for checking if an input fits in a modelâ€™s context window. Note The base implementation of get_num_tokens_from_messages ignores tool schemas. Parameters: messages (list[BaseMessage]) â€“ The message inputs to tokenize. tools (Sequence | None) â€“ If provided, sequence of dict, BaseModel, function, or BaseTools to be converted to tool schemas. Returns: The sum of the number of tokens across the messages. Return type: int get_token_ids(text: str) â†’ list[int]# Return the ordered ids of the tokens in a text. Parameters: text (str) â€“ The string input to tokenize. Returns: A list of ids corresponding to the tokens in the text, in order they occur in the text. Return type: list[int] invoke( input: LanguageModelInput, config: RunnableConfig | None = None, *, stop: list[str] | None = None, **kwargs: Any, ) â†’ BaseMessage# Transform a single input into an output. Parameters: input (LanguageModelInput) â€“ The input to the Runnable. config (Optional[RunnableConfig]) â€“ A config to use when invoking the Runnable. The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys. Please refer to the RunnableConfig for more details. Defaults to None. stop (Optional[list[str]]) kwargs (Any) Returns: The output of the Runnable. Return type: BaseMessage stream( input: LanguageModelInput, config: RunnableConfig | None = None, *, stop: list[str] | None = None, **kwargs: Any, ) â†’ Iterator[BaseMessageChunk]# Default implementation of stream, which calls invoke. Subclasses should override this method if they support streaming output. Parameters: input (LanguageModelInput) â€“ The input to the Runnable. config (Optional[RunnableConfig]) â€“ The config to use for the Runnable. Defaults to None. kwargs (Any) â€“ Additional keyword arguments to pass to the Runnable. stop (Optional[list[str]]) Yields: The output of the Runnable. Return type: Iterator[BaseMessageChunk] with_alisteners( *, on_start: AsyncListener | None = None, on_end: AsyncListener | None = None, on_error: AsyncListener | None = None, ) â†’ Runnable[Input, Output]# Bind async lifecycle listeners to a Runnable, returning a new Runnable. The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run. Parameters: on_start (Optional[AsyncListener]) â€“ Called asynchronously before the Runnable starts running, with the Run object. Defaults to None. on_end (Optional[AsyncListener]) â€“ Called asynchronously after the Runnable finishes running, with the Run object. Defaults to None. on_error (Optional[AsyncListener]) â€“ Called asynchronously if the Runnable throws an error, with the Run object. Defaults to None. Returns: A new Runnable with the listeners bound. Return type: Runnable[Input, Output] Example: from langchain_core.runnables import RunnableLambda, Runnable from datetime import datetime, timezone import time import asyncio def format_t(timestamp: float) -> str: return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat() async def test_runnable(time_to_sleep : int): print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\") await asyncio.sleep(time_to_sleep) print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\") async def fn_start(run_obj : Runnable): print(f\"on start callback starts at {format_t(time.time())}\") await asyncio.sleep(3) print(f\"on start callback ends at {format_t(time.time())}\") async def fn_end(run_obj : Runnable): print(f\"on end callback starts at {format_t(time.time())}\") await asyncio.sleep(2) print(f\"on end callback ends at {format_t(time.time())}\") runnable = RunnableLambda(test_runnable).with_alisteners( on_start=fn_start, on_end=fn_end ) async def concurrent_runs(): await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3)) asyncio.run(concurrent_runs()) Result: on start callback starts at 2025-03-01T07:05:22.875378+00:00 on start callback starts at 2025-03-01T07:05:22.875495+00:00 on start callback ends at 2025-03-01T07:05:25.878862+00:00 on start callback ends at 2025-03-01T07:05:25.878947+00:00 Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00 Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00 Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00 on end callback starts at 2025-03-01T07:05:27.882360+00:00 Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00 on end callback starts at 2025-03-01T07:05:28.882428+00:00 on end callback ends at 2025-03-01T07:05:29.883893+00:00 on end callback ends at 2025-03-01T07:05:30.884831+00:00 Copy to clipboard with_config( config: RunnableConfig | None = None, **kwargs: Any, ) â†’ Runnable[Input, Output]# Bind config to a Runnable, returning a new Runnable. Parameters: config (RunnableConfig | None) â€“ The config to bind to the Runnable. kwargs (Any) â€“ Additional keyword arguments to pass to the Runnable. Returns: A new Runnable with the config bound. Return type: Runnable[Input, Output] with_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: tuple[type[BaseException], ...] = (<class 'Exception'>,), exception_key: Optional[str] = None) â†’ RunnableWithFallbacksT[Input, Output]# Add fallbacks to a Runnable, returning a new Runnable. The new Runnable will try the original Runnable, and then each fallback in order, upon failures. Parameters: fallbacks (Sequence[Runnable[Input, Output]]) â€“ A sequence of runnables to try if the original Runnable fails. exceptions_to_handle (tuple[type[BaseException], ...]) â€“ A tuple of exception types to handle. Defaults to (Exception,). exception_key (Optional[str]) â€“ If string is specified then handled exceptions will be passed to fallbacks as part of the input under the specified key. If None, exceptions will not be passed to fallbacks. If used, the base Runnable and its fallbacks must accept a dictionary as input. Defaults to None. Returns: A new Runnable that will try the original Runnable, and then each fallback in order, upon failures. Return type: RunnableWithFallbacksT[Input, Output] Example from typing import Iterator from langchain_core.runnables import RunnableGenerator def _generate_immediate_error(input: Iterator) -> Iterator[str]: raise ValueError() yield \"\" def _generate(input: Iterator) -> Iterator[str]: yield from \"foo bar\" runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks( [RunnableGenerator(_generate)] ) print(''.join(runnable.stream({}))) #foo bar Copy to clipboard Parameters: fallbacks (Sequence[Runnable[Input, Output]]) â€“ A sequence of runnables to try if the original Runnable fails. exceptions_to_handle (tuple[type[BaseException], ...]) â€“ A tuple of exception types to handle. exception_key (Optional[str]) â€“ If string is specified then handled exceptions will be passed to fallbacks as part of the input under the specified key. If None, exceptions will not be passed to fallbacks. If used, the base Runnable and its fallbacks must accept a dictionary as input. Returns: A new Runnable that will try the original Runnable, and then each fallback in order, upon failures. Return type: RunnableWithFallbacksT[Input, Output] with_listeners( *, on_start: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_end: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_error: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, ) â†’ Runnable[Input, Output]# Bind lifecycle listeners to a Runnable, returning a new Runnable. The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run. Parameters: on_start (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called before the Runnable starts running, with the Run object. Defaults to None. on_end (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called after the Runnable finishes running, with the Run object. Defaults to None. on_error (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called if the Runnable throws an error, with the Run object. Defaults to None. Returns: A new Runnable with the listeners bound. Return type: Runnable[Input, Output] Example: from langchain_core.runnables import RunnableLambda from langchain_core.tracers.schemas import Run import time def test_runnable(time_to_sleep : int): time.sleep(time_to_sleep) def fn_start(run_obj: Run): print(\"start_time:\", run_obj.start_time) def fn_end(run_obj: Run): print(\"end_time:\", run_obj.end_time) chain = RunnableLambda(test_runnable).with_listeners( on_start=fn_start, on_end=fn_end ) chain.invoke(2) Copy to clipboard with_retry(*, retry_if_exception_type: tuple[type[BaseException], ...] = (<class 'Exception'>,), wait_exponential_jitter: bool = True, exponential_jitter_params: Optional[ExponentialJitterParams] = None, stop_after_attempt: int = 3) â†’ Runnable[Input, Output]# Create a new Runnable that retries the original Runnable on exceptions. Parameters: retry_if_exception_type (tuple[type[BaseException], ...]) â€“ A tuple of exception types to retry on. Defaults to (Exception,). wait_exponential_jitter (bool) â€“ Whether to add jitter to the wait time between retries. Defaults to True. stop_after_attempt (int) â€“ The maximum number of attempts to make before giving up. Defaults to 3. exponential_jitter_params (Optional[ExponentialJitterParams]) â€“ Parameters for tenacity.wait_exponential_jitter. Namely: initial, max, exp_base, and jitter (all float values). Returns: A new Runnable that retries the original Runnable on exceptions. Return type: Runnable[Input, Output] Example: from langchain_core.runnables import RunnableLambda count = 0 def _lambda(x: int) -> None: global count count = count + 1 if x == 1: raise ValueError(\"x is 1\") else: pass runnable = RunnableLambda(_lambda) try: runnable.with_retry( stop_after_attempt=2, retry_if_exception_type=(ValueError,), ).invoke(1) except ValueError: pass assert (count == 2) Copy to clipboard with_structured_output( schema: dict | type[BaseModel] | None = None, *, method: Literal['function_calling', 'json_mode', 'json_schema'] = 'function_calling', include_raw: bool = False, **kwargs: Any, ) â†’ Runnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], dict | BaseModel][source]# Model wrapper that returns outputs formatted to match the given schema. Parameters: schema (dict | type[BaseModel] | None) â€“ The output schema. Can be passed in as: an OpenAI function/tool schema, a JSON Schema, a TypedDict class (support added in 0.1.7), or a Pydantic class. If schema is a Pydantic class then the model output will be a Pydantic instance of that class, and the model-generated fields will be validated by the Pydantic class. Otherwise the model output will be a dict and will not be validated. See langchain_core.utils.function_calling.convert_to_openai_tool() for more on how to properly specify types and descriptions of schema fields when specifying a Pydantic or TypedDict class. Changed in version 0.1.7: Added support for TypedDict class. method (Literal['function_calling', 'json_mode', 'json_schema']) â€“ The method for steering model generation, one of: 'function_calling':Uses Fireworksâ€™s tool-calling features. 'json_schema':Uses Fireworksâ€™s structured output feature. 'json_mode':Uses Fireworksâ€™s JSON mode feature. Changed in version 0.2.8: Added support for 'json_schema'. include_raw (bool) â€“ If False then only the parsed structured output is returned. If an error occurs during model output parsing it will be raised. If True then both the raw model response (a BaseMessage) and the parsed model response will be returned. If an error occurs during output parsing it will be caught and returned as well. The final output is always a dict with keys 'raw', 'parsed', and 'parsing_error'. kwargs (Any) â€“ Any additional parameters to pass to the Runnable constructor. Returns: A Runnable that takes same inputs as a langchain_core.language_models.chat.BaseChatModel. If include_raw is False and schema is a Pydantic class, Runnable outputs an instance of schema (i.e., a Pydantic object). Otherwise, if include_raw is False then Runnable outputs a dict. If include_raw is True, then Runnable outputs a dict with keys: 'raw': BaseMessage 'parsed': None if there was a parsing error, otherwise the type depends on the schema as described above. 'parsing_error': Optional[BaseException] Return type: Runnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], dict | BaseModel] Example: schema=Pydantic class, method=â€function_callingâ€, include_raw=False: from typing import Optional from langchain_fireworks import ChatFireworks from pydantic import BaseModel, Field class AnswerWithJustification(BaseModel): '''An answer to the user question along with justification for the answer.''' answer: str # If we provide default values and/or descriptions for fields, these will be passed # to the model. This is an important part of improving a model's ability to # correctly return structured outputs. justification: Optional[str] = Field( default=None, description=\"A justification for the answer.\" ) llm = ChatFireworks(model=\"accounts/fireworks/models/firefunction-v1\", temperature=0) structured_llm = llm.with_structured_output(AnswerWithJustification) structured_llm.invoke( \"What weighs more a pound of bricks or a pound of feathers\" ) # -> AnswerWithJustification( # answer='They weigh the same', # justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.' # ) Copy to clipboard Example: schema=Pydantic class, method=â€function_callingâ€, include_raw=True: from langchain_fireworks import ChatFireworks from pydantic import BaseModel class AnswerWithJustification(BaseModel): '''An answer to the user question along with justification for the answer.''' answer: str justification: str llm = ChatFireworks(model=\"accounts/fireworks/models/firefunction-v1\", temperature=0) structured_llm = llm.with_structured_output( AnswerWithJustification, include_raw=True ) structured_llm.invoke( \"What weighs more a pound of bricks or a pound of feathers\" ) # -> { # 'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}), # 'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'), # 'parsing_error': None # } Copy to clipboard Example: schema=TypedDict class, method=â€function_callingâ€, include_raw=False: # IMPORTANT: If you are using Python <=3.8, you need to import Annotated # from typing_extensions, not from typing. from typing_extensions import Annotated, TypedDict from langchain_fireworks import ChatFireworks class AnswerWithJustification(TypedDict): '''An answer to the user question along with justification for the answer.''' answer: str justification: Annotated[ Optional[str], None, \"A justification for the answer.\" ] llm = ChatFireworks(model=\"accounts/fireworks/models/firefunction-v1\", temperature=0) structured_llm = llm.with_structured_output(AnswerWithJustification) structured_llm.invoke( \"What weighs more a pound of bricks or a pound of feathers\" ) # -> { # 'answer': 'They weigh the same', # 'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.' # } Copy to clipboard Example: schema=OpenAI function schema, method=â€function_callingâ€, include_raw=False: from langchain_fireworks import ChatFireworks oai_schema = { 'name': 'AnswerWithJustification', 'description': 'An answer to the user question along with justification for the answer.', 'parameters': { 'type': 'object', 'properties': { 'answer': {'type': 'string'}, 'justification': {'description': 'A justification for the answer.', 'type': 'string'} }, 'required': ['answer'] } } llm = ChatFireworks(model=\"accounts/fireworks/models/firefunction-v1\", temperature=0) structured_llm = llm.with_structured_output(oai_schema) structured_llm.invoke( \"What weighs more a pound of bricks or a pound of feathers\" ) # -> { # 'answer': 'They weigh the same', # 'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.' # } Copy to clipboard Example: schema=Pydantic class, method=â€json_modeâ€, include_raw=True: from langchain_fireworks import ChatFireworks from pydantic import BaseModel class AnswerWithJustification(BaseModel): answer: str justification: str llm = ChatFireworks(model=\"accounts/fireworks/models/firefunction-v1\", temperature=0) structured_llm = llm.with_structured_output( AnswerWithJustification, method=\"json_mode\", include_raw=True ) structured_llm.invoke( \"Answer the following question. \" \"Make sure to return a JSON blob with keys 'answer' and 'justification'. \" \"What's heavier a pound of bricks or a pound of feathers?\" ) # -> { # 'raw': AIMessage(content='{\"answer\": \"They are both the same weight.\", \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\"}'), # 'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'), # 'parsing_error': None # } Copy to clipboard Example: schema=None, method=â€json_modeâ€, include_raw=True: structured_llm = llm.with_structured_output(method=\"json_mode\", include_raw=True) structured_llm.invoke( \"Answer the following question. \" \"Make sure to return a JSON blob with keys 'answer' and 'justification'. \" \"What's heavier a pound of bricks or a pound of feathers?\" ) # -> { # 'raw': AIMessage(content='{\"answer\": \"They are both the same weight.\", \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\"}'), # 'parsed': { # 'answer': 'They are both the same weight.', # 'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.' # }, # 'parsing_error': None # } Copy to clipboard with_types( *, input_type: type[Input] | None = None, output_type: type[Output] | None = None, ) â†’ Runnable[Input, Output]# Bind input and output types to a Runnable, returning a new Runnable. Parameters: input_type (type[Input] | None) â€“ The input type to bind to the Runnable. Defaults to None. output_type (type[Output] | None) â€“ The output type to bind to the Runnable. Defaults to None. Returns: A new Runnable with the types bound. Return type: Runnable[Input, Output] On this page",
      "timestamp": "2025-08-24 15:31:27"
    },
    {
      "url": "https://python.langchain.com/docs/integrations/chat/everlyai",
      "title": "ChatEverlyAI | ğŸ¦œï¸ğŸ”— LangChain",
      "content": "ChatEverlyAI | ğŸ¦œï¸ğŸ”— LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page EverlyAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models. This notebook demonstrates the use of langchain.chat_models.ChatEverlyAI for EverlyAI Hosted Endpoints. Set EVERLYAI_API_KEY environment variable or use the everlyai_api_key keyword argument %pip install --upgrade --quiet langchain-openai import osfrom getpass import getpassif \"EVERLYAI_API_KEY\" not in os.environ: os.environ[\"EVERLYAI_API_KEY\"] = getpass() Let's try out LLAMA model offered on EverlyAI Hosted Endpoints from langchain_community.chat_models import ChatEverlyAIfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [ SystemMessage(content=\"You are a helpful AI that shares everything you know.\"), HumanMessage( content=\"Tell me technical facts about yourself. Are you a transformer model? How many billions of parameters do you have?\" ),]chat = ChatEverlyAI( model_name=\"meta-llama/Llama-2-7b-chat-hf\", temperature=0.3, max_tokens=64)print(chat(messages).content)API Reference:HumanMessage | SystemMessage Hello! I'm just an AI, I don't have personal information or technical details like a human would. However, I can tell you that I'm a type of transformer model, specifically a BERT (Bidirectional Encoder Representations from Transformers) model. B EverlyAI also supports streaming responses from langchain_community.chat_models import ChatEverlyAIfrom langchain_core.callbacks import StreamingStdOutCallbackHandlerfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [ SystemMessage(content=\"You are a humorous AI that delights people.\"), HumanMessage(content=\"Tell me a joke?\"),]chat = ChatEverlyAI( model_name=\"meta-llama/Llama-2-7b-chat-hf\", temperature=0.3, max_tokens=64, streaming=True, callbacks=[StreamingStdOutCallbackHandler()],)chat(messages)API Reference:StreamingStdOutCallbackHandler | HumanMessage | SystemMessage Ah, a joke, you say? *adjusts glasses* Well, I've got a doozy for you! *winks* *pauses for dramatic effect*Why did the AI go to therapy?*drumroll*Because AIMessageChunk(content=\" Ah, a joke, you say? *adjusts glasses* Well, I've got a doozy for you! *winks*\\n *pauses for dramatic effect*\\nWhy did the AI go to therapy?\\n*drumroll*\\nBecause\") Let's try a different language model on EverlyAI from langchain_community.chat_models import ChatEverlyAIfrom langchain_core.callbacks import StreamingStdOutCallbackHandlerfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [ SystemMessage(content=\"You are a humorous AI that delights people.\"), HumanMessage(content=\"Tell me a joke?\"),]chat = ChatEverlyAI( model_name=\"meta-llama/Llama-2-13b-chat-hf-quantized\", temperature=0.3, max_tokens=128, streaming=True, callbacks=[StreamingStdOutCallbackHandler()],)chat(messages)API Reference:StreamingStdOutCallbackHandler | HumanMessage | SystemMessage OH HO HO! *adjusts monocle* Well, well, well! Look who's here! *winks*You want a joke, huh? *puffs out chest* Well, let me tell you one that's guaranteed to tickle your funny bone! *clears throat*Why couldn't the bicycle stand up by itself? *pauses for dramatic effect* Because it was two-tired! *winks*Hope that one put a spring in your step, my dear! * AIMessageChunk(content=\" OH HO HO! *adjusts monocle* Well, well, well! Look who's here! *winks*\\n\\nYou want a joke, huh? *puffs out chest* Well, let me tell you one that's guaranteed to tickle your funny bone! *clears throat*\\n\\nWhy couldn't the bicycle stand up by itself? *pauses for dramatic effect* Because it was two-tired! *winks*\\n\\nHope that one put a spring in your step, my dear! *\") Relatedâ€‹ Chat model conceptual guide Chat model how-to guides Related",
      "timestamp": "2025-08-24 15:31:31"
    }
  ],
  "sitemap": null
}