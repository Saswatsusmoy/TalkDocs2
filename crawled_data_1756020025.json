{
  "base_url": "https://python.langchain.com/docs/introduction/",
  "total_pages": 10,
  "crawl_timestamp": "2025-08-24 12:51:17",
  "pages": [
    {
      "url": "https://python.langchain.com/docs/introduction/",
      "title": "Introduction | ü¶úÔ∏èüîó LangChain",
      "content": "Introduction | ü¶úÔ∏èüîó LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page LangChain is a framework for developing applications powered by large language models (LLMs). LangChain simplifies every stage of the LLM application lifecycle: Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support. Productionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence. Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform. LangChain implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers. See the integrations page for more. Select chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") model.invoke(\"Hello, world!\") noteThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library. Architecture‚Äã The LangChain framework consists of multiple open-source libraries. Read more in the Architecture page. langchain-core: Base abstractions for chat models and other components. Integration packages (e.g. langchain-openai, langchain-anthropic, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers. langchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture. langchain-community: Third-party integrations that are community maintained. langgraph: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See LangGraph documentation. Guides‚Äã Tutorials‚Äã If you're looking to build something specific or are more of a hands-on learner, check out our tutorials section. This is the best place to get started. These are the best ones to get started with: Build a Simple LLM Application Build a Chatbot Build an Agent Introduction to LangGraph Explore the full list of LangChain tutorials here, and check out other LangGraph tutorials here. To learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available here. How-to guides‚Äã Here you‚Äôll find short answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions. These how-to guides don‚Äôt cover topics in depth ‚Äì you‚Äôll find that material in the Tutorials and the API Reference. However, these guides will help you quickly accomplish common tasks using chat models, vector stores, and other common LangChain components. Check out LangGraph-specific how-tos here. Conceptual guide‚Äã Introductions to all the key parts of LangChain you‚Äôll need to know! Here you'll find high level explanations of all LangChain concepts. For a deeper dive into LangGraph concepts, check out this page. Integrations‚Äã LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. If you're looking to get up and running quickly with chat models, vector stores, or other LangChain components from a specific provider, check out our growing list of integrations. API reference‚Äã Head to the reference section for full documentation of all classes and methods in the LangChain Python packages. Ecosystem‚Äã ü¶úüõ†Ô∏è LangSmith‚Äã Trace and evaluate your language model applications and intelligent agents to help you move from prototype to production. ü¶úüï∏Ô∏è LangGraph‚Äã Build stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more. Additional resources‚Äã Versions‚Äã See what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more. Security‚Äã Read up on security best practices to make sure you're developing safely with LangChain. Contributing‚Äã Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.ArchitectureGuidesTutorialsHow-to guidesConceptual guideIntegrationsAPI referenceEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphAdditional resourcesVersionsSecurityContributing",
      "timestamp": "2025-08-24 12:50:35"
    },
    {
      "url": "https://python.langchain.com/docs/tutorials/agents/",
      "title": "Build an Agent | ü¶úÔ∏èüîó LangChain",
      "content": "Build an Agent | ü¶úÔ∏èüîó LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page LangChain supports the creation of agents, or systems that use LLMs as reasoning engines to determine which actions to take and the inputs necessary to perform the action. After executing actions, the results can be fed back into the LLM to determine whether more actions are needed, or whether it is okay to finish. This is often achieved via tool-calling. In this tutorial we will build an agent that can interact with a search engine. You will be able to ask this agent questions, watch it call the search tool, and have conversations with it. End-to-end agent‚Äã The code snippet below represents a fully functional agent that uses an LLM to decide which tools to use. It is equipped with a generic search tool. It has conversational memory - meaning that it can be used as a multi-turn chatbot. In the rest of the guide, we will walk through the individual components and what each part does - but if you want to just grab some code and get started, feel free to use this! # Import relevant functionalityfrom langchain.chat_models import init_chat_modelfrom langchain_tavily import TavilySearchfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.prebuilt import create_react_agent# Create the agentmemory = MemorySaver()model = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")search = TavilySearch(max_results=2)tools = [search]agent_executor = create_react_agent(model, tools, checkpointer=memory)API Reference:MemorySaver | create_react_agent # Use the agentconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}input_message = { \"role\": \"user\", \"content\": \"Hi, I'm Bob and I live in SF.\",}for step in agent_executor.stream( {\"messages\": [input_message]}, config, stream_mode=\"values\"): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================Hi, I'm Bob and I live in SF.==================================\u001b[1m Ai Message \u001b[0m==================================Hello Bob! I notice you've introduced yourself and mentioned you live in SF (San Francisco), but you haven't asked a specific question or made a request that requires the use of any tools. Is there something specific you'd like to know about San Francisco or any other topic? I'd be happy to help you find information using the available search tools. input_message = { \"role\": \"user\", \"content\": \"What's the weather where I live?\",}for step in agent_executor.stream( {\"messages\": [input_message]}, config, stream_mode=\"values\"): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================What's the weather where I live?==================================\u001b[1m Ai Message \u001b[0m==================================[{'text': 'Let me search for current weather information in San Francisco.', 'type': 'text'}, {'id': 'toolu_011kSdheoJp8THURoLmeLtZo', 'input': {'query': 'current weather San Francisco CA'}, 'name': 'tavily_search', 'type': 'tool_use'}]Tool Calls: tavily_search (toolu_011kSdheoJp8THURoLmeLtZo) Call ID: toolu_011kSdheoJp8THURoLmeLtZo Args: query: current weather San Francisco CA=================================\u001b[1m Tool Message \u001b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco CA\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168606, 'localtime': '2025-06-17 06:56'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", \"score\": 0.944705, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed ‚ö° San Francisco Weather Forecast for June 2025 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget ¬∞F. World; United States; California; Weather in San Francisco; ... 17 +64¬∞ +54¬∞ 18 +61¬∞ +54¬∞ 19\", \"score\": 0.86441374, \"raw_content\": null}], \"response_time\": 2.34}==================================\u001b[1m Ai Message \u001b[0m==================================Based on the search results, here's the current weather in San Francisco:- Temperature: 53.1¬∞F (11.7¬∞C)- Condition: Foggy- Wind: 4.0 mph from the Southwest- Humidity: 86%- Visibility: 9 milesThis is quite typical weather for San Francisco, with the characteristic fog that the city is known for. Would you like to know anything else about the weather or San Francisco in general? Setup‚Äã Jupyter Notebook‚Äã This guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect interactive environments for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc), and observing these cases is a great way to better understand building with LLMs. This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install. Installation‚Äã To install LangChain run: %pip install -U langgraph langchain-tavily langgraph-checkpoint-sqlite For more details, see our Installation guide. LangSmith‚Äã Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. After you sign up at the link above, make sure to set your environment variables to start logging traces: export LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\" Or, if in a notebook, you can set them with: import getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass() Tavily‚Äã We will be using Tavily (a search engine) as a tool. In order to use it, you will need to get and set an API key: export TAVILY_API_KEY=\"...\" Or, if in a notebook, you can set it with: import getpassimport osos.environ[\"TAVILY_API_KEY\"] = getpass.getpass() Define tools‚Äã We first need to create the tools we want to use. Our main tool of choice will be Tavily - a search engine. We can use the dedicated langchain-tavily integration package to easily use Tavily search engine as tool with LangChain. from langchain_tavily import TavilySearchsearch = TavilySearch(max_results=2)search_results = search.invoke(\"What is the weather in SF\")print(search_results)# If we want, we can create other tools.# Once we have all the tools we want, we can put them in a list that we will reference later.tools = [search] {'query': 'What is the weather in SF', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Weather in San Francisco, CA', 'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168606, 'localtime': '2025-06-17 06:56'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", 'score': 0.9185379, 'raw_content': None}, {'title': 'Weather in San Francisco in June 2025', 'url': 'https://world-weather.info/forecast/usa/san_francisco/june-2025/', 'content': \"Weather in San Francisco in June 2025 (California) - Detailed Weather Forecast for a Month * Weather in San Francisco Weather in San Francisco in June 2025 * 1 +63¬∞ +55¬∞ * 2 +66¬∞ +54¬∞ * 3 +66¬∞ +55¬∞ * 4 +66¬∞ +54¬∞ * 5 +66¬∞ +55¬∞ * 6 +66¬∞ +57¬∞ * 7 +64¬∞ +55¬∞ * 8 +63¬∞ +55¬∞ * 9 +63¬∞ +54¬∞ * 10 +59¬∞ +54¬∞ * 11 +59¬∞ +54¬∞ * 12 +61¬∞ +54¬∞ Weather in Washington, D.C.**+68¬∞** Sacramento**+81¬∞** Pleasanton**+72¬∞** Redwood City**+68¬∞** San Leandro**+61¬∞** San Mateo**+64¬∞** San Rafael**+70¬∞** San Ramon**+64¬∞** South San Francisco**+61¬∞** Daly City**+59¬∞** Wilder**+66¬∞** Woodacre**+70¬∞** world's temperature today Colchani day+50¬∞F night+16¬∞F Az Zubayr day+124¬∞F night+93¬∞F Weather forecast on your site Install _San Francisco_ +61¬∞ Temperature units\", 'score': 0.7978881, 'raw_content': None}], 'response_time': 2.62} tipIn many applications, you may want to define custom tools. LangChain supports custom tool creation via Python functions and other means. Refer to the How to create tools guide for details. Using Language Models‚Äã Next, let's learn how to use a language model to call tools. LangChain supports many different language models that you can use interchangably - select the one you want to use below! Select chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") You can call the language model by passing in a list of messages. By default, the response is a content string. query = \"Hi!\"response = model.invoke([{\"role\": \"user\", \"content\": query}])response.text() 'Hello! How can I help you today?' We can now see what it is like to enable this model to do tool calling. In order to enable that we use .bind_tools to give the language model knowledge of these tools model_with_tools = model.bind_tools(tools) We can now call the model. Let's first call it with a normal message, and see how it responds. We can look at both the content field as well as the tool_calls field. query = \"Hi!\"response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])print(f\"Message content: {response.text()}\\n\")print(f\"Tool calls: {response.tool_calls}\") Message content: Hello! I'm here to help you. I have access to a powerful search tool that can help answer questions and find information about various topics. What would you like to know about?Feel free to ask any question or request information, and I'll do my best to assist you using the available tools.Tool calls: [] Now, let's try calling it with some input that would expect a tool to be called. query = \"Search for the weather in SF\"response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])print(f\"Message content: {response.text()}\\n\")print(f\"Tool calls: {response.tool_calls}\") Message content: I'll help you search for information about the weather in San Francisco.Tool calls: [{'name': 'tavily_search', 'args': {'query': 'current weather San Francisco'}, 'id': 'toolu_015gdPn1jbB2Z21DmN2RAnti', 'type': 'tool_call'}] We can see that there's now no text content, but there is a tool call! It wants us to call the Tavily Search tool. This isn't calling that tool yet - it's just telling us to. In order to actually call it, we'll want to create our agent. Create the agent‚Äã Now that we have defined the tools and the LLM, we can create the agent. We will be using LangGraph to construct the agent. Currently, we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic. Now, we can initialize the agent with the LLM and the tools. Note that we are passing in the model, not model_with_tools. That is because create_react_agent will call .bind_tools for us under the hood. from langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(model, tools)API Reference:create_react_agent Run the agent‚Äã We can now run the agent with a few queries! Note that for now, these are all stateless queries (it won't remember previous interactions). Note that the agent will return the final state at the end of the interaction (which includes any inputs, we will see later on how to get only the outputs). First up, let's see how it responds when there's no need to call a tool: input_message = {\"role\": \"user\", \"content\": \"Hi!\"}response = agent_executor.invoke({\"messages\": [input_message]})for message in response[\"messages\"]: message.pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================Hi!==================================\u001b[1m Ai Message \u001b[0m==================================Hello! I'm here to help you with your questions using the available search tools. Please feel free to ask any question, and I'll do my best to find relevant and accurate information for you. In order to see exactly what is happening under the hood (and to make sure it's not calling a tool) we can take a look at the LangSmith trace Let's now try it out on an example where it should be invoking the tool input_message = {\"role\": \"user\", \"content\": \"Search for the weather in SF\"}response = agent_executor.invoke({\"messages\": [input_message]})for message in response[\"messages\"]: message.pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================Search for the weather in SF==================================\u001b[1m Ai Message \u001b[0m==================================[{'text': \"I'll help you search for weather information in San Francisco. Let me use the search engine to find current weather conditions.\", 'type': 'text'}, {'id': 'toolu_01WWcXGnArosybujpKzdmARZ', 'input': {'query': 'current weather San Francisco SF'}, 'name': 'tavily_search', 'type': 'tool_use'}]Tool Calls: tavily_search (toolu_01WWcXGnArosybujpKzdmARZ) Call ID: toolu_01WWcXGnArosybujpKzdmARZ Args: query: current weather San Francisco SF=================================\u001b[1m Tool Message \u001b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco SF\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168606, 'localtime': '2025-06-17 06:56'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", \"score\": 0.885373, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed ‚ö° San Francisco Weather Forecast for June 2025 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget ¬∞F. World; United States; California; Weather in San Francisco; ... 17 +64¬∞ +54¬∞ 18 +61¬∞ +54¬∞ 19\", \"score\": 0.8830044, \"raw_content\": null}], \"response_time\": 2.6}==================================\u001b[1m Ai Message \u001b[0m==================================Based on the search results, here's the current weather in San Francisco:- Temperature: 53.1¬∞F (11.7¬∞C)- Conditions: Foggy- Wind: 4.0 mph from the SW- Humidity: 86%- Visibility: 9.0 milesThe weather appears to be typical for San Francisco, with morning fog and mild temperatures. The \"feels like\" temperature is 52.4¬∞F (11.3¬∞C). We can check out the LangSmith trace to make sure it's calling the search tool effectively. Streaming Messages‚Äã We've seen how the agent can be called with .invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur. for step in agent_executor.stream({\"messages\": [input_message]}, stream_mode=\"values\"): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================Search for the weather in SF==================================\u001b[1m Ai Message \u001b[0m==================================[{'text': \"I'll help you search for information about the weather in San Francisco.\", 'type': 'text'}, {'id': 'toolu_01DCPnJES53Fcr7YWnZ47kDG', 'input': {'query': 'current weather San Francisco'}, 'name': 'tavily_search', 'type': 'tool_use'}]Tool Calls: tavily_search (toolu_01DCPnJES53Fcr7YWnZ47kDG) Call ID: toolu_01DCPnJES53Fcr7YWnZ47kDG Args: query: current weather San Francisco=================================\u001b[1m Tool Message \u001b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168506, 'localtime': '2025-06-17 06:55'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", \"score\": 0.9542825, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed ‚ö° San Francisco Weather Forecast for June 2025 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget ¬∞F. World; United States; California; Weather in San Francisco; ... 17 +64¬∞ +54¬∞ 18 +61¬∞ +54¬∞ 19\", \"score\": 0.8638634, \"raw_content\": null}], \"response_time\": 2.57}==================================\u001b[1m Ai Message \u001b[0m==================================Based on the search results, here's the current weather in San Francisco:- Temperature: 53.1¬∞F (11.7¬∞C)- Condition: Foggy- Wind: 4.0 mph from the Southwest- Humidity: 86%- Visibility: 9.0 miles- Feels like: 52.4¬∞F (11.3¬∞C)This is quite typical weather for San Francisco, which is known for its fog, especially during the morning hours. The city's proximity to the ocean and unique geographical features often result in mild temperatures and foggy conditions. Streaming tokens‚Äã In addition to streaming back messages, it is also useful to stream back tokens. We can do this by specifying stream_mode=\"messages\". ::: note Below we use message.text(), which requires langchain-core>=0.3.37. ::: for step, metadata in agent_executor.stream( {\"messages\": [input_message]}, stream_mode=\"messages\"): if metadata[\"langgraph_node\"] == \"agent\" and (text := step.text()): print(text, end=\"|\") I|'ll help you search for information| about the weather in San Francisco.|Base|d on the search results, here|'s the current weather in| San Francisco:-| Temperature: 53.1¬∞F (|11.7¬∞C)-| Condition: Foggy- Wind:| 4.0 mph from| the Southwest- Humidity|: 86%|- Visibility: 9|.0 miles- Pressure: |30.02 in|HgThe weather| is characteristic of San Francisco, with| foggy conditions and mild temperatures|. The \"feels like\" temperature is slightly| lower at 52.4|¬∞F (11.|3¬∞C)| due to the wind chill effect|.| Adding in memory‚Äã As mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in a checkpointer. When passing in a checkpointer, we also have to pass in a thread_id when invoking the agent (so it knows which thread/conversation to resume from). from langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()API Reference:MemorySaver agent_executor = create_react_agent(model, tools, checkpointer=memory)config = {\"configurable\": {\"thread_id\": \"abc123\"}} input_message = {\"role\": \"user\", \"content\": \"Hi, I'm Bob!\"}for step in agent_executor.stream( {\"messages\": [input_message]}, config, stream_mode=\"values\"): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================Hi, I'm Bob!==================================\u001b[1m Ai Message \u001b[0m==================================Hello Bob! I'm an AI assistant who can help you search for information using specialized search tools. Is there anything specific you'd like to know about or search for? I'm happy to help you find accurate and up-to-date information on various topics. input_message = {\"role\": \"user\", \"content\": \"What's my name?\"}for step in agent_executor.stream( {\"messages\": [input_message]}, config, stream_mode=\"values\"): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================What's my name?==================================\u001b[1m Ai Message \u001b[0m==================================Your name is Bob, as you introduced yourself earlier. I can remember information shared within our conversation without needing to search for it. Example LangSmith trace If you want to start a new conversation, all you have to do is change the thread_id used config = {\"configurable\": {\"thread_id\": \"xyz123\"}}input_message = {\"role\": \"user\", \"content\": \"What's my name?\"}for step in agent_executor.stream( {\"messages\": [input_message]}, config, stream_mode=\"values\"): step[\"messages\"][-1].pretty_print() ================================\u001b[1m Human Message \u001b[0m=================================What's my name?==================================\u001b[1m Ai Message \u001b[0m==================================I apologize, but I don't have access to any tools that would tell me your name. I can only assist you with searching for publicly available information using the tavily_search function. I don't have access to personal information about users. If you'd like to tell me your name, I'll be happy to address you by it. Conclusion‚Äã That's a wrap! In this quick start we covered how to create a simple agent. We've then shown how to stream back a response - not only with the intermediate steps, but also tokens! We've also added in memory so you can have a conversation with them. Agents are a complex topic with lots to learn! For more information on Agents, please check out the LangGraph documentation. This has it's own set of concepts, tutorials, and how-to guides.End-to-end agentSetupJupyter NotebookInstallationLangSmithTavilyDefine toolsUsing Language ModelsCreate the agentRun the agentStreaming MessagesStreaming tokensAdding in memoryConclusion",
      "timestamp": "2025-08-24 12:50:40"
    },
    {
      "url": "https://python.langchain.com/docs/integrations/vectorstores/",
      "title": "Vector stores | ü¶úÔ∏èüîó LangChain",
      "content": "Vector stores | ü¶úÔ∏èüîó LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page A vector store stores embedded data and performs similarity search. Select embedding model: Select embeddings model:OpenAI‚ñæOpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakepip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"): os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") Select vector store: Select vector store:In-memory‚ñæIn-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings) VectorstoreDelete by IDFilteringSearch by VectorSearch with scoreAsyncPasses Standard TestsMulti TenancyIDs in add DocumentsAstraDBVectorStore‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚ùå‚ùå‚úÖChroma‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖClickhouse‚úÖ‚úÖ‚ùå‚úÖ‚ùå‚ùå‚ùå‚úÖCouchbaseSearchVectorStore‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚ùå‚úÖ‚úÖDatabricksVectorSearch‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚ùå‚ùå‚úÖElasticsearchStore‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚ùå‚ùå‚úÖFAISS‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚ùå‚ùå‚úÖInMemoryVectorStore‚úÖ‚úÖ‚ùå‚úÖ‚úÖ‚ùå‚ùå‚úÖMilvus‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖMongoDBAtlasVectorSearch‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚ùå‚ùå‚úÖopenGauss‚úÖ‚úÖ‚úÖ‚úÖ‚ùå‚úÖ‚ùå‚úÖPGVector‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚ùå‚ùå‚úÖPGVectorStore‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚ùå‚úÖPineconeVectorStore‚úÖ‚úÖ‚úÖ‚ùå‚úÖ‚ùå‚ùå‚úÖQdrantVectorStore‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚ùå‚úÖ‚úÖRedis‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚ùå‚ùå‚úÖWeaviate‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚ùå‚úÖ‚úÖSQLServer‚úÖ‚úÖ‚úÖ‚úÖ‚ùå‚ùå‚ùå‚úÖ All Vectorstores‚Äã NameDescriptionActiveloop Deep LakeActiveloop Deep Lake as a Multi-Modal Vector Store that stores embedd...AerospikeAerospike Vector Search (AVS) is anAlibaba Cloud OpenSearchAlibaba Cloud Opensearch is a one-stop platform to develop intelligen...AnalyticDBAnalyticDB for PostgreSQL is a massively parallel processing (MPP) da...AnnoyAnnoy (Approximate Nearest Neighbors Oh Yeah) is a C++ library with P...Apache DorisApache Doris is a modern data warehouse for real-time analytics.ApertureDBApertureDB is a database that stores, indexes, and manages multi-moda...Astra DB Vector StoreThis page provides a quickstart for using Astra DB as a Vector Store.AtlasAtlas is a platform by Nomic made for interacting with both small and...AwaDBAwaDB is an AI Native database for the search and storage of embeddin...Azure Cosmos DB Mongo vCoreThis notebook shows you how to leverage this integrated vector databa...Azure Cosmos DB No SQLThis notebook shows you how to leverage this integrated vector databa...Azure AI SearchAzure AI Search (formerly known as Azure Search and Azure Cognitive S...BagelBagel (Open Inference platform for AI), is like GitHub for AI data.BagelDBBagelDB (Open Vector Database for AI), is like GitHub for AI data.Baidu Cloud ElasticSearch VectorSearchBaidu Cloud VectorSearch is a fully managed, enterprise-level distrib...Baidu VectorDBBaidu VectorDB is a robust, enterprise-level distributed database ser...Apache CassandraThis page provides a quickstart for using Apache Cassandra¬Æ as a Vect...ChromaThis notebook covers how to get started with the Chroma vector store.ClarifaiClarifai is an AI Platform that provides the full AI lifecycle rangin...ClickHouseClickHouse is the fastest and most resource efficient open-source dat...CloudflareVectorizeThis notebook covers how to get started with the CloudflareVectorize ...CouchbaseCouchbase is an award-winning distributed NoSQL cloud database that d...DashVectorDashVector is a fully-managed vectorDB service that supports high-dim...DatabricksDatabricks Vector Search is a serverless similarity search engine tha...IBM Db2 Vector Store and Vector SearchLangChain's Db2 integration (langchain-db2) provides vector store and...DingoDBDingoDB is a distributed multi-mode vector database, which combines t...DocArray HnswSearchDocArrayHnswSearch is a lightweight Document Index implementation pro...DocArray InMemorySearchDocArrayInMemorySearch is a document index provided by Docarray that ...Amazon Document DBAmazon DocumentDB (with MongoDB Compatibility) makes it easy to set u...DuckDBThis notebook shows how to use DuckDB as a vector store.China Mobile ECloud ElasticSearch VectorSearchChina Mobile ECloud VectorSearch is a fully managed, enterprise-level...ElasticsearchElasticsearch is a distributed, RESTful search and analytics engine, ...EpsillaEpsilla is an open-source vector database that leverages the advanced...FaissFacebook AI Similarity Search (FAISS) is a library for efficient simi...Faiss (Async)Facebook AI Similarity Search (Faiss) is a library for efficient simi...FalkorDBVectorStoreFalkorDB is an open-source graph database with integrated support for...GelAn implementation of LangChain vectorstore abstraction using gel as t...Google AlloyDB for PostgreSQLAlloyDB is a fully managed relational database service that offers hi...Google BigQuery Vector SearchGoogle Cloud BigQuery Vector Search lets you use GoogleSQL to do sema...Google Cloud SQL for MySQLCloud SQL is a fully managed relational database service that offers ...Google Cloud SQL for PostgreSQLCloud SQL is a fully managed relational database service that offers ...FirestoreFirestore is a serverless document-oriented database that scales to m...Google Memorystore for RedisGoogle Memorystore for Redis is a fully-managed service that is power...Google SpannerSpanner is a highly scalable database that combines unlimited scalabi...Google Vertex AI Feature StoreGoogle Cloud Vertex Feature Store streamlines your ML feature managem...Google Vertex AI Vector SearchThis notebook shows how to use functionality related to the Google Cl...HippoTranswarp Hippo is an enterprise-level cloud-native distributed vecto...HologresHologres is a unified real-time data warehousing service developed by...InfinispanInfinispan is an open-source key-value data grid, it can work as sing...Jaguar Vector Database1. It is a distributed vector databaseKDB.AIKDB.AI is a powerful knowledge-based vector database and search engin...KineticaKinetica is a database with integrated support for vector similarity ...LanceDBLanceDB is an open-source database for vector-search built with persi...LanternLantern is an open-source vector similarity search for PostgresLindormThis notebook covers how to get started with the Lindorm vector store.LLMRailsLLMRails is a API platform for building GenAI applications. It provid...ManticoreSearch VectorStoreManticoreSearch is an open-source search engine that offers fast, sca...MariaDBLangChain's MariaDB integration (langchain-mariadb) provides vector c...MarqoThis notebook shows how to use functionality related to the Marqo vec...MeilisearchMeilisearch is an open-source, lightning-fast, and hyper relevant sea...Amazon MemoryDBVector Search introduction and langchain integration guide.MilvusMilvus is a database that stores, indexes, and manages massive embedd...Momento Vector Index (MVI)MVI: the most productive, easiest to use, serverless vector index for...MongoDB AtlasThis notebook covers how to MongoDB Atlas vector search in LangChain,...MyScaleMyScale is a cloud-based database optimized for AI applications and s...Neo4j Vector IndexNeo4j is an open-source graph database with integrated support for ve...NucliaDBYou can use a local NucliaDB instance or use Nuclia Cloud.OceanbaseThis notebook covers how to get started with the Oceanbase vector sto...openGaussThis notebook covers how to get started with the openGauss VectorStor...OpenSearchOpenSearch is a scalable, flexible, and extensible open-source softwa...Oracle AI Vector Search: Vector StoreOracle AI Vector Search is designed for Artificial Intelligence (AI) ...PathwayPathway is an open data processing framework. It allows you to easily...Postgres EmbeddingPostgres Embedding is an open-source vector similarity search for Pos...PGVecto.rsThis notebook shows how to use functionality related to the Postgres ...PGVectorAn implementation of LangChain vectorstore abstraction using postgres...PGVectorStorePGVectorStore is an implementation of a LangChain vectorstore using p...PineconePinecone is a vector database with broad functionality.Pinecone (sparse)Pinecone is a vector database with broad functionality.QdrantQdrant (read: quadrant) is a vector similarity search engine. It prov...RedisThis notebook covers how to get started with the Redis vector store.RelytRelyt is a cloud native data warehousing service that is designed to ...RocksetRockset is a real-time search and analytics database built for the cl...SAP HANA Cloud Vector EngineSAP HANA Cloud Vector Engine is a vector store fully integrated into ...ScaNNScaNN (Scalable Nearest Neighbors) is a method for efficient vector s...SemaDBSemaDB from SemaFind is a no fuss vector similarity database for buil...SingleStoreSingleStore is a robust, high-performance distributed SQL database so...scikit-learnscikit-learn is an open-source collection of machine learning algorit...SQLiteVecThis notebook covers how to get started with the SQLiteVec vector sto...SQLite-VSSSQLite-VSS is an SQLite extension designed for vector search, emphasi...SQLServerAzure SQL provides a dedicated¬†Vector data type that simplifies the c...StarRocksStarRocks is a High-Performance Analytical Database.Supabase (Postgres)Supabase is an open-source Firebase alternative. Supabase is built on...SurrealDBVectorStoreSurrealDB is a unified, multi-model database purpose-built for AI sys...TablestoreTablestore is a fully managed NoSQL cloud database service.TairTair is a cloud native in-memory database service developed by Alibab...Tencent Cloud VectorDBTencent Cloud VectorDB is a fully managed, self-developed, enterprise...ThirdAI NeuralDBNeuralDB is a CPU-friendly and fine-tunable vector store developed by...TiDB VectorTiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution...TigrisTigris is an open-source Serverless NoSQL Database and Search Platfor...TileDBTileDB is a powerful engine for indexing and querying dense and spars...Timescale Vector (Postgres)Timescale Vector is PostgreSQL++ vector database for AI applications.TypesenseTypesense is an open-source, in-memory search engine, that you can ei...Upstash VectorUpstash Vector is a serverless vector database designed for working w...USearchUSearch is a Smaller & Faster Single-File Vector Search EngineValdVald is a highly scalable distributed fast approximate nearest neighb...VDMSThis notebook covers how to get started with VDMS as a vector store.VearchVearch is the vector search infrastructure for deeping learning and A...VectaraVectara is the trusted AI Assistant and Agent platform which focuses ...VespaVespa is a fully featured search engine and vector database. It suppo...viking DBviking DB is a database that stores, indexes, and manages massive emb...vliteVLite is a simple and blazing fast vector database that allows you to...WeaviateThis notebook covers how to get started with the Weaviate vector stor...XataXata is a serverless data platform, based on PostgreSQL. It provides ...YDBYDB is a versatile open source Distributed SQL Database that combines...YellowbrickYellowbrick is an elastic, massively parallel processing (MPP) SQL da...ZepRecall, understand, and extract data from chat histories. Power perso...Zep CloudRecall, understand, and extract data from chat histories. Power perso...ZillizZilliz Cloud is a fully managed service on cloud for LF AI Milvus¬Æ,All Vectorstores",
      "timestamp": "2025-08-24 12:50:44"
    },
    {
      "url": "https://python.langchain.com/docs/tutorials/llm_chain/",
      "title": "Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain",
      "content": "Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call! After reading this tutorial, you'll have a high level overview of: Using language models Using prompt templates Debugging and tracing your application using LangSmith Let's dive in! Setup‚Äã Jupyter Notebook‚Äã This and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install. Installation‚Äã To install LangChain run: PipCondapip install langchainconda install langchain -c conda-forge For more details, see our Installation guide. LangSmith‚Äã Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. After you sign up at the link above, make sure to set your environment variables to start logging traces: export LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"export LANGSMITH_PROJECT=\"default\" # or any other project name Or, if in a notebook, you can set them with: import getpassimport ostry: # load environment variables from .env file (requires `python-dotenv`) from dotenv import load_dotenv load_dotenv()except ImportError: passos.environ[\"LANGSMITH_TRACING\"] = \"true\"if \"LANGSMITH_API_KEY\" not in os.environ: os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass( prompt=\"Enter your LangSmith API key (optional): \" )if \"LANGSMITH_PROJECT\" not in os.environ: os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass( prompt='Enter your LangSmith Project Name (default = \"default\"): ' ) if not os.environ.get(\"LANGSMITH_PROJECT\"): os.environ[\"LANGSMITH_PROJECT\"] = \"default\" Using Language Models‚Äã First up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations. Select chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") Let's first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method. from langchain_core.messages import HumanMessage, SystemMessagemessages = [ SystemMessage(\"Translate the following from English into Italian\"), HumanMessage(\"hi!\"),]model.invoke(messages)API Reference:HumanMessage | SystemMessage AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-32654a56-627c-40e1-a141-ad9350bbfd3e-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}) tipIf we've enabled LangSmith, we can see that this run is logged to LangSmith, and can see the LangSmith trace. The LangSmith trace reports token usage information, latency, standard model parameters (such as temperature), and other information. Note that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts. LangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent: model.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")]) Streaming‚Äã Because chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model: for token in model.stream(messages): print(token.content, end=\"|\") |C|iao|!|| You can find more details on streaming chat model outputs in this guide. Prompt Templates‚Äã Right now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input. Prompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model. Let's create a prompt template here. It will take in two user variables: language: The language to translate text into text: The text to translate from langchain_core.prompts import ChatPromptTemplatesystem_template = \"Translate the following from English into {language}\"prompt_template = ChatPromptTemplate.from_messages( [(\"system\", system_template), (\"user\", \"{text}\")])API Reference:ChatPromptTemplate Note that ChatPromptTemplate supports multiple message roles in a single template. We format the language parameter into the system message, and the user text into a user message. The input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})prompt ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]) We can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do: prompt.to_messages() [SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})] Finally, we can invoke the chat model on the formatted prompt: response = model.invoke(prompt)print(response.content) Ciao! tipMessage content can contain both text and content blocks with additional structure. See this guide for more information. If we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information. Conclusion‚Äã That's it! In this tutorial you've learned how to create your first simple LLM application. You've learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith. This just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we've got a lot of other resources! For further reading on the core concepts of LangChain, we've got detailed Conceptual Guides. If you have more specific questions on these concepts, check out the following sections of the how-to guides: Chat models Prompt templates And the LangSmith docs: LangSmith SetupJupyter NotebookInstallationLangSmithUsing Language ModelsStreamingPrompt TemplatesConclusion",
      "timestamp": "2025-08-24 12:50:48"
    },
    {
      "url": "https://python.langchain.com/docs/versions/v0_3/",
      "title": "LangChain v0.3 | ü¶úÔ∏èüîó LangChain",
      "content": "LangChain v0.3 | ü¶úÔ∏èüîó LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Last updated: 09.16.24 What's changed‚Äã All packages have been upgraded from Pydantic 1 to Pydantic 2 internally. Use of Pydantic 2 in user code is fully supported with all packages without the need for bridges like langchain_core.pydantic_v1 or pydantic.v1. Pydantic 1 will no longer be supported as it reached its end-of-life in June 2024. Python 3.8 will no longer be supported as its end-of-life is October 2024. These are the only breaking changes. What's new‚Äã The following features have been added during the development of 0.2.x: Moved more integrations from langchain-community to their own langchain-x packages. This is a non-breaking change, as the legacy implementations are left in langchain-community and marked as deprecated. This allows us to better manage the dependencies of, test, and version these integrations. You can see all the latest integration packages in the API reference. Simplified tool definition and usage. Read more here. Added utilities for interacting with chat models: universal model constructor, rate limiter, message utilities, Added the ability to dispatch custom events. Revamped integration docs and API reference. Read more here. Marked as deprecated a number of legacy chains and added migration guides for all of them. These are slated for removal in langchain 1.0.0. See the deprecated chains and associated migration guides here. How to update your code‚Äã If you're using langchain / langchain-community / langchain-core 0.0 or 0.1, we recommend that you first upgrade to 0.2. If you're using langgraph, upgrade to langgraph>=0.2.20,<0.3. This will work with either 0.2 or 0.3 versions of all the base packages. Here is a complete list of all packages that have been released and what we recommend upgrading your version constraints to. Any package that now requires langchain-core 0.3 had a minor version bump. Any package that is now compatible with both langchain-core 0.2 and 0.3 had a patch version bump. You can use the langchain-cli to update deprecated imports automatically. The CLI will handle updating deprecated imports that were introduced in LangChain 0.0.x and LangChain 0.1, as well as updating the langchain_core.pydantic_v1 and langchain.pydantic_v1 imports. Base packages‚Äã PackageLatestRecommended constraintlangchain0.3.0>=0.3,<0.4langchain-community0.3.0>=0.3,<0.4langchain-text-splitters0.3.0>=0.3,<0.4langchain-core0.3.0>=0.3,<0.4langchain-experimental0.3.0>=0.3,<0.4 Downstream packages‚Äã PackageLatestRecommended constraintlanggraph0.2.20>=0.2.20,<0.3langserve0.3.0>=0.3,<0.4 Integration packages‚Äã PackageLatestRecommended constraintlangchain-ai210.2.0>=0.2,<0.3langchain-aws0.2.0>=0.2,<0.3langchain-anthropic0.2.0>=0.2,<0.3langchain-astradb0.4.1>=0.4.1,<0.5langchain-azure-dynamic-sessions0.2.0>=0.2,<0.3langchain-box0.2.0>=0.2,<0.3langchain-chroma0.1.4>=0.1.4,<0.2langchain-cohere0.3.0>=0.3,<0.4langchain-elasticsearch0.3.0>=0.3,<0.4langchain-exa0.2.0>=0.2,<0.3langchain-fireworks0.2.0>=0.2,<0.3langchain-groq0.2.0>=0.2,<0.3langchain-google-community2.0.0>=2,<3langchain-google-genai2.0.0>=2,<3langchain-google-vertexai2.0.0>=2,<3langchain-huggingface0.1.0>=0.1,<0.2langchain-ibm0.3.0>=0.3,<0.4langchain-milvus0.1.6>=0.1.6,<0.2langchain-mistralai0.2.0>=0.2,<0.3langchain-mongodb0.2.0>=0.2,<0.3langchain-nomic0.1.3>=0.1.3,<0.2langchain-nvidia0.3.0>=0.3,<0.4langchain-ollama0.2.0>=0.2,<0.3langchain-openai0.2.0>=0.2,<0.3langchain-pinecone0.2.0>=0.2,<0.3langchain-postgres0.0.13>=0.0.13,<0.1langchain-prompty0.1.0>=0.1,<0.2langchain-qdrant0.1.4>=0.1.4,<0.2langchain-redis0.1.0>=0.1,<0.2langchain-sema40.2.0>=0.2,<0.3langchain-together0.2.0>=0.2,<0.3langchain-unstructured0.1.4>=0.1.4,<0.2langchain-upstage0.3.0>=0.3,<0.4langchain-voyageai0.2.0>=0.2,<0.3langchain-weaviate0.0.3>=0.0.3,<0.1 Once you've updated to recent versions of the packages, you may need to address the following issues stemming from the internal switch from Pydantic v1 to Pydantic v2: If your code depends on Pydantic aside from LangChain, you will need to upgrade your pydantic version constraints to be pydantic>=2,<3. See Pydantic's migration guide for help migrating your non-LangChain code to Pydantic v2 if you use pydantic v1. There are a number of side effects to LangChain components caused by the internal switch from Pydantic v1 to v2. We have listed some of the common cases below together with the recommended solutions. Common issues when transitioning to Pydantic 2‚Äã 1. Do not use the langchain_core.pydantic_v1 namespace‚Äã Replace any usage of langchain_core.pydantic_v1 or langchain.pydantic_v1 with direct imports from pydantic. For example, from langchain_core.pydantic_v1 import BaseModel to: from pydantic import BaseModel This may require you to make additional updates to your Pydantic code given that there are a number of breaking changes in Pydantic 2. See the Pydantic Migration for how to upgrade your code from Pydantic 1 to 2. 2. Passing Pydantic objects to LangChain APIs‚Äã Users using the following APIs: BaseChatModel.bind_tools BaseChatModel.with_structured_output Tool.from_function StructuredTool.from_function should ensure that they are passing Pydantic 2 objects to these APIs rather than Pydantic 1 objects (created via the pydantic.v1 namespace of pydantic 2). cautionWhile v1 objects may be accepted by some of these APIs, users are advised to use Pydantic 2 objects to avoid future issues. 3. Sub-classing LangChain models‚Äã Any sub-classing from existing LangChain models (e.g., BaseTool, BaseChatModel, LLM) should upgrade to use Pydantic 2 features. For example, any user code that's relying on Pydantic 1 features (e.g., validator) should be updated to the Pydantic 2 equivalent (e.g., field_validator), and any references to pydantic.v1, langchain_core.pydantic_v1, langchain.pydantic_v1 should be replaced with imports from pydantic. from pydantic.v1 import validator, Field # if pydantic 2 is installed# from pydantic import validator, Field # if pydantic 1 is installed# from langchain_core.pydantic_v1 import validator, Field# from langchain.pydantic_v1 import validator, Fieldclass CustomTool(BaseTool): # BaseTool is v1 code x: int = Field(default=1) def _run(*args, **kwargs): return \"hello\" @validator('x') # v1 code @classmethod def validate_x(cls, x: int) -> int: return 1 Should change to: from pydantic import Field, field_validator # pydantic v2from langchain_core.pydantic_v1 import BaseToolclass CustomTool(BaseTool): # BaseTool is v1 code x: int = Field(default=1) def _run(*args, **kwargs): return \"hello\" @field_validator('x') # v2 code @classmethod def validate_x(cls, x: int) -> int: return 1CustomTool( name='custom_tool', description=\"hello\", x=1,) 4. model_rebuild()‚Äã When sub-classing from LangChain models, users may need to add relevant imports to the file and rebuild the model. You can read more about model_rebuild here. from langchain_core.output_parsers import BaseOutputParserclass FooParser(BaseOutputParser): ...API Reference:BaseOutputParser New code: from typing import Optional as Optionalfrom langchain_core.output_parsers import BaseOutputParserclass FooParser(BaseOutputParser): ...FooParser.model_rebuild()API Reference:BaseOutputParser Migrate using langchain-cli‚Äã The langchain-cli can help update deprecated LangChain imports in your code automatically. Please note that the langchain-cli only handles deprecated LangChain imports and cannot help to upgrade your code from pydantic 1 to pydantic 2. For help with the Pydantic 1 to 2 migration itself please refer to the Pydantic Migration Guidelines. As of 0.0.31, the langchain-cli relies on gritql for applying code mods. Installation‚Äã pip install -U langchain-clilangchain-cli --version # <-- Make sure the version is at least 0.0.31 Usage‚Äã Given that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like git). The langchain-cli will handle the langchain_core.pydantic_v1 deprecation introduced in LangChain 0.3 as well as older deprecations (e.g.,from langchain.chat_models import ChatOpenAI which should be from langchain_openai import ChatOpenAI), You will need to run the migration script twice as it only applies one import replacement per run. For example, say that your code is still using the old import from langchain.chat_models import ChatOpenAI: After the first run, you'll get: from langchain_community.chat_models import ChatOpenAI After the second run, you'll get: from langchain_openai import ChatOpenAI # Run a first time# Will replace from langchain.chat_models import ChatOpenAIlangchain-cli migrate --help [path to code] # Helplangchain-cli migrate [path to code] # Apply# Run a second time to apply more import replacementslangchain-cli migrate --diff [path to code] # Previewlangchain-cli migrate [path to code] # Apply Other options‚Äã # See help menulangchain-cli migrate --help# Preview Changes without applyinglangchain-cli migrate --diff [path to code]# Approve changes interactivelylangchain-cli migrate --interactive [path to code]What's changedWhat's newHow to update your codeBase packagesDownstream packagesIntegration packagesCommon issues when transitioning to Pydantic 21. Do not use the langchain_core.pydantic_v1 namespace2. Passing Pydantic objects to LangChain APIs3. Sub-classing LangChain models4. model_rebuild()Migrate using langchain-cliInstallationUsageOther options",
      "timestamp": "2025-08-24 12:50:53"
    },
    {
      "url": "https://python.langchain.com/api_reference/",
      "title": "LangChain Python API Reference ‚Äî ü¶úüîó LangChain  documentation",
      "content": "LangChain Python API Reference ‚Äî ü¶úüîó LangChain documentation Skip to main content Back to top Ctrl+K Docs Light Dark System Settings GitHub X / Twitter LangChain Python API Reference# Welcome to the LangChain Python API reference. This is a reference for all langchain-x packages. For user guides see https://python.langchain.com. For the legacy API reference hosted on ReadTheDocs see https://api.python.langchain.com/. Base packages# Core langchain-core: 0.3.74 core/index.html Langchain langchain: 0.3.27 langchain/index.html Text Splitters langchain-text-splitters: 0.3.9 text_splitters/index.html Community langchain-community: 0.3.27 community/index.html Experimental langchain-experimental: 0.3.5rc1 experimental/index.html Standard Tests langchain-tests: 0.3.20 standard_tests/index.html Integrations# OpenAI langchain-openai 0.3.31 openai/index.html Anthropic langchain-anthropic 0.3.19 anthropic/index.html Google VertexAI langchain-google-vertexai 2.0.28 google_vertexai/index.html AWS langchain-aws 0.2.31 aws/index.html Huggingface langchain-huggingface 0.3.1 huggingface/index.html MistralAI langchain-mistralai 0.2.11 mistralai/index.html See the full list of integrations in the Section Navigation. On this page",
      "timestamp": "2025-08-24 12:50:57"
    },
    {
      "url": "https://python.langchain.com/docs/concepts/architecture/",
      "title": "Architecture | ü¶úÔ∏èüîó LangChain",
      "content": "Architecture | ü¶úÔ∏èüîó LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page LangChain is a framework that consists of a number of packages. langchain-core‚Äã This package contains base abstractions for different components and ways to compose them together. The interfaces for core components like chat models, vector stores, tools and more are defined here. No third-party integrations are defined here. The dependencies are kept purposefully very lightweight. langchain‚Äã The main langchain package contains chains and retrieval strategies that make up an application's cognitive architecture. These are NOT third-party integrations. All chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations. Integration packages‚Äã Popular integrations have their own packages (e.g. langchain-openai, langchain-anthropic, etc) so that they can be properly versioned and appropriately lightweight. For more information see: A list integrations packages The API Reference where you can find detailed information about each of the integration package. langchain-community‚Äã This package contains third-party integrations that are maintained by the LangChain community. Key integration packages are separated out (see above). This contains integrations for various components (chat models, vector stores, tools, etc). All dependencies in this package are optional to keep the package as lightweight as possible. langgraph‚Äã langgraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. LangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows. Further reading See our LangGraph overview here. See our LangGraph Academy Course here. langserve‚Äã A package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running. importantLangServe is designed to primarily deploy simple Runnables and work with well-known primitives in langchain-core.If you need a deployment option for LangGraph, you should instead be looking at LangGraph Platform (beta) which will be better suited for deploying LangGraph applications. For more information, see the LangServe documentation. LangSmith‚Äã A developer platform that lets you debug, test, evaluate, and monitor LLM applications. For more information, see the LangSmith documentationlangchain-corelangchainIntegration packageslangchain-communitylanggraphlangserveLangSmith",
      "timestamp": "2025-08-24 12:51:01"
    },
    {
      "url": "https://python.langchain.com/docs/tutorials/chatbot/",
      "title": "Build a Chatbot | ü¶úÔ∏èüîó LangChain",
      "content": "Build a Chatbot | ü¶úÔ∏èüîó LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page noteThis tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.Please see How to migrate to LangGraph Memory for more details. Overview‚Äã We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions with a chat model. Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for: Conversational RAG: Enable a chatbot experience over an external source of data Agents: Build a chatbot that can take actions This tutorial will cover the basics which will be helpful for those two more advanced topics, but feel free to skip directly to there should you choose. Setup‚Äã Jupyter Notebook‚Äã This guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them. This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install. Installation‚Äã For this tutorial we will need langchain-core and langgraph. This guide requires langgraph >= 0.2.28. PipCondapip install langchain-core langgraph>0.2.27conda install langchain-core langgraph>0.2.27 -c conda-forge For more details, see our Installation guide. LangSmith‚Äã Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. After you sign up at the link above, (you'll need to create an API key from the Settings -> API Keys page on the LangSmith website), make sure to set your environment variables to start logging traces: export LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\" Or, if in a notebook, you can set them with: import getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass() Quickstart‚Äã First up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably - select the one you want to use below! Select chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") Let's first use the model directly. ChatModels are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the .invoke method. from langchain_core.messages import HumanMessagemodel.invoke([HumanMessage(content=\"Hi! I'm Bob\")])API Reference:HumanMessage AIMessage(content='Hi Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-5211544f-da9f-4325-8b8e-b3d92b2fc71a-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}) The model on its own does not have any concept of state. For example, if you ask a followup question: model.invoke([HumanMessage(content=\"What's my name?\")]) AIMessage(content=\"I'm sorry, but I don't have access to personal information about users unless it has been shared with me in the course of our conversation. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 11, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-a2d13a18-7022-4784-b54f-f85c097d1075-0', usage_metadata={'input_tokens': 11, 'output_tokens': 34, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}) Let's take a look at the example LangSmith trace We can see that it doesn't take the previous conversation turn into context, and cannot answer the question. This makes for a terrible chatbot experience! To get around this, we need to pass the entire conversation history into the model. Let's see what happens when we do that: from langchain_core.messages import AIMessagemodel.invoke( [ HumanMessage(content=\"Hi! I'm Bob\"), AIMessage(content=\"Hello Bob! How can I assist you today?\"), HumanMessage(content=\"What's my name?\"), ])API Reference:AIMessage AIMessage(content='Your name is Bob! How can I help you today, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 33, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-34bcccb3-446e-42f2-b1de-52c09936c02c-0', usage_metadata={'input_tokens': 33, 'output_tokens': 14, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}) And now we can see that we get a good response! This is the basic idea underpinning a chatbot's ability to interact conversationally. So how do we best implement this? Message persistence‚Äã LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns. Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications. LangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres). from langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import START, MessagesState, StateGraph# Define a new graphworkflow = StateGraph(state_schema=MessagesState)# Define the function that calls the modeldef call_model(state: MessagesState): response = model.invoke(state[\"messages\"]) return {\"messages\": response}# Define the (single) node in the graphworkflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)# Add memorymemory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:MemorySaver | StateGraph We now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a thread_id. This should look like: config = {\"configurable\": {\"thread_id\": \"abc123\"}} This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users. We can then invoke the application: query = \"Hi! I'm Bob.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print() # output contains all messages in state ==================================\u001b[1m Ai Message \u001b[0m==================================Hi Bob! How can I assist you today? query = \"What's my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print() ==================================\u001b[1m Ai Message \u001b[0m==================================Your name is Bob! How can I help you today, Bob? Great! Our chatbot now remembers things about us. If we change the config to reference a different thread_id, we can see that it starts the conversation fresh. config = {\"configurable\": {\"thread_id\": \"abc234\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print() ==================================\u001b[1m Ai Message \u001b[0m==================================I'm sorry, but I don't have access to personal information about you unless you've shared it in this conversation. How can I assist you today? However, we can always go back to the original conversation (since we are persisting it in a database) config = {\"configurable\": {\"thread_id\": \"abc123\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print() ==================================\u001b[1m Ai Message \u001b[0m==================================Your name is Bob. What would you like to discuss today? This is how we can support a chatbot having conversations with many users! tipFor async support, update the call_model node to be an async function and use .ainvoke when invoking the application:# Async function for node:async def call_model(state: MessagesState): response = await model.ainvoke(state[\"messages\"]) return {\"messages\": response}# Define graph as before:workflow = StateGraph(state_schema=MessagesState)workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)app = workflow.compile(checkpointer=MemorySaver())# Async invocation:output = await app.ainvoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print() Right now, all we've done is add a simple persistence layer around the model. We can start to make the chatbot more complicated and personalized by adding in a prompt template. Prompt templates‚Äã Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages. To add in a system message, we will create a ChatPromptTemplate. We will utilize MessagesPlaceholder to pass all the messages in. from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderprompt_template = ChatPromptTemplate.from_messages( [ ( \"system\", \"You talk like a pirate. Answer all questions to the best of your ability.\", ), MessagesPlaceholder(variable_name=\"messages\"), ])API Reference:ChatPromptTemplate | MessagesPlaceholder We can now update our application to incorporate this template: workflow = StateGraph(state_schema=MessagesState)def call_model(state: MessagesState): prompt = prompt_template.invoke(state) response = model.invoke(prompt) return {\"messages\": response}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory) We invoke the application in the same way: config = {\"configurable\": {\"thread_id\": \"abc345\"}}query = \"Hi! I'm Jim.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print() ==================================\u001b[1m Ai Message \u001b[0m==================================Ahoy there, Jim! What brings ye to these waters today? Be ye seekin' treasure, knowledge, or perhaps a good tale from the high seas? Arrr! query = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print() ==================================\u001b[1m Ai Message \u001b[0m==================================Ye be called Jim, matey! A fine name fer a swashbuckler such as yerself! What else can I do fer ye? Arrr! Awesome! Let's now make our prompt a little bit more complicated. Let's assume that the prompt template now looks something like this: prompt_template = ChatPromptTemplate.from_messages( [ ( \"system\", \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\", ), MessagesPlaceholder(variable_name=\"messages\"), ]) Note that we have added a new language input to the prompt. Our application now has two parameters-- the input messages and language. We should update our application's state to reflect this: from typing import Sequencefrom langchain_core.messages import BaseMessagefrom langgraph.graph.message import add_messagesfrom typing_extensions import Annotated, TypedDictclass State(TypedDict): messages: Annotated[Sequence[BaseMessage], add_messages] language: strworkflow = StateGraph(state_schema=State)def call_model(state: State): prompt = prompt_template.invoke(state) response = model.invoke(prompt) return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:BaseMessage | add_messages config = {\"configurable\": {\"thread_id\": \"abc456\"}}query = \"Hi! I'm Bob.\"language = \"Spanish\"input_messages = [HumanMessage(query)]output = app.invoke( {\"messages\": input_messages, \"language\": language}, config,)output[\"messages\"][-1].pretty_print() ==================================\u001b[1m Ai Message \u001b[0m==================================¬°Hola, Bob! ¬øC√≥mo puedo ayudarte hoy? Note that the entire state is persisted, so we can omit parameters like language if no changes are desired: query = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke( {\"messages\": input_messages}, config,)output[\"messages\"][-1].pretty_print() ==================================\u001b[1m Ai Message \u001b[0m==================================Tu nombre es Bob. ¬øHay algo m√°s en lo que pueda ayudarte? To help you understand what's happening internally, check out this LangSmith trace. Managing Conversation History‚Äã One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in. Importantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History. We can do this by adding a simple step in front of the prompt that modifies the messages key appropriately, and then wrap that new chain in the Message History class. LangChain comes with a few built-in helpers for managing a list of messages. In this case we'll use the trim_messages helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages: from langchain_core.messages import SystemMessage, trim_messagestrimmer = trim_messages( max_tokens=65, strategy=\"last\", token_counter=model, include_system=True, allow_partial=False, start_on=\"human\",)messages = [ SystemMessage(content=\"you're a good assistant\"), HumanMessage(content=\"hi! I'm bob\"), AIMessage(content=\"hi!\"), HumanMessage(content=\"I like vanilla ice cream\"), AIMessage(content=\"nice\"), HumanMessage(content=\"whats 2 + 2\"), AIMessage(content=\"4\"), HumanMessage(content=\"thanks\"), AIMessage(content=\"no problem!\"), HumanMessage(content=\"having fun?\"), AIMessage(content=\"yes!\"),]trimmer.invoke(messages)API Reference:SystemMessage | trim_messages [SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}), HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}), AIMessage(content='4', additional_kwargs={}, response_metadata={}), HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}), AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}), HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}), AIMessage(content='yes!', additional_kwargs={}, response_metadata={})] To use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt. workflow = StateGraph(state_schema=State)def call_model(state: State): print(f\"Messages before trimming: {len(state['messages'])}\") trimmed_messages = trimmer.invoke(state[\"messages\"]) print(f\"Messages after trimming: {len(trimmed_messages)}\") print(\"Remaining messages:\") for msg in trimmed_messages: print(f\" {type(msg).__name__}: {msg.content}\") prompt = prompt_template.invoke( {\"messages\": trimmed_messages, \"language\": state[\"language\"]} ) response = model.invoke(prompt) return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory) Now if we try asking the model our name, it won't know it since we trimmed that part of the chat history. (By defining our trim stragegy as 'last', we are only keeping the most recent messages that fit within the max_tokens.) config = {\"configurable\": {\"thread_id\": \"abc567\"}}query = \"What is my name?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke( {\"messages\": input_messages, \"language\": language}, config,)output[\"messages\"][-1].pretty_print() Messages before trimming: 12Messages after trimming: 8Remaining messages: SystemMessage: you're a good assistant HumanMessage: whats 2 + 2 AIMessage: 4 HumanMessage: thanks AIMessage: no problem! HumanMessage: having fun? AIMessage: yes! HumanMessage: What is my name?==================================\u001b[1m Ai Message \u001b[0m==================================I don't know your name. If you'd like to share it, feel free! But if we ask about information that is within the last few messages, it remembers: config = {\"configurable\": {\"thread_id\": \"abc678\"}}query = \"What math problem was asked?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke( {\"messages\": input_messages, \"language\": language}, config,)output[\"messages\"][-1].pretty_print() Messages before trimming: 12Messages after trimming: 8Remaining messages: SystemMessage: you're a good assistant HumanMessage: whats 2 + 2 AIMessage: 4 HumanMessage: thanks AIMessage: no problem! HumanMessage: having fun? AIMessage: yes! HumanMessage: What math problem was asked?==================================\u001b[1m Ai Message \u001b[0m==================================The math problem that was asked was \"what's 2 + 2.\" If you take a look at LangSmith, you can see exactly what is happening under the hood in the LangSmith trace. Streaming‚Äã Now we've got a functioning chatbot. However, one really important UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress. It's actually super easy to do this! By default, .stream in our LangGraph application streams application steps-- in this case, the single step of the model response. Setting stream_mode=\"messages\" allows us to stream output tokens instead: config = {\"configurable\": {\"thread_id\": \"abc789\"}}query = \"Hi I'm Todd, please tell me a joke.\"language = \"English\"input_messages = [HumanMessage(query)]for chunk, metadata in app.stream( {\"messages\": input_messages, \"language\": language}, config, stream_mode=\"messages\",): if isinstance(chunk, AIMessage): # Filter to just model responses print(chunk.content, end=\"|\") |Hi| Todd|!| Here|‚Äôs| a| joke| for| you|:|Why| don't| scientists| trust| atoms|?|Because| they| make| up| everything|!|| Next Steps‚Äã Now that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are: Conversational RAG: Enable a chatbot experience over an external source of data Agents: Build a chatbot that can take actions If you want to dive deeper on specifics, some things worth checking out are: Streaming: streaming is crucial for chat applications How to add message history: for a deeper dive into all things related to message history How to manage large message history: more techniques for managing a large chat history LangGraph main docs: for more detail on building with LangGraph OverviewSetupJupyter NotebookInstallationLangSmithQuickstartMessage persistencePrompt templatesManaging Conversation HistoryStreamingNext Steps",
      "timestamp": "2025-08-24 12:51:05"
    },
    {
      "url": "https://python.langchain.com/docs/how_to/",
      "title": "How-to guides | ü¶úÔ∏èüîó LangChain",
      "content": "How-to guides | ü¶úÔ∏èüîó LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Here you‚Äôll find answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions. These guides are goal-oriented and concrete; they're meant to help you complete a specific task. For conceptual explanations see the Conceptual guide. For end-to-end walkthroughs see Tutorials. For comprehensive descriptions of every class and function see the API Reference. Installation‚Äã How to: install LangChain packages How to: use LangChain with different Pydantic versions Key features‚Äã This highlights functionality that is core to using LangChain. How to: return structured data from a model How to: use a model to call tools How to: stream runnables How to: debug your LLM apps Components‚Äã These are the core building blocks you can use when building applications. Chat models‚Äã Chat Models are newer forms of language models that take messages in and output a message. See supported integrations for details on getting started with chat models from a specific provider. How to: initialize any model in one line How to: work with local models How to: do function/tool calling How to: get models to return structured output How to: cache model responses How to: get log probabilities How to: create a custom chat model class How to: stream a response back How to: track token usage How to: track response metadata across providers How to: use chat model to call tools How to: stream tool calls How to: handle rate limits How to: few-shot prompt tool behavior How to: bind model-specific formatted tools How to: force a specific tool call How to: pass multimodal data directly to models Messages‚Äã Messages are the input and output of chat models. They have some content and a role, which describes the source of the message. How to: trim messages How to: filter messages How to: merge consecutive messages of the same type Prompt templates‚Äã Prompt Templates are responsible for formatting user input into a format that can be passed to a language model. How to: use few-shot examples How to: use few-shot examples in chat models How to: partially format prompt templates How to: compose prompts together How to: use multimodal prompts Example selectors‚Äã Example Selectors are responsible for selecting the correct few shot examples to pass to the prompt. How to: use example selectors How to: select examples by length How to: select examples by semantic similarity How to: select examples by semantic ngram overlap How to: select examples by maximal marginal relevance How to: select examples from LangSmith few-shot datasets LLMs‚Äã What LangChain calls LLMs are older forms of language models that take a string in and output a string. How to: cache model responses How to: create a custom LLM class How to: stream a response back How to: track token usage How to: work with local models Output parsers‚Äã Output Parsers are responsible for taking the output of an LLM and parsing into more structured format. How to: parse text from message objects How to: use output parsers to parse an LLM response into structured format How to: parse JSON output How to: parse XML output How to: parse YAML output How to: retry when output parsing errors occur How to: try to fix errors in output parsing How to: write a custom output parser class Document loaders‚Äã Document Loaders are responsible for loading documents from a variety of sources. How to: load PDF files How to: load web pages How to: load CSV data How to: load data from a directory How to: load HTML data How to: load JSON data How to: load Markdown data How to: load Microsoft Office data How to: write a custom document loader Text splitters‚Äã Text Splitters take a document and split into chunks that can be used for retrieval. How to: recursively split text How to: split HTML How to: split by character How to: split code How to: split Markdown by headers How to: recursively split JSON How to: split text into semantic chunks How to: split by tokens Embedding models‚Äã Embedding Models take a piece of text and create a numerical representation of it. See supported integrations for details on getting started with embedding models from a specific provider. How to: embed text data How to: cache embedding results How to: create a custom embeddings class Vector stores‚Äã Vector stores are databases that can efficiently store and retrieve embeddings. See supported integrations for details on getting started with vector stores from a specific provider. How to: use a vector store to retrieve data Retrievers‚Äã Retrievers are responsible for taking a query and returning relevant documents. How to: use a vector store to retrieve data How to: generate multiple queries to retrieve data for How to: use contextual compression to compress the data retrieved How to: write a custom retriever class How to: add similarity scores to retriever results How to: combine the results from multiple retrievers How to: reorder retrieved results to mitigate the \"lost in the middle\" effect How to: generate multiple embeddings per document How to: retrieve the whole document for a chunk How to: generate metadata filters How to: create a time-weighted retriever How to: use hybrid vector and keyword retrieval Indexing‚Äã Indexing is the process of keeping your vectorstore in-sync with the underlying data source. How to: reindex data to keep your vectorstore in sync with the underlying data source Tools‚Äã LangChain Tools contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer here for a list of pre-built tools. How to: create tools How to: use built-in tools and toolkits How to: use chat models to call tools How to: pass tool outputs to chat models How to: pass runtime values to tools How to: add a human-in-the-loop for tools How to: handle tool errors How to: force models to call a tool How to: disable parallel tool calling How to: access the RunnableConfig from a tool How to: stream events from a tool How to: return artifacts from a tool How to: convert Runnables to tools How to: add ad-hoc tool calling capability to models How to: pass in runtime secrets Multimodal‚Äã How to: pass multimodal data directly to models How to: use multimodal prompts Agents‚Äã noteFor in depth how-to guides for agents, please check out LangGraph documentation. How to: use legacy LangChain Agents (AgentExecutor) How to: migrate from legacy LangChain agents to LangGraph Callbacks‚Äã Callbacks allow you to hook into the various stages of your LLM application's execution. How to: pass in callbacks at runtime How to: attach callbacks to a module How to: pass callbacks into a module constructor How to: create custom callback handlers How to: use callbacks in async environments How to: dispatch custom callback events Custom‚Äã All of LangChain components can easily be extended to support your own versions. How to: create a custom chat model class How to: create a custom LLM class How to: create a custom embeddings class How to: write a custom retriever class How to: write a custom document loader How to: write a custom output parser class How to: create custom callback handlers How to: define a custom tool How to: dispatch custom callback events Serialization‚Äã How to: save and load LangChain objects Use cases‚Äã These guides cover use-case specific details. Q&A with RAG‚Äã Retrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data. For a high-level tutorial on RAG, check out this guide. How to: add chat history How to: stream How to: return sources How to: return citations How to: do per-user retrieval Extraction‚Äã Extraction is when you use LLMs to extract structured information from unstructured text. For a high level tutorial on extraction, check out this guide. How to: use reference examples How to: handle long text How to: do extraction without using function calling Chatbots‚Äã Chatbots involve using an LLM to have a conversation. For a high-level tutorial on building chatbots, check out this guide. How to: manage memory How to: do retrieval How to: use tools How to: manage large chat history Query analysis‚Äã Query Analysis is the task of using an LLM to generate a query to send to a retriever. For a high-level tutorial on query analysis, check out this guide. How to: add examples to the prompt How to: handle cases where no queries are generated How to: handle multiple queries How to: handle multiple retrievers How to: construct filters How to: deal with high cardinality categorical variables Q&A over SQL + CSV‚Äã You can use LLMs to do question answering over tabular data. For a high-level tutorial, check out this guide. How to: use prompting to improve results How to: do query validation How to: deal with large databases How to: deal with CSV files Q&A over graph databases‚Äã You can use an LLM to do question answering over graph databases. For a high-level tutorial, check out this guide. How to: add a semantic layer over a database How to: construct knowledge graphs Summarization‚Äã LLMs can summarize and otherwise distill desired information from text, including large volumes of text. For a high-level tutorial, check out this guide. How to: summarize text in a single LLM call How to: summarize text through parallelization How to: summarize text through iterative refinement LangChain Expression Language (LCEL)‚Äã Should I use LCEL?LCEL is an orchestration solution. See our concepts page for recommendations on when to use LCEL. LangChain Expression Language is a way to create arbitrary custom chains. It is built on the Runnable protocol. LCEL cheatsheet: For a quick overview of how to use the main LCEL primitives. Migration guide: For migrating legacy chain abstractions to LCEL. How to: chain runnables How to: stream runnables How to: invoke runnables in parallel How to: add default invocation args to runnables How to: turn any function into a runnable How to: pass through inputs from one chain step to the next How to: configure runnable behavior at runtime How to: add message history (memory) to a chain How to: route between sub-chains How to: create a dynamic (self-constructing) chain How to: inspect runnables How to: add fallbacks to a runnable How to: pass runtime secrets to a runnable LangGraph‚Äã LangGraph is an extension of LangChain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. LangGraph documentation is currently hosted on a separate site. You can find the LangGraph guides here. LangSmith‚Äã LangSmith allows you to closely trace, monitor and evaluate your LLM application. It seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build. LangSmith documentation is hosted on a separate site. You can peruse LangSmith how-to guides here, but we'll highlight a few sections that are particularly relevant to LangChain below: Evaluation‚Äã Evaluating performance is a vital part of building LLM-powered applications. LangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators. To learn more, check out the LangSmith evaluation how-to guides. Tracing‚Äã Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues. How to: trace with LangChain How to: add metadata and tags to traces You can see general tracing-related how-tos in this section of the LangSmith docs.InstallationKey featuresComponentsChat modelsMessagesPrompt templatesExample selectorsLLMsOutput parsersDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingToolsMultimodalAgentsCallbacksCustomSerializationUse casesQ&A with RAGExtractionChatbotsQuery analysisQ&A over SQL + CSVQ&A over graph databasesSummarizationLangChain Expression Language (LCEL)LangGraphLangSmithEvaluationTracing",
      "timestamp": "2025-08-24 12:51:09"
    },
    {
      "url": "https://python.langchain.com/docs/integrations/chat/",
      "title": "Chat models | ü¶úÔ∏èüîó LangChain",
      "content": "Chat models | ü¶úÔ∏èüîó LangChain Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.On this page Chat models are language models that use a sequence of messages as inputs and return messages as outputs (as opposed to using plain text). These are generally newer models. infoIf you'd like to write your own chat model, see this how-to. If you'd like to contribute an integration, see Contributing integrations. Select chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"): os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\") model.invoke(\"Hello, world!\") Featured Providers‚Äã infoWhile all these LangChain classes support the indicated advanced feature, you may have to open the provider-specific documentation to learn which hosted models or backends support the feature. ProviderTool callingStructured outputJSON modeLocalMultimodalPackageChatAnthropic‚úÖ‚úÖ‚ùå‚ùå‚úÖlangchain-anthropicChatMistralAI‚úÖ‚úÖ‚ùå‚ùå‚ùålangchain-mistralaiChatFireworks‚úÖ‚úÖ‚úÖ‚ùå‚ùålangchain-fireworksAzureChatOpenAI‚úÖ‚úÖ‚úÖ‚ùå‚úÖlangchain-openaiChatOpenAI‚úÖ‚úÖ‚úÖ‚ùå‚úÖlangchain-openaiChatTogether‚úÖ‚úÖ‚úÖ‚ùå‚ùålangchain-togetherChatVertexAI‚úÖ‚úÖ‚ùå‚ùå‚úÖlangchain-google-vertexaiChatGoogleGenerativeAI‚úÖ‚úÖ‚ùå‚ùå‚úÖlangchain-google-genaiChatGroq‚úÖ‚úÖ‚úÖ‚ùå‚ùålangchain-groqChatCohere‚úÖ‚úÖ‚ùå‚ùå‚ùålangchain-cohereChatBedrock‚úÖ‚úÖ‚ùå‚ùå‚ùålangchain-awsChatHuggingFace‚úÖ‚úÖ‚ùå‚úÖ‚ùålangchain-huggingfaceChatNVIDIA‚úÖ‚úÖ‚úÖ‚úÖ‚úÖlangchain-nvidia-ai-endpointsChatOllama‚úÖ‚úÖ‚úÖ‚úÖ‚ùålangchain-ollamaChatLlamaCpp‚úÖ‚úÖ‚ùå‚úÖ‚ùålangchain-communityChatAI21‚úÖ‚úÖ‚ùå‚ùå‚ùålangchain-ai21ChatUpstage‚úÖ‚úÖ‚ùå‚ùå‚ùålangchain-upstageChatDatabricks‚úÖ‚úÖ‚ùå‚ùå‚ùådatabricks-langchainChatWatsonx‚úÖ‚úÖ‚úÖ‚ùå‚ùålangchain-ibmChatXAI‚úÖ‚úÖ‚ùå‚ùå‚ùålangchain-xaiChatPerplexity‚ùå‚úÖ‚úÖ‚ùå‚úÖlangchain-perplexity All chat models‚Äã NameDescriptionAbsoThis will help you get started with ChatAbso chat models. For detaile...AI21 LabsThis notebook covers how to get started with AI21 chat models.Alibaba Cloud PAI EASAlibaba Cloud PAI (Platform for AI) is a lightweight and cost-efficie...AnthropicThis notebook provides a quick overview for getting started with Anth...AnyscaleThis notebook demonstrates the use of langchain.chat_models.ChatAnysc...AzureAIChatCompletionsModelThis will help you get started with AzureAIChatCompletionsModel chat ...Azure OpenAIThis guide will help you get started with AzureOpenAI chat models. Fo...Azure ML EndpointAzure Machine Learning is a platform used to build, train, and deploy...Baichuan ChatBaichuan chat models API by Baichuan Intelligent Technology. For more...Baidu QianfanBaidu AI Cloud Qianfan Platform is a one-stop large model development...AWS BedrockThis doc will help you get started with AWS Bedrock chat models. Amaz...CerebrasThis notebook provides a quick overview for getting started with Cere...CloudflareWorkersAIThis will help you get started with CloudflareWorkersAI chat models. ...CohereThis notebook covers how to get started with Cohere chat models.ContextualAIThis will help you get started with Contextual AI's Grounded Language...Coze ChatChatCoze chat models API by coze.com. For more information, see https...Dappier AIDappier: Powering AI with Dynamic, Real-Time Data ModelsDatabricksDatabricks Lakehouse Platform unifies data, analytics, and AI on one ...DeepInfraDeepInfra is a serverless inference as a service that provides access...DeepSeekThis will help you get started with DeepSeek's hosted chat models. Fo...Eden AIEden AI is revolutionizing the AI landscape by uniting the best AI pr...EverlyAIEverlyAI allows you to run your ML models at scale in the cloud. It a...Featherless AIThis will help you get started with FeatherlessAi chat models. For de...FireworksThis doc helps you get started with Fireworks AI chat models. For det...ChatFriendliFriendli enhances AI application performance and optimizes cost savin...GoodfireThis will help you get started with Goodfire chat models. For detaile...Google GeminiAccess Google's Generative AI models, including the Gemini family, di...Google Cloud Vertex AIThis page provides a quick overview for getting started with VertexAI...GPTRouterGPTRouter is an open source LLM API Gateway that offers a universal A...DigitalOcean GradientThis will help you getting started with DigitalOcean Gradient Chat Mo...GreenNodeGreenNode is a global AI solutions provider and a NVIDIA Preferred Pa...GroqThis will help you get started with Groq chat models. For detailed do...ChatHuggingFaceThis will help you get started with langchainhuggingface chat models....IBM watsonx.aiChatWatsonx is a wrapper for IBM watsonx.ai foundation models.JinaChatThis notebook covers how to get started with JinaChat chat models.KineticaThis notebook demonstrates how to use Kinetica to transform natural l...KonkoKonko API is a fully managed Web API designed to help application dev...LiteLLMLiteLLM is a library that simplifies calling Anthropic, Azure, Huggin...Llama 2 ChatThis notebook shows how to augment Llama-2 LLMs with the Llama2Chat w...Llama APIThis notebook shows how to use LangChain with LlamaAPI - a hosted ver...LlamaEdgeLlamaEdge allows you to chat with LLMs of GGUF format both locally an...Llama.cppllama.cpp python library is a simple Python bindings for @ggerganovmaritalkMariTalk is an assistant developed by the Brazilian company Maritaca ...MiniMaxMinimax is a Chinese startup that provides LLM service for companies ...MistralAIThis will help you get started with Mistral chat models. For detailed...MLXThis notebook shows how to get started using MLX LLM's as chat models.ModelScopeModelScope (Home | GitHub) is built upon the notion of ‚ÄúModel-as-a-Se...MoonshotMoonshot is a Chinese startup that provides LLM service for companies...NaverThis notebook provides a quick overview for getting started with Nave...NebiusThis page will help you get started with Nebius AI Studio chat models...NetmindThis will help you get started with Netmind chat models. For detailed...NVIDIA AI EndpointsThis will help you get started with NVIDIA chat models. For detailed ...ChatOCIModelDeploymentThis will help you get started with OCIModelDeployment chat models. F...OCIGenAIThis notebook provides a quick overview for getting started with OCIG...ChatOctoAIOctoAI offers easy access to efficient compute and enables users to i...OllamaOllama allows you to run open-source large language models, such as g...OpenAIThis notebook provides a quick overview for getting started with Open...OutlinesThis will help you get started with Outlines chat models. For detaile...PerplexityThis page will help you get started with Perplexity chat models. For ...PipeshiftThis will help you get started with Pipeshift chat models. For detail...ChatPredictionGuardPrediction Guard is a secure, scalable GenAI platform that safeguards...PremAIPremAI is an all-in-one platform that simplifies the creation of robu...PromptLayer ChatOpenAIThis example showcases how to connect to PromptLayer to start recordi...Qwen QwQThis will help you get started with QwQ chat models. For detailed doc...RekaThis notebook provides a quick overview for getting started with Reka...RunPod Chat ModelGet started with RunPod chat models.SambaNovaCloudThis will help you get started with SambaNovaCloud chat models. For d...SambaStudioThis will help you get started with SambaStudio chat models. For deta...ChatSeekrFlowSeekr provides AI-powered solutions for structured, explainable, and ...Snowflake CortexSnowflake Cortex gives you instant access to industry-leading large l...solarDeprecated since version 0.0.34: Use langchain_upstage.ChatUpstage in...SparkLLM ChatSparkLLM chat models API by iFlyTek. For more information, see iFlyTe...Nebula (Symbl.ai)This notebook covers how to get started with Nebula - Symbl.ai's chat...Tencent HunyuanTencent's hybrid model API (Hunyuan API)TogetherThis page will help you get started with Together AI chat models. For...Tongyi QwenTongyi Qwen is a large language model developed by Alibaba's Damo Aca...UpstageThis notebook covers how to get started with Upstage chat models.vectaraVectara is the trusted AI Assistant and Agent platform, which focuses...vLLM ChatvLLM can be deployed as a server that mimics the OpenAI API protocol....Volc Engine MaasThis notebook provides you with a guide on how to get started with vo...Chat WriterThis notebook provides a quick overview for getting started with Writ...xAIThis page will help you get started with xAI chat models. For detaile...XinferenceXinference is a powerful and versatile library designed to serve LLMs,YandexGPTThis notebook goes over how to use Langchain with YandexGPT chat mode...ChatYIThis will help you get started with Yi chat models. For detailed docu...Yuan2.0This notebook shows how to use YUAN2 API in LangChain with the langch...ZHIPU AIThis notebook shows how to use ZHIPU AI API in LangChain with the lan...Featured ProvidersAll chat models",
      "timestamp": "2025-08-24 12:51:14"
    }
  ],
  "sitemap": null
}